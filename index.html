<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Audiovision Blog</title>

    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <h1>Audiovision Blog</h1>

    <!-- ------------------------------------ WEEK 1 ---------------------------------------------- -->
    <details class="week" id="week-1">
      <summary>Week 1</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week introduced sound as an emotional and perceptual experience
          that can guide interpretation and narrative. I was particularly
          interested in the exercise which played a score and we had to guess
          what the intended purpose was. I wrote it as a somber feeling mixed
          with hope, like an anticipation for something better when everything
          is wrong. It was nice to hear everyones perceptions of what they heard
          because they were geared towards the same sound, but different
          answers. It was very impressive when someone got the exact answer, I
          think that attests to how well the sound design was made for it.
        </p>
        <p>
          The Four Ways of Knowing stood out, especially participatory knowing
          (the knowing of feeling what it is like to be in an experience).
          Feeling what its like to be in a situation is very important with
          creating sound design (especially for films or games), even if youve
          never been in that situation yourself. Although it didn't just
          underline the importance to me, it also made me start thinking more
          about how the role of sound can be important for audiences to
          participate more emotionally to a piece of media. A lot of my personal
          projects with sound design have been very focused on realism, but I
          never really thought to consider exactly what I am aiming for with it.
          Am I trying to establish realism to make the visual space more
          believeable and draw attention to it? Am I trying to create certain
          emotions for things that haven't happened on screen? The video shown
          about how sound can change someones perception of visual media also
          led me to think more about it. A moment I found interesting was where
          it explained how sound could shift viewer attention, and how people
          change where they look on the screen based on what they can hear. I
          never really put 2 and 2 together that since I can hint towards
          something sonically, that would change how people perceive the visual
          media, thus changing where they look, maybe anticipating something. I
          just thought that was very interesting that sound can also be a tool
          to guide visual perception.
        </p>

        <h3>Academic Research</h3>
        <p>
          I was inspired by how sound could influence perception, and I ended up
          reading this article by Claudia Gorbman (Narrative Film Music), which
          just talks about how music can guide how people interpret narrative
          meaning. She talked specifically about 'unheard melodies' which is
          basically just how music can support narrative meaning in a subtle
          way, ways that the listener wont really realize unless thinking about
          it. It tied really nicely to what we learnt in class. I also went back
          to this previous paper that I had read previously by Chion. He
          discussed how audio could reflect on visual media in different ways,
          and how you use it is critical. For example, a disconnect in the music
          can create uneasy tension that adds complexity to the visuals, making
          something once seen as normal, into an underlaying feeling of tension.
        </p>

        <h3>Artistic Research</h3>
        <p>
          I started to look more deeply into music and emotions tying together,
          listening to my favorite songs and analysing what musically makes me
          love them so much. A specific example is Let Down by Radiohead. Or any
          radiohead song in general. The way that they build atmosphere, the
          experimental sound, the guiding structure, its moody, its spacey. Even
          i'm not 100% sure the technical reasons of why it makes people feel so
          much. There was a YouTube video I
          <a href="https://www.youtube.com/watch?v=PX2hQdcEvyA">watched</a>
          that I think explained it perfectly, and why it's so good alongside
          visual media. The use of harmony, texture, mood, rhythm. I think its
          refreshing considering a lot of songs nowadays follow certain
          formulas, but that's a completely different topic that I could go
          into.
        </p>

        <h3>Technical Research</h3>
        <p>
          Because of the Gina Moore assignment, i did a lot of research on how
          to use Ableton Live, which is my daw of choice. Experimenting with
          automation and how to make glitches were the highlights. Although I've
          done it automation before, I haven't done it extensively. The more I
          did it the more shocked I was of how it made a difference, especially
          because if I wanted a sound to go away, I just cut it out, but having
          it play soft in the background added that weight I always felt was
          missing from my sound projects. Learning different ways to make
          glitching sounds was really fun. I layered effects like pitch
          shifting, grain delay and reverb and then used automation to create
          sudden cuts, fade ins and randomized movement, which helped me achieve
          that glitch texture I was looking for.
        </p>
        <p>
          Other than Ableton Live, I actually learnt how to compose a simple
          orchestra piece for this assignment. I recognize that scores aren't my
          strong suit and I wanted to do something that challenged me,
          especially since the animation I chose was around 30 seconds, so I
          didn't have to make it too long. I watched
          <a href="https://www.youtube.com/watch?v=ZdhdC2wx2Ew">this video</a>
          which was actually extremely helpful and I learnt many new things. I
          thought the violin and viola were basically the same intstrument
          before this! It was interesting how different instruments in brass,
          woodwind and strings could play the same thing and it gave off
          different emotions and different vibes, even though they were in the
          same frequency spectrum. I particuarly liked the hack of making
          everything on a simple piano, then assigning it to different
          instruments, I definitely think it helped with my workflow as I
          started focusing more on chords and notes and why I wanted to press
          those, rther than being stuck figuring everything all out at once. I
          would highly reccommend that video to anyone looking at how to start
          with orchestral writing.
        </p>

        <h3>Progress Report</h3>
        <p>
          I began by choosing the animation I wanted, then thinking about what
          direction I wanted to go with. This will be written on my report but I
          originally started with a realistic approach but felt that the
          animation was too abstract, and wanted something to fit with that,
          which is why I chose to do a musical score. Of course as mentioned
          before, I chose to do an orchestral piece because of my nonexistent
          experience. Before watching the video I made the mistake of just
          jumping straigh into, which resulted in a terrible score. I just did
          chords and was playing random lead lines within the key, but it
          sounded terrible:
        </p>
        <img src="/photos/week1/original.png" alt="" />
        <p>
          My composition improved greatly after I watched the video. I started
          with a rough piano roll and kept refining it until I had a sound and
          structure I liked which consisted of a lead, a second lead, chords and
          a bass which was all I needed!
        </p>
        <img src="/photos/week1/pianoroll.png" alt="" />
        <p>
          From this I made different groups of the main instruments of strings,
          and assigned the different parts appropriately. The plugin I used was
          BBC's expressive strings, it was super great and useful and free!
        </p>
        <img src="photos/week1/strings.png" alt="" />
        <p>
          then i did the same for brass, using similar mapping techniques to
          fill out the orchestration
        </p>
        <img src="photos/week1/brass.png" alt="" />
        <p>and then woodwinds.</p>
        <img src="photos/week1/woodwind.png" alt="" />
        <p>
          after that it was just a matter of using automation to create
          structure! Then the sound design was finished! Originally it took up
          the entire time, but when I decided that the glitches would turn it
          into synths, I cut the midi short. I used a bunch of random effects
          tomake the glitch sounds as I mentioend earlier, and then to
          transition, I layered a abunch of the elad lines with different sounds
          and textures to lead into the glitchiness, but also give it some
          melodic strucutre so the change wasn't too jarring! This completed my
          first draft and I'm still refining, but I have a structure I like and
          an idea I like so I am happy.
        </p>
        <img src="photos/week1/transition.png" alt="" />
      </div>
    </details>

    <!-- ------------------------------------ WEEK 2 ---------------------------------------------- -->
    <details class="week" id="week-2">
      <summary>Week 2</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week in class we analysed the Turn Down for What music video with
          and without sound. I first watched it muted and made notes on what I
          thought the visuals were trying to communicate. Without the audio, it
          felt like a chaotic horror-comedy film where someone’s out of control
          and something is taking over them. The character’s movements were
          exaggerated and crazy, which made the visuals seem comedic, but also
          strange and hard to follow. Without sound, the pacing felt confusing,
          and the structure was harder to grasp.
        </p>
        <p>
          Once the audio was added back in, the whole thing made more sense. The
          music gave it rhythm and energy, making it feel more like a
          conventional music video, where weird visuals are usually expected.
          The sound helped me anticipate moments like when the chorus began, the
          movements got more repetitive and bizarre. it gave the piece form and
          made it easier to follow and highlighted how sound can structure
          visuals and change how we interpret movement.
        </p>
        <p>
          Since I’ve been away, I haven't able to attend the lectures this week,
          but I did get to watch some of my friend's work online. It was really
          inspiring to see how different people approached the same animations
          in their own way, especially because when I was choosing what
          animation to do last assignment, I had my own ideas for each one. It
          was interesting to listen to how they handled mood, timing, and
          transitions, and really hammered in how much creative interpretation
          in sound design is just as important as technical choices.
        </p>

        <h3>Academic Research</h3>
        <p>
          For this week, I mainly referenced two texts that helped shape how I
          approached the Resonance video. The first one was Karen Collins, who
          talked about how ambient textures can help set the emotional tone
          before anything even happens visually. That stuck with me, because I
          start my projecst by focusing on atmosphere rather than realism, and
          it gave me context on why I liked doing that. I wanted the sound to
          slowly evolve and feel like something unstable was underneath, and her
          ideas made me feel more confident in that choice as a lot of the
          people around me usually start with diegetic sound.
        </p>
        <p>
          The second article I read was by Sonnenschein, where he explains how
          different frequency layers being used can create space and shape how
          things feel. I used this information for the narrative arc, using the
          lower frequencies at the start, then evolving the atmosphere by
          introducing different sound frequencies. By balancing the low end
          rumbles with higher pitched textures, it gave the piece more depth,
          like something was exploding.
        </p>

        <h3>Artistic Research</h3>
        <p>
          For this week, I tried to find artists where the music builds mood and
          atmosphere that makes the audience feel emotional. One of them was
          Jeff Buckley, especially his Grace album. Music alone, his use of
          dynamics, reverb and vocal layering creates this haunting and
          immersive sound that really stuck with me. I was especially immersed
          with how his songs often start soft and open, then swells into
          intensity in a yearning way. The switch in dynamics is something that
          influenced how I looked at Resonance.
        </p>
        <p>
          I also listened to Johann Johannsson and his score for Arrival. The
          blending of orchestral instruments with electronic textures felt
          realistic, but otherwordly, which really suited the film. I liked the
          shifts in tone and textures to guide the audience through emotion
          without the need for strong, imposing melodies. It highlights the
          possibility that music can be felt rather than heard, and still have a
          strong impact.
        </p>

        <h3>Technical Research</h3>
        <p>
          I explored how to build space and emotion through effects this week.
          To make the sound design feel wide and immersive, I experimented with
          varying levels of reverb, delay, and layering and keeping the dry/wet
          balance lower at the start and opening it up as the visuals progress.
          I also experimented more with automation than I usually do, especially
          to make certain parts fade in or glitch out. Mastering has always been
          difficult for me, and I watched a few different videos to help me with
          it! One of them being
          <a href="https://www.youtube.com/watch?v=fZdlUiqiDoM">this video</a>
          which talks about different techniques to make audio spaces feel wider
        </p>

        <h3>Progress Report</h3>
        <p>
          This week I mainly focused on the completion of Assignment 2.1. I
          started by creating the atmosphere first by layering different
          horror-themed ambience to create the general structure
        </p>
        <img src="photos/week2/ambience.png" alt="" />
        <p>
          Then I added all the sound effects, including the wet sounds for the
          bursting tentacles (I used a variety of different slime sounds), the
          cinematic risers and faders, etc.
        </p>
        <img src="photos/week2/fx.png" alt="" />
        <p>
          After that I experimented with different odd and unique sounds to get
          a more interesting texture
        </p>
        <img src="photos/week2/unique.png" alt="" />
        <p>The finally just the static at the start.</p>
        <img src="photos/week2/static.png" alt="" />
        <p>
          After that I spent quite abit of time adding reverb and sending it
          through buses, and EQ shaping and effects to make the sound a bit
          wider a larger!
        </p>
        <p>
          Other than the Resonance project, I was also able to secure the spot
          to make sound design for one of my friends games! It is a pretty
          simple game, but it is a good introduction into make sound for games,
          as I have never tried that before. I will need to learn Unity and
          FMOD, and just interactive music in general.
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 3 ---------------------------------------------- -->
    <details class="week" id="week-3">
      <summary>Week 3</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          As I can't attend class, it's pretty difficult to write a reflection
          on ideas raised in class, especially when there isn’t any official
          lecture, but I will try my best!
          <br />
          <br />
          The first class focuses on presentations and feedback for Assignment
          2.1. I showed a few of my family members I am with my project and
          asked their thoughts on it. They aren't sound designers and don't
          really think like them, but this is exactly why I thought their
          feedback was so good. They were able to tell me feedback based on a
          normal consumer level. For this project I generally was told that it
          was very low and that I tend to focus on lower sounds and frequencies,
          leaving the higher ones lacking. I didn't make use of the entire
          frequency spectrum. This made the sound seem a bit muddy or even
          lacking, and hard to make out specific parts. I think especially
          listening to my own work on speakers made this very clear, as I didn’t
          really notice when I was mixing it with headphones in.
          <br />
          <br />
          Another piece of feedback I got was that my mixing could have been
          better. The atmospheric sounds were a bit low and hard to hear, while
          the squelching noise was loud and overbearing, it was basically all
          they could hear. I think feedback like this is especially important,
          and will keep in mind for the future! I think I really need to invest
          in some speakers... It also made me realise how important it is to
          check a mix on different playback systems, what sounds balanced on
          headphones can feel completely different on speakers. This is
          something I’ll try and consciously do for all my future projects.
          <br />
          <br />
          Although, they did like the amount of layers I put in, they felt like
          it had good amounts of sound, was realistic, and they could make the
          connection between the visuals and the audio I chose. They also liked
          the atmosphere I was able to create, and they were particularly
          impressed with the sound design when the tentacles started exploding
          out of the planet. Both the positive and negative feedback is very
          important to me, so I was happy to receive detailed feedback from a
          regular, consumer perspective. It reinforced the idea that while
          technical skill is important, the audience’s emotional and intuitive
          response is ultimately what matters most.
          <br />
          <br />
          The second class had a guest lecturer, and I’m honestly so annoyed I
          missed it because the topics covered sound like they would have been
          incredibly useful. Since I wasn't there I will just talk about my own
          thoughts...
          <br />
          <br />
          I’ve always been a bit confused about the difference between sound
          editing and sound design. A lot of my own process involves editing and
          adding effects to audio from sound libraries, so does that still count
          as editing, or is it considered sound design? Does sound design only
          mean foley or creating audio entirely from scratch? It makes me wonder
          what actually defines a sound designer, especially when I’ve seen
          purists online who refuse to use sound libraries at all, and shame
          anyone who does. Personally, I think there are certain sounds that are
          just impractical or nearly impossible to record yourself, so using a
          library can be the smarter option. I mean, if it already exists, why
          not use it? But I do think people should at least try to build up
          their own sound repertoire so they don't have to rely on sound
          libraries forever.
          <br />
          <br />
          I also think that knowing how to make exactly what you're thinking is
          incredibly hard, but super rewarding once you are able to. As with all
          creative practices, you get the hang of it once you start doing it
          more, and being in this studio and having to make audio for visuals
          every week so far has actually shown a significant improvement in my
          skills, which I am both surprised and thankful for. Missing the
          lecture makes me want to spend some time researching the roles and
          workflows the lecturer discussed, because I can already see that
          understanding the boundary between design and editing could help me
          plan my projects more efficiently.
          <br />
          <br />
          This also made me think about the phenomenon of the Wilhelm scream,
          which is that infamous stock sound effect everyone recognises. When
          it’s used, it can snap the audience out of immersion because they
          immediately associate it with other films. I think that raises an
          interesting question about when familiarity adds charm or nostalgia
          versus when it becomes distracting. In this case, I can see how using
          sound libraries can be not ideal, especially for big blockbuster
          films. Like surely they could have filmed a guy screaming instead of
          choosing to use a stock sound? I mean they have such big budgets
          right? This makes me reflect on my own sound choices, maybe I should
          be more conscious of using highly recognisable sounds unless I’m
          intentionally going for humour.
          <br />
          <br />
          Anyways, the rest of the lecture content, things like EQ, editing
          concepts, layering techniques, emotional architecture, and more, all
          sound super interesting. I’d love to dive deeper into those topics, so
          if the lecturer presents again or if there’s an online recording
          somewhere, I’d definitely want to check it out. In the meantime, I
          will probably explore some online tutorials or breakdowns from
          professional sound editors to get a better grasp of these concepts, so
          that next time I’m working on a project, I can apply more deliberate
          choices in rhythm, layering, and tonal shaping.
        </p>

        <h3>Academic Research</h3>
        <p>
          This week I read Zwicker and Fastl's Psychoacoustics: Facts and Models
          to understand more about how frequency content affects how we perceive
          time and motion in sound. They talk about how removing higher
          frequencies can make things feel slower and heavier, which is exactly
          what I needed for the slow-motion sections of my project. It gave me
          the idea to use frequency filtering instead of pitch-shifting or
          time-stretching. It was nice to have theory backed reasonings for my
          work, as I did notice a significant difference by using the pitch
          filtering because it was exactly what I was aiming for.
          <br /><br />
          I also read Altmans Sound Theory, Sound Practice which kinda goes into
          how sound and image work together to make meaning. Although this made
          sense to me already and in my head I thought this was obvious, I
          started to think more intentionally about my design choices, and why I
          chose a specific pad sound. I realized I chose it because
          subconsciously I wanted the pad to build up in the same way that the
          cube does visually. I connected this with the pitch frequency
          filtering, where the sound slowly regains its higher frequencies and
          the audience will get reinforcement that the video is progressing, and
          just gives it structure overall.
          <br /><br />
          Holman's Sound for Film and Television was also another useful text
          for this week. He talks about matching the texture of sound to the
          visuals, which made me change my EQ choices for the Lego building
          sound. I knew that the Lego sound felt out of place, but with EQ I was
          able to get it to feel shinier and cleaner which matches the visuals
          aesthetics way more. I did this by boosting the higher end, lowering
          the bottom and keeping the mids relatively the same to still maintain
          that tactile feeling of the original recording!
        </p>

        <h3>Artistic Research</h3>
        <p>
          I was rewatching a few old Pixar films recently and one of them
          particularly caught my attention sound wise. Wall-E's different sounds
          and expressions are made by using everyday objects to make sound, yet
          still felt so emotional and believable and had this weight to it. It
          felt authentic. I was inspired by this, as I thought it was so
          impressive how this sound was made and edited. It was what gave me the
          idea to find everyday objects for this weeks project, which ended up
          being Lego. It is a familiar and nostalgic sound and while its not a
          direct match for the sound I imaged for the cube, I could process it
          but still keep that tactile identity.
          <br /><br />
          I also listened to Royji Ikeda, who makes super minimalist and precise
          tones to make his sound. It's very clean and he uses a lot of control
          and timing to make his music (though I would just call it sounds
          rather than music). This is what made me want to try limiting the
          amount of sounds I can use, to try getting the full potential out of
          the sounds I already have in the project, by using control and timing.
          Ikeda showed me that simplicity can be just as powerful and impactful
          as complexity, and I really wanted to experiment with that in my
          project this week.
          <br /><br />
        </p>

        <h3>Technical Research</h3>
        <p>
          This week I mainly focused on how to make glassy and shimmery sounds
          from scratch, as I tend to rely on a lot of sound libraries for it.
          Although there is nothing wrong with using sound libraries and
          manipulating the sound, I thought it would be best to at least learn
          how to make some so I could build my own sound effect library. I
          followed this
          <a
            href="https://www.youtube.com/watch?v=CEZHfQOqQxQ&ab_channel=UnderdogElectronicMusicSchool"
            >YouTube video</a
          >
          which uses FM synthesis to make those glassy sounds. He is a very good
          teacher, calm and explains himself well, so I will be looking more
          into his other videos in the future. Although he used an Ableton
          feature that I don't have, I found the skills to be transferrable to
          other budget options and plugins, as he explains why he does each
          step, and even showing the difference between each parameter in the
          different effects he uses. It was a super informative video and I was
          able to make a few sounds I liked! <br /><br />
          I also watched
          <a href="https://www.youtube.com/watch?v=UQy7VHm-Ucg">this video</a>
          which is a tutorial on how the PaulXStretch plugin works. It goes
          super in depth and explained all the features and I can't believe the
          plug in is free. Although I ended up having some technical troubles
          and couldn't use it for this weeks projects, I am excited to fix it
          and use it in the future! Especially the ability to change different
          harmonics and use it in conjuction with other effects would make for
          really good experimenting.
        </p>

        <h3>Progress Report</h3>
        <p>
          I had a lot of projects to work on this week! I had a bunch of people
          reach out to me to do the sound design for their projects but
          unfortunately I wasn't able to do all of them, even though they were
          all such interesting concepts. One of the projects I chose to work on
          was "The Reconnect". Caroline, the animator, sent me her draft
          animatic, as well as a sound design brief and the project concept. I
          really appreciated how organized and professional the documents she
          gave me was, it was super helpful to decide what I wanted to do for
          her project, while bringing her vision to life. She gave me a small
          part of her animatic that was confirmed to stay the same, so I worked
          on that scene.
        </p>
        <p>
          I first started by making the ambience of the scene, like I usually
          do. I layered a few different textures to make the structure of the
          scene. Though it was pretty hard because it was a pretty small scene,
          but I'm sure when I work on the full thing, I'll be able to transition
          between sounds better, without sounding too abrupt.
        </p>
        <img src="photos/week3/reconnect2.png" alt="" />
        <p>Then I worked on the diegetic sound:</p>
        <img src="photos/week3/reconnect3.png" alt="" />
        <p>
          Then worked on the non-diegetic sound, and transitional sounds and
          wooshes:
        </p>
        <img src="photos/week3/reconnect1.png" alt="" />
        <p>
          And that was basically done for the scene! Again, it was pretty short
          and I kept the narration that was in the original animatic (which is
          why it might sound bad in terms of the mix, because the narration was
          also a draft), but I am happy with the general idea, and so was
          Caroline. She mentioned some changes when she finalizes the animation
          but it is a good start! You can watch the draft
          <a href="https://youtu.be/UgO7CwLyiRE">here</a>!
        </p>
        <p>
          Then I also worked (and finished) my Assignment 2.2 as I am busy next
          week and won't have time to do it! I first began by making my own pad
          sound using the tutorial I showed you earlier. I used his tip of
          turning the midi file into an audio file and used that to make all the
          changes, and added it into my personal sound library.
        </p>
        <img src="photos/week3/own.png" alt="" />
        <p>
          I then experimented with grain delay, using the pitch parameter to try
          and simulate the slow motion effect:
        </p>
        <img src="photos/week3/graindelay.png" alt="" />
        <p>
          I wasn't a fan of this as I wrote on my report, and decided to try
          using the PaulXStretch plugin:
        </p>
        <img src="photos/week3/paulstretch.png" alt="" />
        <p>
          But again, wasn't a fan and there was a steep learning curve and
          technical difficulties. I then decided to use frequencies filtering
          instead, which I was a fan of!:
        </p>
        <img src="photos/week3/padauto.png" alt="" />
        <p>
          It sounded exactly like I wanted. And then I did it for the lego
          building sound as well, with less of a sweep:
        </p>
        <img src="photos/week3/legoauto.png" alt="" />
        <p>
          Then it was just a matter of finishing touches! I added the sparkles
          that go throughout the entire video, but used volume automation so it
          was more prominent in the later half of the video:
        </p>
        <img src="photos/week3/sparkle.png" alt="" />
        <p>
          And that was done for AV2.2! I just had to write the report. But other
          than that, I also decided to use one of my old song projects for the
          music video I will do for this semester. Although I haven't 100%
          decided it will be a music video, I was also thinking of doing a light
          show for the Capitol Theatre! Which I thought would be cool. Anyways I
          went through a bunch of my old guitar files, and found this one I
          liked, where it was in the shoegaze genre, and I added some lead
          guitar over it and messed with the effects a bit, as well as adding
          the much needed drums. Although I am not a drummer and had to MIDI the
          entire thing, when I get back to Australia I'll probably use real
          drums and fix up the guitar parts, as well as add more instruments.
          You can watch the WIP of the song
          <a href="https://youtu.be/6MhdX7PVxWc">here</a>. I chose that song
          because it's very atmospheric, but still incorporates live instruments
          so it is the best of both worlds! (and I am a huge shoegaze fan)
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 4 ---------------------------------------------- -->

    <details class="week" id="week-4">
      <summary>Week 4</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This weeks lesson was about giving feedback to your peers about
          Assignment 2.2. Since once again I am away, I've decided to ask the
          people around me for feedback! Thankfully this is the last lesson I am
          away since next week there's no classes! The advice I got last week
          was actually extremely helpful in learning on what other people
          typically hear in a consumer standpoint, it helped me inform my
          decisions for this project.
          <br />
          <br />
          The main critique I had for my sound design was about the foley that I
          used for the cube-builing sequences and that it didn't feel as
          convincing as it could have been. They mentioned that it felt too
          clean and didn't match too much with the visuals. I realized that this
          probably happened because I relied too much on just the one sound
          layer, instead of using multiple diffrent textures. I usually do like
          to stack a bunch of effects and layers to achieve a sound, so I'm not
          sure why I decided not to do it here! Maybe because I was too focused
          in matching the clean visuals, and the fact I tried to impose a
          challenge on myself in using minimal layers. I think doing this lost a
          lot of weight into the sound that could have made the foley more
          believable. But now, I can actively use more layers on my next project
          (sourcing a variety of different sounds) and then layering them to
          create more depth and realism!
          <br />
          <br />
          The other criticism I received was that the atmosphere wasn't as
          immersive as my previous project. Even though there was that challenge
          of the minimal layers, I do think the atmospheric sounds and ambience
          was a bit low, and you could barely hear it once the layers started to
          build up. I definitely spent more time building layers and acheiving
          depth compared to this work, which is probably why they felt there was
          a lack of atmosphere in this project. Next time, I would like to be
          more intentional about the spatial audio, and making sure the
          environment feels alive before I start working on the diegetic sound!
          <br />
          <br />
          However, the consensus was that the slow motion sections felt great,
          they could feel that it was meant to be slow motion. It felt natural
          and well-timed, and they particularly liked how all the aspects felt
          like they were slowing dowm, and helped reinforce the visuals. I put a
          lot of focus into getting the feeling of slow motion down, and getting
          all the automation to match up to the visuals and all the sound, so I
          was very glad and happy to receive that feedback! Very encouraging.
          The changes between different motion speeds flowed in a way that kept
          the pacing smooth, which tells me that my use of automation has gotten
          much better! I'd like to experiment a bit more on the gradual blends
          and transitional elements to really nail it for my future projects!
          <br />
          <br />
          Overall this feedback made me realize I tend to prioritize certain
          aspects and because of that, potentially can overlook others. I'm
          happy with the parts I spent a lot of time on, but I know I really
          need to push my foley design and environmental layering if I want to
          improve my design further!
        </p>
        <h3>Academic Research</h3>
        <p>
          This week I researched a lot on how to create horror ambiences, and
          what can cause viewers to be scared just through audio alone, as well
          as how audio can enhance visuals and imply something is there to keep
          viewers on the edge of their seat
          <br />
          <br />
          Blumstein, Davitan and Kaye discuss how harsh, non-linear sounds can
          trigger instinctive threat responses in their article, 'Do film
          soundtracks contain nonlinear analogues to influence emotion?'. This
          can be sudden distortion, bursts of noise, unpredicatable modulation,
          etc. because they can resemble animal distress calls. I could use this
          idea by adding these short bursts of sound and effects at moments
          where I want to create sudden tension. Using this technique would give
          me a more natural way to make an audience feel unsettled without
          solely relying on visuals jumpscares.
          <br />
          <br />
          Another interesting article I read was by Menninghaus 'The
          Distancing-EMbracing model of the enjoyment of negative emotions in
          art reception'. It looks about how inharmonic or detuned sounds create
          a sense of unease because it disrputs the harmonic balance that we
          expect. This would be interesting to apply into my horror/triller
          ambiences that I am creating by layering slightly out of tuen drones
          or pads underneath other layers to make a sense of unease in my
          projects. It would let a scene feel tense even if nothing dramatic is
          happening visually, which is important for horror ambiences.
          <br />
          <br />
          A lot of the research I do on ambiences and music typically involve
          sound and making it. But I think this book by ZhouI read is
          particilarly interesting ebcause it talks about the absence of sound
          and its effects ('Effects of silence on emotion response to sound').
          He explains how silence and sudden changes in dynamic can actually
          heighten emotional impact and reactons to sound. I think I can use
          this in my own projects by adding short, deliberate pauses before a
          loud or important sound effect , which can bring attention to it and
          heighten its impact. It would make those moments stand out more,
          especially in suspenseful scenes.
        </p>
        <h3>Artistic Research</h3>
        <p>
          Because I've been so focused on horror this week, I went back to the
          Silent Hill series, a famous horror game known for its creepy
          atmosphere. Akira Yamaoka was the sound designer for the franchise,
          and he tends to blend industrial noise, detuned instruments and
          minimal melodic elements to create spaces that feel haunting. Since
          Silent Hill is a game, he uses looping textures that evolve slowly,
          making the player feel trapped in the emotional state he creates. I
          could use this technique in my future projects, and even the
          animations I am involved in this semester by creating subtle
          variations between each scene to sustain tension in scenes where not a
          lot of dramatic events happen. However this would be especially useful
          in long sequences where the user explores a space and they tend to get
          lost and immersed, that they forget about the sound.
          <br />
          <br />
          Another game that is pretty famous for its horror sound design is
          Resivent Evil. Shusaku Uchiyama relies heavily on lower drowns, tight
          reverb and metallic sounds to manipulate the players feeling of space,
          making them often feel ecnlosed and claustrophobic. I love this game
          series and I genuinely feel scared while playing the game, especially
          being hyper aware of my surroundings and getting scared over things
          that wouldn't scare me usually. I feel like I could adapt this into my
          own work by shaping the reverb and adding lower frequencies to make
          the space feel smaller and constricted, to try and make the viewers
          feel the same as I do when I play Resident Evil. I think using sharp
          high pitched stingers can also make the listener snap and pay
          attention to key moments if I use them alongside the low drones.
          <br />
          <br />
          Other than games, I also found this sound designer and composer called
          Lustmord, who specializes in making ambient music that feature low
          drones, reverb and little high frequency details. This skewed
          frequency spectrum can evoke a sense of oppressive vast emptiness. His
          music is kind of minimalist, which proves that horror sound design
          doesn't need to be super busy or full of layers to be effective, it
          comes from scale and patience and building (which I could probably use
          in my projects as I tend to just stack layers and hope it works). I
          could use his philopsophy to try to strip back my soundscapes and
          focus on depth and detail in a few elements to make the atmosphere
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I watched
          <a href="https://www.youtube.com/watch?v=U7wkq_s2t4M"
            >How to Design Dark and Scary Sounds</a
          >
          which went through a bunch of techniques on, well, how to design dark
          and scary sounds! It went through backmasking, backwards reverb,
          backwards talk, the importance of slowing things down and combining
          all of these. I liked this tutorial because it shows how different you
          can make sounds just by transforming existing recordings using effects
          and such, rather than trying to make a purely scary sound.
          Experimenting with my own sounds and seeing what I can create with
          them is an important part of sound design and I want to try
          experimenting with this in the projects this semester! I think it'd be
          cool to have backwards talk for an easter egg in one of the
          animations.
          <br />
          <br />
          I also watched
          <a href="https://www.youtube.com/watch?v=8QlWUGtB_9w"
            >How to Sound Design Horror Atmospheres using Vital</a
          >
          which was basically a deep dive into building horror ambiences from
          the very beginning. It also was kind of a tutorial on how to use Vital
          as I am still not 100% comfortable using it. Anyways, it went step by
          step on how to set up the oscillators and effects and adding pitch
          modulation and movement. It also went through a bit of basic music
          theory, specifically using minor seconds to create dissonance and also
          showed how to create intros, risers and just evolving atmospheres in
          general. This was super helpful because it explained what to do, but
          also why were doing each step so I could make my own atmospheres using
          the same basics. Even though it was specific to Vital, I can
          definitely use this information in other synths or plugins!
        </p>
        <h3>Progress Report</h3>
        <p>
          This week was when I was the most busy overseas, so I didn't get to
          work as much on my projects as other weeks(which is why a lot of my
          assignments were done super early if you noticed!), but I still tried
          to do a few.
          <br />
          <br />
          I mainly tried to work on things that I could do without external
          equipment (as I usually use a MIDI keyboard, and I don't have access
          to a professional recorder). I worked on the song I will use for the
          music video (although plans have changed and I'm not sure if my
          videographer has time, so I might do a lighting show alongside my
          song) and fixed up a bit of the drums. I used the free kits in the
          Steven Slate Drum's pack and it sounds way better than the default in
          Ableton. Though some of the snares were a bit off so I had to go in
          and manually change the different MIDI notes which was a bit tedious,
          but I am very happy with how it sounds so far. I also used Shreddage
          to add a bit of bass and other guitar parts that I would like to try
          once I'm back at home with my instruments. I also started to
          experiment with lyrics (this is what I was dreading) and melody, but
          haven't found anything that particularly stuck to me.
          <img src="photos/week4/1.png" alt="" />
          I did the same with some piano synths to create a kind of wall of
          sound. I used the video mentioned in the technical research to really
          help me with the order of effects and such. I'm very excited to work
          on this with real instruments (especially real drums!)
          <br />
          <br />
          This week had a lot of focusing on planning the semester, especially
          how I will organize my time once I am back.
          <br />
          <br />
          I had a call with Tara, the animator of The Hollow Child, as well as
          Maria (a sound designer from our class), as we are both working on
          this project together. We just talked about what her vision is for the
          sound design, and what we could achieve based on that. She was very
          understanding and very helpful, gave us a timeline of when things will
          be done, what she expected of each scene, but also trusts us and gave
          us a lot of room to be creative and do what we want to make the sound
          match the visuals. It's hard to work on specific sounds as the
          animatic isn't confirmed 100% yet and is still subject to change, but
          she said she will be done soon so we can start as soon as possible!
          She also put us in contact with the voice actors, so we are able to
          communicate directly in case certain sounds need to be rerecorded, I
          am happy to be working with someone who is on top of everything and is
          thinking ahead!
          <br />
          <br />
          The other animation, 'The Reconnect', that I am also working on, I
          also had a proper chat with animator on what she wanted the sound
          design to achieve. She was more loose with the descriptions, and
          instead wrote more about what she wants each scene to feel like. I
          think this will be good experience on creating my own visions based on
          client's requirements on the atmosphere and end product, while the
          other project will be good experience on achieving balance betweenthe
          specifics of what the client wants for each scene, versus what I
          believe as a sound designer. I am very grateful to both for being very
          flexible and understanding, while also giving guidance, I am very
          excited to be working with them! At the time of writing this, she
          actually reached out to me saying the first 10 seconds of the
          animation is fully complete so I will be working on that next week!
          <br />
          <br />
          Because both of these animations are very horror, thriller and tension
          based, I started to resarch a lot with creating ambiences, since I can
          use these as layering materials for any of these projects (and even
          future ones!). What I learnt can be seen in the technical research
          section! This was my favorite part as I was able to create a lot of
          audio clips that I can use in the projects, maybe with a bit of
          editing to fit the actual scene, but overall I am very happy with the
          progress. I have a lot to work on once I am back next week!
          <br />
          <br />
          I also had a call with the game developers of 'Fired Before Hired' and
          what sound design and foley they expected of me. This project is
          heavily reliant on realistic sounds, like whooshing of swords,
          furniture being broken and torn, etc. so I really get to work on my
          audio recording skills (which honestly need a lot of work...!). I am
          nervous to record, but also excited because I know its a skill that is
          necessary and would like to get better at. I was also asked to make
          the menu music, and kind of like office/elevator music. They said they
          weren't sure of what kind of vibe or genre they are going for, so they
          want me to make a small snippet of different ones, like 8bit,
          electronic, real instruments, etc. This makes it kind of difficult,
          but hopefully I can only make a 5-10 second clip so they know. Or I
          was thinking of using MIDI, so I am able to easily change the
          instruments so they can decide, and then I can further refine it.
          <br />
          <br />
          With this information I made a general timeline of when I want to have
          things done, but the general timeline is as follows:
          <br />
          21st-24th = Fired Before Hired menu music clips, figure out all parts
          of the song
          <br />
          25th-31st = Begin working on the animations (this is when both
          animators are expecting to finish the final draft so we know the
          timing of each scene), especially using the pod alongside Maria to get
          our ideas down, start officially producing song
          <br />
          1st-7th = Finishing producing song, as well as mixing and mastering,
          record and finish the sound bits for Fired Before Hired, continue to
          work on animations as scenes gets finished
          <br />
          8th-14th = Figure out how to work sound in Unity, start working on the
          level music for Fired Before Hired, continue to work on animations as
          scenes gets finished
          <br />
          <br />
          This is just the general plan for the next few weeks, I am very aware
          that it is a lot per week, and other classes may take priortity in
          certain weeks. This is just a general guideline of the best case
          scenario and finishing early, which I know won't 100% happen. But
          having a guide of what I want to do and pushing myself helps me to do
          more work! Anyways I am definitely savouring my last week of being
          away before all my assignments starts getting due soon....
          <br />
          <br />
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 5 ---------------------------------------------- -->

    <details class="week" id="week-5">
      <summary>Week 5</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          There was no class this week so nothing I can write for in class
          learning, but I am writing more reflective writing in the research
          section to make up for it!
        </p>
        <h3>Academic Research</h3>
        <p>
          I did some research in collaboration in creative audio communities,
          and found this article by Calefato, Lanubile, Novielli and Valetto,
          titled "Collaboration success factors in an online music community".
          It talks about successfull collaboration depends on momentum,
          recognition, and individual contributions for the project. This really
          helped me figure out a way to interact with the people I am
          collaborating with, changing my mindset into a more professional one
          and some tips on how to make the collaboration successful. The article
          made me consider how my sound design interacts with their creative
          vision, rather than just being a seperate entity. Since the
          collaborative works I am working on are majorly for someone elses
          creative visual work, it is important to keep a line of communication
          open, while also figuring out on creating a balance of what the
          collaborator wants, as well as putting my own creative vision into the
          work. I also found that receiving feedback and adjusting drafts was
          not only about fixing mistakes, but also creating cintinuity with the
          animators intention for the work. I also noted that some collaborative
          practices could be helpful in my own solo projects, for example,
          creating a variety of drafts to see which I like better, rather than
          continuously working on the same project to see if I'll end up liking
          it. The article really helped me develop this mindset of openness and
          responsiveness that could definitely imrpove both group and individual
          projects.
          <br />
          <br />
          I read another article titled "Footsteps with character: The art and
          craft of Foley" written by Wright. It describes Foley as not just a
          technical task to record sounds, but also a craft where sound
          designers have the chance to add character and intention into ordinary
          noises. This information will greatly help me in the future when I do
          more intensive foley tasks. I have taken foley as just a task that I
          needed to tick off before I start doing actual sound design work, but
          this shifted my mindset into adding more intention with my recording.
          I could experiment with textures and performance to transform regular
          sounds into something that would fit hte emotional tone of the
          project.
          <br />
          <br />
          The last article I read is by Sonnenschein, "Sound design: The
          expressive power of music, voice and sound effects in cinema". It
          talks more deeply into creating atmosphere, and especially reverb and
          other spacial effects can play a crucial role in establishing a
          storytelling place and guides audience's emotional and perceptual
          focus. This directly connects to by attempt to design the atmosphere
          of a church which is a setting in one of the animations I am working
          on. In my draft, I leaned heavily of reverb to create a sense of
          scale, but after reading this, I realize I might have prioritized size
          over clarify of my work. Sonnenschein'd discussion of how sound
          environments shoould evolve dynamically made me think of reverb as
          something I should modulate and change across scenes, which I will
          definitely keep in mind for the future scenes of the animation.
          <br />
          <br />
        </p>
        <h3>Artistic Research</h3>
        <p>
          Ben Burtt, who is best known for his work in Star Wars, is a foley
          artist that I looked into this week. He invented the sounds which have
          become icons in culture, and he approached it by using unexpected
          sources for sound. He shows that sound design is about creativity, not
          just accuract. I realized I could adopt a similar mindset for my own
          foley work. For example, instead of trying to find the most accurate
          footstep in the animation I'm working, I could isntead try to find a
          sound that could convey the weight and unease and tension in that
          moment. Burtt;s practive reminds me that emotional believability is
          also an important factor to consider, rather than just realism. I want
          to explore more freely with Foley in my upcoming projects so they can
          carry the sound I want it to express.
          <br />
          <br />
          Chris Watson is another foley artist that mainly captures amazing
          natural soundscapes with amazing detail and is often used in nature
          documentaries. His work shows how sound can transport a listener into
          the place and immerse them in its atmosphere, even without visuals.
          This pushes me to think of environmental sounds not just as background
          noise, but the backbone of the scene. I realize I should listen and
          put more intention in the ambience that I create, experimenting with
          layers of tone, resonance and silence. I want to to make a mix that
          tries to capture a living environment, and shape how the audience can
          feel inside that space.
          <br />
          <br />
          The last artist I looekd into was Suzanne Ciani, who makes electronic
          soundscapes using synthesizers to design immersive textures! She
          created electronic sounds for Atari and Coca Cola ads and was a
          pioneer with proving how synthetic audio could feel tactile and human.
          I really like her ability to make electronic tones sound like they
          have so much character and movement. I would like to see how I could
          experiment with this in the future, maybe in the game I am designing
          the sound for.
          <br />
          <br />
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I just researched how to record good foley, taking
          inspiration from professional foley artists.
          <a href="https://www.youtube.com/watch?v=amNxmSVYc34">This</a> video
          explores how professionals get ready and set up for recording, with
          most of the work before the actual recording date. Whenever I look at
          doing foley recordings I always just grab a microphone and look at the
          list of things I need, which gets the job done, but I noticed not as
          well as I hoped. From this video I saw that they have a specific
          recording schedule, packed with information like what props will be
          needed, the duration needed, where the recording will be held, etc. In
          the future I want to be more organized with my shooting schedule,
          similar to this so I know what exactly I need without just winging it.
          <br />
          <br />
          For the actual recording they had a few helpful tips that I will use
          in my future foley recording sessions. I tend to just make generic
          sounds and manipulate it to fit the scene later, especially if it's
          something generic like clothes rustling or such, but I see the benefit
          in actually recording while watching the visuals. It seems like such a
          simple thing, but because I don't have a studio and I tend to record
          everything myself, I find it difficult to put a screen down where I
          can see the visuals of what I'm recording, but still holding up the
          microphone (since I don't own a stand) and also making the sound
          needed. In the future I will probably ask for some help, or at least
          borrow a mic stand to make it easier on myself.
          <br />
          <br />
          The video also talks a lot about how certain sounds can be scarily
          close to other sounds. The famous example of pan frying bacon sounding
          like rain is a prime example of this. I want to be more creative with
          my ideas, and not be such a purist when it comes to sounds, as I tend
          to like recording the exact same sound that is in the film, even if
          there could be an easier version.
        </p>
        <h3>Progress Report</h3>
        <p>
          I was very busy this week again with my flight and then had a trip to
          the emergency room (I am fine!) but I was able to get a draft down for
          the animation 'The Reconnect'. Caroline the animator sent me the first
          scene of her animation and sent me a little brief as well as what
          sounds she imagined for that scene.
          <br />
          <br />
          She mentioned footsteps playing a main role in the animation, so I
          started with that. It was a bit tedious to get the timing right but I
          got it in the end. I layered a clip of wood creaking, as well as
          isolated footsteps so the individual footsteps would be a bit clearer
          and wouldn't get drowned in the creaking noise. Since the setting of
          this scene was in a church, I did some pretty spacious reverb to get
          that wide sound, and put the wetness lower for the isolated footsteps,
          again to be a bit more clear. Since there were 2 pairs of footsteps, I
          made the closer ones with a bit less space and delay, so the listeners
          can differentiate them, as well as they are in a more walled off area
          than the other character.
          <br />
          <br />
          <img src="photos/week5/1.png" alt="" />
          <br />
          <br />
          I then went and layered a few horror ambiences, trying to focus on
          having the full frequency range in the scene. I did some low humming
          drones, a creepy airy atmosphere sound, as well as a high pitched
          chimes to add to that creepy atmosphere. I also tried to experiment
          with a sample of a choir singing, and pitching that around, but I'm
          not sure if that vision is what the animator wanted, so I've sent her
          varying sound design works to see which one she likes the best.
          <br />
          <br />
          <img src="photos/week5/2.png" alt="" />
          <br />
          <br />
          Finally I added some finishing touches! I added a heartbeat which I
          automated to get increasing louder and faster to help build tension. I
          also added a riser near the end to accompany the buildup of the rising
          heartbeat. Other than that, I added a few shaky breaths of both the
          person dying at the start, as well as another shaky breath when the
          girl sees the scene and gets scared.
          <br />
          <br />
          You can watch the first draft
          <a href="https://youtu.be/bs8I764QiyE">here</a>! I think a bit of the
          ambience is lacking, I think I could have made it a bit more
          expansive. Caroline mentioned she wanted it to be "witchy" but I am
          not sure how to achieve that so I will be sure to ask her then add it
          to the second draft! I'm excited to hear the feedback for this draft.
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 6 ---------------------------------------------- -->

    <details class="week" id="week-6">
      <summary>Week 6</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week in class we analysed the opening sequence of A Bug’s Life in
          three different times, one with effects only, music only, and then the
          combined track. Watching the film with only effects highlighted just
          how minimal yet purposeful the sound design is. The crickets, wind,
          and water effects built a naturalistic atmosphere, while footsteps
          were much heavier than I initially expected, maybe I expected it to be
          lighter because they were ants! That weight gave the characters a
          tangible presence and energy within the scene. I actually was shocked
          with how little effects were used. I alsways assumed there was more,
          but maybe because it is a children's animated film, it would have less
          than something recorded with real actors and such. I noticed how many
          environmental sounds faded away whenever the characters returned to
          focus, suggesting that hierarchy in the mix is used deliberately to
          guide attention.
          <br />
          <br />
          When listening to the music track separately, it revealed how much it
          shapes the emotional soundscape. There were stings placed on small
          visual actions, like the bass note hitting when the ant is struck by a
          falling fruit. At other times the score helped the flow of the
          narrative, for example when everything is calm, the melody was light
          and playful, but when problems emerged the bass slowed and thickened,
          and the music grew louder to match the threatening buzz of a chainsaw.
          <br />
          <br />
          Hearing both together showed how layers of sound work across the
          frequency spectrum. Music filled the higher registers while the foley
          sat lower, producing a more complete sense of space. What stood out
          most to me was how much character movement can be conveyed through
          sound alone, like their clumsiness, confidence, and awkwardness were
          all suggested simply by timing and emphasis. The exercise reminded me
          that sound is not just accompaniment to visuals, but a central force
          that defines mood, guides attention, and can even reshape how we
          interpret characters and their behaviors.
          <br />
          <br />
        </p>
        <h3>Academic Research</h3>
        <p>
          I read an article by Philipp Schmerheim and Tobias Kurwinkel titled
          "Sound Design in Children’s Film". It explains how sound in children’s
          media can play a crucial role in guiding perception, since younger
          audiences often rely on audio cues more than visual details to
          understand what is happening in scenes. The article pointed out how
          important it is to create a hierarchy of sounds so that key actions
          are easy to follow without overwhelming the listener. This perspective
          changed how I think about my own work, because I usually focus on
          adding detail and texture, but this showed me that sometimes clarity
          and restraint are more powerful, especially when designing for
          children. I can apply this to my own work even if the content I'm
          producing isn't really meant for children. I can do this by being more
          deliberate about which sounds need to be in the foreground, and which
          can fade back, ensuring that the main story always comes through
          clearly.
          <br />
          <br />
          I also read a scholarly reflection on sound design in contemporary
          animated films such as Coraline, The Incredibles and Bolt. The article
          compared how each film tailored its approach to sound design to suit
          its genre and mood. Coraline used minimal soundscapes to create a
          strange and unsettling atmosphere, while the other two relied on rich,
          layered mixes to heighten the action and energy in scenes. What stood
          out to me was the emphasis on clarity across all styles. This reminded
          me that sound design should never be about filling space, but about
          supporting the narrative with intention. I found this inspiring
          because it encouraged me to think more flexibly, and allowing a
          minimal approach to make a scene more effective, while at other times
          layering multiple sounds can build intensity. In both cases, clarity
          should remain the goal, and I want to carry that mindset into my
          future projects, especially with my tendency to add too many sounds.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For my artistic research, I decided to look into kids animated films
          again, and I looked into the work of Randy Thom, who has done
          extensive sound design for animated films including The Incredibles
          and How to Train Your Dragon (which is one of my favorite animated
          films of all time). Thom talks about how children’s films often
          require a very careful balance between realism and exaggeration
          assounds need to feel believable enough to anchor the world, but they
          also have to be expressive and playful. For example, he often layers
          real recordings with stylized or exaggerated effects to give everyday
          actions more personality I can use this for my own work by designing
          sound (especially if I ever do sound design for children's media) by
          finding the right degree of clarity and character, rather than aiming
          for complete realism. It made me realise that when I work on projects
          with younger audiences in mind, and even older audiences, I should
          think about sound as a way of storytelling on its own, where even
          small effects can reveal emotion or intention.
          <br />
          <br />
          I also decided to look into horror sound designers, and I found Gary
          Rydstrom, who has worked on films like Jurassic Park and A Quiet
          Place. Rydstrom describes horror sound design as being less about what
          is heard and more about what is implied through silences, distant
          rumbles, or subtle textures that can create more fear than overtly
          loud sounds. He also notes how horror often uses low frequencies and
          dynamic shifts in volume to keep audiences on edge, exploiting the
          body’s physical reactions to sound. This made me reflect on how
          different the goals of sound design can be across genres like where
          children’s films need clarity and guidance, horror thrives on
          ambiguity and tension. For my own projects, it pushes me to think
          about silence and restraint as creative tools, and not just the sounds
          themselves. By considering how audiences are meant to feel, I can
          adapt my design strategies to match the emotional core of the genre.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I watched a tutorial on the fundamentals of EQ and how it
          can be applied in sound design. The video explained how EQ isn't
          really just about boosting or cutting frequencies to make something
          sound better but about carving out space so each element in a mix can
          be heard clearly. It broke down the frequency spectrum into ranges,
          like sub-bass, low mids, high mids, and highs, and showed what
          typically lives in each area. It kind of reminded me when I created
          the sound design for Shelves by Gina Moore, when I was working with
          orchestra. All the different instruments focus on different frequency
          ranges to create this full sound! I just thought that was pretty cool
          and linked together something I've done before to something I focused
          learning this week.
          <br />
          <br />
          I found it especially useful when he demonstrated how too much low-mid
          energy can make a sound muddy, while carefully reducing those
          frequencies can suddenly make a mix feel clearer. Reflecting on this,
          I realised how often I tend to layer sounds without thinking about how
          they overlap sonically, which can cause clutter. EQ gives me the
          technical ability to separate those layers, making each one
          purposeful. In my own projects, I definitely can see myself using EQ
          to sculpt sounds to sit in their own space and highlighting the
          qualities that make them expressive. I will definitely try this next
          week for The Hollow Child draft.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week I mainly did work on The Hollow Child animation! I had a
          chat with the animator Tara in person, and we went scene by scene of
          what her plans were, what she expected and gave us a timeline to have
          the draft done next week! Which was kind of really quick for me which
          is why this week I mainly only did this project.
          <br />
          <br />
          Tara had hired voice actors for the different characters and sent us
          the voice recordings for it, so the first thing I did was do the
          timing for all the laughs and sounds for the ghost girl:
          <br />
          <br />
          <img src="photos/week6/girl.png" alt="" />
          <br />
          <br />
          There wasn't any actual dialogue or speaking, but there was a lot of
          laughing and random sounds, so I really focused on making sure I
          didn't miss anything. I then did the same for the werewolf:
          <br />
          <br />
          <img src="photos/week6/werewolf2.png" alt="" />
          <br />
          <br />
          It was pretty difficult in general because obviously the animation
          isn't finished, so I don't think the timing will be correct, but it is
          an easy fix and just a matter of moving the timings around once the
          animation is finished! These steps took me a surprisingly long time,
          especially listening to the MINUTES of each characters' different
          sounds and organizing them based on what it sounded like to me/what I
          will use it for, and then trying to find ones that fit the scene. I
          would have liked for more sounds from the werewolf voice actor, so I
          might ask if that is possible for the final sound design.
          <br />
          <br />
          I then worked on the intro, trying my best to create a lonely
          atmosphere, and focusing on the breaths of the werewolf, as well as
          the wind. I used reverb to achieve this:
          <br />
          <br />
          <img src="photos/week6/reverb.png" alt="" />
          <br />
          <br />
          And I automated this reverb so when the camera zoomed out on the
          werewolf laying down, the size and dry/wet ratio would go higher to
          get that sense of vastness. I also made it so at the start, the breath
          and sounds were mono, and as the reverb automates to get larger, the
          stereo width gets wider!
          <br />
          <br />
          <img src="photos/week6/automation.png" alt="" />
          <br />
          <br />
          I then made a reverb bus, and put all my tracks to have varying levels
          of reverb, including the girl's and werewolf's sounds for later in the
          animation. After this I also added some diegetic sound like crows, and
          birds chirping, as well as wind and grass/leaves rustling. This set me
          up to create the ambience and non-diegetic sounds next week!
          <br />
          <br />
          Other than working on The Hollow Child, I also followed up with my
          other projects, and am waiting to get more scenes to do sound design
          for The Reconnect! The animator said she would get them to me by the
          end of the week! I also started to master and mix some of my song that
          I made, as well as finalized the structure since I wasn't a fan of it
          before! I want to add a guitar solo to it!
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 7 ---------------------------------------------- -->

    <details class="week" id="week-7">
      <summary>Week 7</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This weeks class mainly centred around our project presentations,
          which provided a valuable checkpoint to evaluate our progress so far
          and make sure everything was on track. I also enjoyed looking at
          everyone else's projects and the sounds they made! Preparing the
          slides helped me organize my ideas clearly, as well as remind me and
          allowed me to reflect how each project connects to the goals of each
          of my collaborators. I aimed to make the presentation informative but
          concise to show both creative and technical development, but keep the
          attention of everyone watching. I also wanted to leave space for
          feedback from everyone in the class, since even if something is my
          intention, doesn't mean that it comes out that way.
          <br />
          <br />
          The feedback that I received was extremely helpful! For The Hollow
          CHild, the comments I got highlighted how important the final sequence
          will be to the emotional impact of the animation. Even though the
          current ending music was only a placeholder, it just reminded me that
          the final moments need to feel cohesive and powerful, and Maria was
          present at the feedback session so she is aware of it as well. I was
          also advised that the wind ambience throughout the scene felt too
          static, which made is sound dull and unalive. This was useful to hear
          because it encourages me to revisit the spacilization and dynamics of
          the wind, perhaps by layering subtle movements, low frequency swells
          or maybe modulation effects to create more presence and variation.
          Someone in the class suggested using EQ to do this!
          <br />
          <br />
          For Fired Before Hired, I received feedback on the menu music, mainly
          suggesting to add a small amount of reverb, especially to the horns to
          make the mix sound less dry and out of place. This was an easy but
          meaningful adjustment and it made me realize how much space and
          ambience can affect the tone, even if I just changed one tihng.
          Overall, the presentation week was an important oportunity to pause
          and assess my direction and reminded me the value of feedback and
          could help me stage the next phase of refinement!
        </p>
        <h3>Academic Research</h3>
        <p>
          While developing the Fired Before Hired menu music, I wanted to
          capture the relaxing qualities of bossa nova that is often seen in
          office music/elevator music! I read an article by Perrone 'Masters of
          Contemporary Brazilian Song' that talks about bossa nova being defined
          by its rhythms, jazz harmonies and instrumental delivery to create a
          sense of calm intimacy that contrarsts the energy of samba. The genre
          often relies on a classical nylon string guitar with more subtle
          percussions and chord extensions that add harmonics like 7ths or 9ths.
          This understanding helped me understand what I wanted in my own sound,
          using a guitar loop as a base and a vibraphone to create an airy
          texture. Bossa nova highlighting balance and restraint made me think
          carefully on how to make things expressive quietly, which was the key
          to making the loop feel natural and calming for players!
          <br />
          <br />
          I also researched into menu music, specifically in game design and how
          it sets the tone and guides the players emotion going into the game.
          Playing with Sound: A Theory of Interacting with Sound and Music in
          Video Games by Collins explains that menu music functions as a form of
          emotinoal priming, establishing the atmosphere and expectation before
          the gameplay actually begins. The most effective themes use loopable
          strucutres and a moderate tempo. This framework is something I wanted
          to explore with Fired Before Hired, as it appears before the intense
          gameplay, so I wanted something more calm to contrast it. I want to
          apply a looping piece that ensures endless playback because we won't
          know how long someone would stay in the menu screen. This helped me
          refine how I wanted to arrange the menu music, but also gave me an
          insight of how the sonic design can contribute to user experience!
        </p>
        <h3>Artistic Research</h3>
        <p>
          While researching examples of menu music, I went back to one of my
          favorite games growing up Life is Strange! Every time I hear that menu
          music I get hit with a wave of nostalgia, and its one of the only
          games where I sit and listen to the menu music for a along time. I
          wanted to see how and why it was so effective. Life is Strange uses
          sound to reflect its emotional tone, the main menu theme composed by
          the band Syd Matters, establishes a reflective and nostalgic
          atmosphere through its use of acoustic guitar, ambient pads and soft
          reverb. The track feels so intimate and melancholic, which mirrors the
          game's focus on memory, choice and adolescnece, which is why I think
          that this menu music is so compelling. It takes users into this calm
          space that aligns with the narratives emotional tone and also loops
          infinitely, which maintains immersion! I love this theme music so much
          (and side note I am very excited for the show they are making) and it
          helped me realize that I don't need complex melodies and instruments,
          but just subtle mood could be just as powerful.
          <br />
          <br />
          Another game I particularly enjoyed the main menu sound design was
          Detroit: Become Human, and achieves a very different effect. The story
          focuses around robots and androids, and the main menu features this
          girl that talks to you alongside the story, and changes what she says
          based on your choices. I thought this was extremely cool and also
          creepy and perfectly fit what the story was about! It was a creative
          way that a main menu can directly manipulate the player's emotions
          going into the actual game. In terms of the actual music, it uses
          orchestral strings and also very airy, ethereal vocals to convey a
          sense of future and moral tension. The piece slowly builds and also
          changes depending on the player's progress, for example when the story
          reaches darker points, the music shifts in tonality and
          instrumentation. This adaptiveness adds depth to the player's
          emotinoal engagement, especially when paired alongside the main menu
          android!
          <br />
          <br />
          Another strong example is Firewatch, where its menu music combines
          warm acoustic guitar and ambient drones to convey this sense of
          solitute and quiet, especially since that the game is set in a quiet
          forest and is about human connection and isolation. The track fades
          into the background but creates emotion through its harmonic structure
          and it demonstrates how environmental and emotional themes can be
          translated into sound through its texture and pacing, and not just the
          melody. Good exampels of main menu music helps build the world through
          sound and I want Fire Before Hired's menu music to feel appropriate
          for its statirical and comedic nature, but also using bossa nova to
          mirror the game's light heartedness.
        </p>
        <h3>Technical Research</h3>
        <p>
          To create the menu music, I watched this YouTube tutorial,
          <a href="https://www.youtube.com/watch?v=MDEzMTQXAs8"
            >How To Make Elevator Music</a
          >. Despite its 2 minute runtime it provided such a clear and practical
          overview on the stylistic and technical choices that define this
          genre. Although it only really goes into bossa nova as elevator music,
          now elevator music as whole. Anyways, this video breaks down how
          elevator and lounge musicc can typically blend elements of bossa nova,
          easy listening and jazz harmony and using a relaxed swing for a light
          instrumentation. Bossa nova works so effectively in this context
          because of its subtle sound, smooth chord progression and a
          non-intrusive melodic phrasing. It talks about layering a range of
          insturments like soft percussion, electric piano, brushed drums and a
          vibraphone to create a rich textural environment without overwhellming
          the listener.
          <br />
          <br />
          Watching this helped me think more critically about how I would make
          my own arrangement for Fired Before Hired. I realized that the menu
          music I wanted to create was a catchy, melodic tune but more
          importantly, a comfortable sound that can repeat endlessly without
          feeling annoying.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week included a lot because I'm also counting the break week! I
          was also thinking of dropping the music video project because things
          have come up unexpectedly and I still believe that I can hit the 100
          hour mark even without it, considering how much work I've done so far.
          This was a mix between the videographer pulling out, and me being
          unable to do a Pharos project because of how late I was given notice.
          If I have time, I might do the music video myself since I already the
          song worked on, but it really depends on time in regards to making the
          music video myself.
          <br />
          <br />
          For the projects I am continuing to pursue, I mainly did a lot of work
          for The Hollow Child since the deadline was closer compared to the
          other projects.
          <br />
          <br />
          I then started to make the ambience for The Hollow Child. I was having
          a bit of trouble because I was meant to create a forest soundscape,
          but empty, so no animal sounds, no rain, etc. I decided to just add
          wind, and also wind chimes to create a musical and creepy sound
          effect, which the animator liked! I used automation and EQ and pitch
          shifting to make the chimes less predictable and different each time
          it plays.
          <br />
          <br />
          <img src="photos/week7/7.2.png" alt="" />
          <br />
          <br />
          I added a lot of drones to add this feeling of dread when the werewolf
          looks around, especially when he is hearing the laughs in the distance
          , but hasn't seen the girl yet. I tend to go towards the deeper and
          lower sounds because I know that Maria is going higher for the music
          effects, and I want don't want to muddle her sounds. I did the same
          droning and impact when the werewolf sees the dead animals, and he
          starts to realize who and what the ghost is.
          <br />
          <br />
          <img src="photos/week7/7.1.png" alt="" />
          <br />
          <br />
          When the girl gives the werewolf a gift of the dead rabbit, I thought
          it would be cool to echo the laughs to create a creepy atmosphere. I
          was also thinking of doing a high pitched sound, like the effect of
          when you hit your head, but I decided to see what Maria does for the
          music, and if it will be too much if I add it in. Instead, I did a
          heartbeat, but I also wasn't sure if the werewolf guy would even have
          a heartbeat, so I need to double check with the animator.
          <br />
          <br />
          For the end sequence, it is very music heavy so I just added a
          placeholder song for now while I wait for Maria's track. I tried to
          make it very sparkly and big, but I do think it needs more, but I
          think I will experiment around with it once I have the song.
          <br />
          <br />
          <img src="photos/week7/7.3.png" alt="" />
          <br />
          <br />
          So overall, I just basically added all of the diegetic and
          non-diegetic sound, not including the music. In the coming weeks I aim
          to do the automation and refinements as the animator sends in more of
          the finished parts, so I can properly line up the sounds.
          <br />
          <br />
          You can watch it here:
          <a
            href="https://www.youtube.com/watch?v=JH6n9ncHDc8&list=PLvt9uDYuOQg7i28uXG7PKJC8Hk7bAglt2&index=4"
            >https://www.youtube.com/watch?v=JH6n9ncHDc8&list=PLvt9uDYuOQg7i28uXG7PKJC8Hk7bAglt2&index=4</a
          >
          <br />
          <br />
          I also started to do work on Fired Before Hired, the game I am doing
          the sound design for. I started with doing the menu music! I gave the
          main game designer a few different clips of different videos to see
          what his vision of what "Office Music" sounded like. This ranged from
          jazz, elevator music, waiting room music, etc. What resonated with him
          the most actually ended up being this waiting room music that had a
          bossa nova vibe to it, so that was the direction I wanted to head
          towards.
          <br />
          <br />
          I usually think of either a soft electric piano or a classical guitar
          for the main chords of a bossa nova song, so I started with looking
          for classical guitar loops on Splice. I would have done it myself but
          I actually don't have a very good mic to capture the sound of a guitar
          and not a lot of quiet areas in my house, and most of the recordings
          of guitar I do are through an audio interface with an electric guitar.
          <br />
          <br />
          Anyways, I found this classical guitar loop playing different chords
          in D minor and I loved it immediately! The artist actually played a
          piano version but I liked the guitar so much more. I imported that
          into Ableton and got my MIDI keyboard and used a soft electric piano
          sound and played around with what chords I wanted to play on top of
          it. I put a lot of focus into making it very soft, so it supports the
          guitar in the background, rather than trying to overpower it because I
          wanted the guitar to be the main focus. I just did a basic 4 chord
          structure and looped it.
          <br />
          <br />
          Then after that I noodled around with a vibraphone and came up with a
          simple melody. Since I was aiming for kind of elevator music, I
          thought it'd be fitting to go up and down the notes in the key and the
          simplicity helps it pass as like waiting room/office music.
          <br />
          <br />
          I thought it needed some percussion so I spent quite a while trying to
          search for one that fit the vibe, and I eventually found one! I added
          some maraccas and a tambourine as well to make it fit better with the
          waiting room theme, the percussion was very light and friendly and
          playful, which was exactly the vibe I was going for! Everything I just
          talked about can be seen here
          <br />
          <br />
          <img src="photos/week7/1.png" alt="" />
          <br />
          <br />
          While I was seacrhing for the percussion, I also found this horn
          sample that I really liked. I was drawn to it for some reason. I added
          it in and asked the game developer if he liked it and he actually
          really liked it and said it was "fire" (his words)! Even though its
          not typical for waiting room music or anything, I think it added to
          the jazzy bossa nova vibe and gave the sound a bit of texture.
          <br />
          <br />
          After this I then started cutting up everything into 3 sections; the
          intro, main loop, and end. Since it is a song for a game, I need to be
          able to have 3 clips of it so I could code that the intro plays, then
          straight after the main loop plays for however long, and then it ends.
          For the intro, I took an open high hat sound and reversed it to work
          as a riser for the start of the percussion part. You can see me do
          this here:
          <br />
          <br />
          <img src="photos/week7/2.png" alt="" />
          <br />
          <br />
          Now that I had all of the parts done, it was just a matter of actually
          assembling the song. I didn't want all of them to play at the same
          time since it makes for a very flat song, theres no variation and
          won't be very interesting. I decided that the main rhythm, which was
          the classical guitar actually doing the bossa nova, would play
          throughout, and the rest would change/get cut off, etc.
          <br />
          <br />
          When the horns were palying, I felt like there was a lot going on, so
          I decided for a part of it, the percussion would stop, highlighting
          the horns and the rhythm, then come back. This part turned out to be
          my favorite part of the entire menu music song! After the horns, I
          felt like the main melody was just being repeated, so I kind of wanted
          to add like a 'bridge' or a B section, but wanted to keep it simple as
          it was just a short loopable song. I just decided to do a little play
          around with the melody, so at least there was some melodic difference
          between that section, and the rest of the song!
          <br />
          <br />
          Then that was done for the menu music! I exported a version where it
          was the intro, main loop twice, then outro so I could gather feedback
          with the song as a whole. The developer absolutely loved it so I'm
          very happy. You can see all the final parts here (I removed the horns
          for some of the parts, but the screenshot is outdated, but everything
          else is right):
          <br />
          <br />
          <img src="photos/week7/3.png" alt="" />
          <br />
          <br />
          You can listen to it here:
          <a href="https://youtu.be/SjpkX_O2ez0"
            >https://youtu.be/SjpkX_O2ez0</a
          >
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 8 ---------------------------------------------- -->

    <details class="week" id="week-8">
      <summary>Week 8</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          Although I wasn't able to attend class this week, but I still explored
          the weekly theme of jingles through my own independent research! I was
          curious about what made such short pieces of music so memorable, and
          what makes them effective as a form of communication. Jingles are
          interesting because it is not only just sound/music but also branding.
          It needs to delivery identity , tone and emotion with just a few
          seconds. Every note and sound choice has to carry meaning.
          <br />
          <br />
          While researching I realized that succesful jingles share three main
          traits: clarity, repoition and revognizability. Jingles usually rely
          on simple melodic intervals and hooks to create a logo with audio.
          Even though I am not creating any jingles for any prohects, something
          linked in my head about how sound can establish identity in other
          contexts such as games, animations, etc. It can embody a character or
          world, which is something that I was talking to Maria about for The
          Hollow Child, we wanted to create a melodic motif that plays when the
          ghost appears! I realize it is a similar thing!
          <br />
          <br />
          I also started thinking about how certain melodic shapes or timbres
          can become associated with emotion or nostalgia! It reminded me that
          people can respond to sound subconsciously and that motifs can carry
          emotional weight when used purposefully. I think it made me solidify
          my decision to ask for motifs for the ghost! Overall, exploring
          jingles gave me a renewed appreciation for sound design!
        </p>
        <h3>Academic Research</h3>
        <p>
          For some academic research, as I mentioned in my reflection, i
          explored jingles and how they are effective and memorable. 'Effects of
          popular music in advertizing on attention and memory' by Allan talks
          about the interaction between melody, rhythm and verbal phrasing to
          form an identity, as I mentioned earlier. It also highlights
          repitition and simplicity to allow the jingle to stick in the
          listeners mind even after just one single listen. It is also important
          to make sure it matches the brand's mood and creating a clear tone
          within seconds!
          <br />
          <br />
          Building on that, I also researched how to create music that loops
          seamlessly, especially for the Fired Before Hired wave system that I
          am working on! It was talked about one of the previous articles I read
          in one of the other weeks, Collins explains that loopable game music
          relies on modular composition, and that musical phrases are written to
          resolve back into themselves so the listenrs don't perceive an
          endpoin. She also describes how transitions can be managed through
          layer and dynamic mixing rather than hard cuts. This informed my
          decision and also lined with what Santino said I should do for the
          wave music! A good loop design depends on the phrasing and rhythm just
          as much as the intensity stages!
          <br />
          <br />
          I also looked into h ow to build musical intensity and tension, since
          the wave music needed to escalate without overwhelming the listener.
          'Writing Interactive Music for Video Games: A Composer's Guide'
          discusses how intensity is often achieved through one or more of these
          points; density, contrast, acceleration. Rather than just increasing
          volume or adding more instruments, the variation of articulation like
          staccato vs legato and layering is more important for listner
          engagement! This helped me refine how I approached my wave
          progression. Instead of making each section louder, I should focus on
          introducing subtle rhythmic and instrumental changes to create motion
          and movement I want to make the viewers feel the escalation rather
          than hear it mechanically.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For artistic research, I looekd into Gordon's work on DOOM, which is
          one of the most striking examples of sound design and structure
          creating physical and emotional intensity. His process is what he
          calls 'dynamic layering' where he records multiple different stems of
          instruments and sounds that can be mixed interactively based on player
          behavior! Which I thought was pretty cool. In interviews he mentioned
          using aggressive compression and distortion to create a wall of sound,
          but still highlight harmonic movement underneath that wall of sound. I
          want to emulate that with my wave music for Fired Before Hired, a big
          wall of sound with a layer of melodic elements so its still tightly
          knit together. It encourages me to make and think about intensity as
          storytelling rather than just volume.
          <br />
          <br />
          I also wanted to explore a simpler example of loopable music, and
          looked into Super Mario Bros themes to show how simplicity and
          repitition can create a different kind of intensity. Kondo, the
          composer, has spoken about designing his music to feel part of the
          gameplay rhythm, matching the players jumps and movement and timing.
          Relying on tight phrasing and small melodic loops that feels
          satisfying to hear so the user wouldn't get annoyed if it played
          endlessly. He also added subte variations to keep it fresh and less
          repetitive! Which I think would be cool if I were making a longer
          game, and is important to keep in mind.
        </p>
        <h3>Technical Research</h3>
        <p>
          For this weeks technical research, I watched this short tutorial that
          explained how to consutrct cinematic loops that build tension without
          obvious start or end points. The key point was just creating a perfect
          point and compositional balance and ensuring that every bar resolves
          naturally into the next. Some points that were brought up were ending
          phrases on suspended or unresolved chords so that people naturally
          want to hear the next chord, which is the start of the bar. Using
          motifs could also maintain that forward momentum, This helped me
          rethink how to structure each of my waves so that the transitions
          between intensity levels would feel organic when stacked.
          <br />
          <br />
          Another technical aspect I was focused on was autmoation and dynamic
          mixing. I watched another YouTube video about advanced automation in
          Ableton! This could help me create the illusion of energy building
          even when the core tempo or melody remains constant. Things such as
          reverb, delay feedback and volume! It makes me think of using low
          frequency EQ automation and slowly opening filters over time to reveal
          more of the low end instruments to make this tonal widening to make
          the feel like it was expanding outward.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week, I started creating the wave music for Fired Before Hired.
          This was difficult at first because it was meant to go up in intensity
          as the players finished each wave, but obviously everyone is not going
          to finish the same wave at the exact same time, so I had to figure out
          a way to make sure the music doesn't suddenly stop and start in a
          different place, and to keep the immersion of the player. I was
          talking to Santino and he gave me the idea to make sure everything was
          a loop. After that I could export each one as a wav file, and then
          mute and unmute the tracks as needed with coding in Unity, which I
          thought was a great idea!
          <br />
          <br />
          I began searched for different loops and samples on Splice to see what
          I could use as my base, and build it up from there. The game designer
          mentioned that it would be funny if the wave music was super serious
          and orchestral and big, because it severely contrasts the visuals.
          Since the game was satirical and comedic, I thought it fit the vibe
          and decided to go towards that direction, and found this super cool
          strings to use as a starting point. I duplicated that 10 times so I
          could have a visual layout of what happens at each stage, the final
          product will just all be one loop stacked on top of each other
          <br />
          <br />
          <img src="photos/week8/8.1.png" alt="" />
          <br />
          <br />
          I then went to find some shakers for a lighter percussion that I
          thought could be used at the start, which will change to a bigger one
          when the player gets into the more intense levels.
          <br />
          <br />
          I also recorded a cello playing the bass note and letting it ring out
          basically the entire song. I wanted to have each part pretty simple
          because I knew that by the end, if I made each part too complex it
          would definitely be too busy, and take away from what I wanted. I then
          also added a melody and counter melody (stuff that I learned when
          doing Gina Moore's Shelves!) with a violin, which I changed to piccolo
          later on!
          <br />
          <br />
          <img src="photos/week8/8.3.png" alt="" />
          <br />
          <br />
          I thought it would be cool to add like a DUN DUN (I don't know how
          else to describe it) to help build up anticipation and build tension.
          ALongside that I added another cello to add a lower bass that wasn't
          just playing the root note the entire time.
          <br />
          <br />
          <img src="photos/week8/8.2.png" alt="" />
          <br />
          <br />
          At this time I also decided to add the more intense drums, I found
          different percussion samples that I thought fit the vibe I was going
          for, and also had varying levels of intensity, so I used that to help
          build up the intensity for each wave. This worked exptremely well and
          I don't think that the final product would have been as intense if I
          didn't decide to do this.
          <br />
          <br />
          <img src="photos/week8/8.4.png" alt="" />
          <br />
          <br />
          From this it was just a matter of finding smaller things to add for
          each section. This ranged from finding different horns with varying
          intensities, and I even found another violin sample with a cool
          melody, and layered that on top of the exisiting one. I even added
          some piano in there that added more rhythm to the entire sound.
          <br />
          <br />
          <img src="photos/week8/8.5.png" alt="" />
          <br />
          <br />
          And that was basically done! I sent it off to the game designer for
          some feedback and am waiting to hear from him regarding that. Each
          section didn't have to be very long since when I was playing the game,
          I pretty much only spent around 10 seconds in each wave, but I also
          have a lot of experience playing FPS games and such, so I had to keep
          in mind that other people may spend longer.
          <br />
          <br />
          You can watch the full thing here:
          <a href="https://youtu.be/zWbvsTAuDr0"
            >https://youtu.be/zWbvsTAuDr0</a
          >
          <br />
          <br />
          Other than working on this wave music, I also started to refine The
          Hollow Child, because the animator reached out with more finished
          parts of the animation. All the work I did last week was all on top of
          the draft, and I let the finished animations build up a bit more
          before I started to refine, so I had a lot to catch up on! ALthough
          this basically just consisted of making sure all the timing matched up
          to the actual animation instead of the draft. I didn't add any new
          sound, just moved things around in the timeline and did some
          automation and a tiny bit of FX editing, but not that much.
          <br />
          <br />
          I don't want to do much more refining work on The Hollow Child until
          Maria sends through the music part, so I know how much I should do for
          foley, as I think there are a few parts I know I'll have to cut out
          because Maria is making the music kind of like ambience for some
          parts.
          <br />
          <br />
          This week I also did the basic ambiences for the scenes I received for
          The Reconnect! I still didn't have the final narration or the final
          sequence, so I just added sound to create the atmosphere that I was
          going for.
          <br />
          <br />
          For the classroom scene, I was having trouble figuring out what I
          wanted to do for atmosphere. I decided to just focus on the ticking
          clock, which I automated to get lower in volume because I wanted to
          highlight the rest of the sound layer on too. I did some foley for
          when the chair/desk moves and the clothes rustling for realism. For
          the atmosphere I found this sound clip of people from a school talking
          and I fiddled around with the EQ (taking away the high end) to make it
          sound like it was muddled and coming from outside. I also added a
          basic room tone to fill in the empty space and to give it this kind of
          boring atmosphere to contrast the next scene.
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144221.png" alt="" />
          <br />
          <br />
          Then it goes into an interrogation scene, switching between the
          protagonist and antagonist, and in the police station. The animator
          told me to keep the same creepy ambience throughout both, and there is
          a narration throughout the entire thing. I started off with this
          creepy cave ambience I found, and mixed that with this sample of a
          violin bow making creepy high pitched sound. I then added another
          sample of deep footsteps to give the antagonist this powerful
          prescence. Mixed that with risers, booms and impact for the scene that
          reveals who the antagonist is to build up the tension and a big
          release for one of the bigger parts of the animation.
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144235.png" alt="" />
          <br />
          <br />
          From then it was the last scene until the final sequence. As a
          reminder the animator friend is doing the music at the end! So this is
          the buildup before then. I used a mix of recorded ambiences and from
          sample and sound libraries. I added a lot near the end, with a low
          drone and a high wind to build and overwhelm the viewer before the
          final riser and release. I also added a fire sound because the girl
          touches a candle that turned out to be hot, which I also added sound
          of the girl wincing to!
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144302.png" alt="" />
          <br />
          <br />
          That is just the general ambience of the vibe I wanted to go for, I
          really am just waiting for the narration, as I mentioned in the
          progress presentation! You can watch it here:
          <a href="https://youtu.be/IUlOBy_d0Tg"
            >https://youtu.be/IUlOBy_d0Tg</a
          >
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 9 ---------------------------------------------- -->

    <details class="week" id="week-9">
      <summary>Week 9</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This class was about the relationship between sound design and music
          in action film sequences and decided to do a bit of my own research
          myself and how these two layers interact to shape tension and pacing.
          Action sequences often rely of precise syncing between the diegetic
          and non-diegetic sound, things like footsteps, impacts, gunfire, etc
          typically merge with percussive elements in the score to create a
          sense of momentum. What stoof out tome was how the most effective
          sequences don't actually seperate sound and music, but it blurs
          together into one cohesive texutre and soundscape. This connect also
          made me think about how I could apply strategies to my own work,
          especially the animations I am working onm particularly The Hollow
          Child. I am definitely going to bring it up to Maria to see how we
          could try and implement it!
          <br />
          <br />
          This week I also received some detailed feedback on The Hollow Child
          from the animator, which helped me see new possibilities for
          integrating sound and music more effectively. I was encouraged to
          expierment with panning when the ghost moves across or off-screen to
          give a even stronger sense of movement and spatial depth! This was
          something that I was planning on doing, but haven't yet while I wait
          for the music to be sent to me to make sure my refinements are
          cohesive with the music. There was also a suggestion to include
          quieter, more minimal moments to heighten tension, basically using
          silence as a narrative device to put the audience on edge. I found
          this advice valuable as I realize I tend to fill space with sound, but
          I realize that restraint can make the next sound hit even harder. I
          was also advised to refine the ambience, maybe experimening with
          adding textures like winter winds, creaking trees, rustling, etc. to
          evoke the setting more vividly. Finally, I got feedback to vary the
          ghost's laughter, beginning subtly and letting it built to a peak
          before dying back down again, creating a clearer emotinoal arc and
          avoids fatigue from the repititon.
          <br />
          <br />
          Overall this week just helped me think about rhythm and restraint in
          how all audio elements flow together, making sure each sound has
          intention and contributes to the large scale of things rather than
          just filling space.
        </p>
        <h3>Academic Research</h3>
        <p>
          Most of this weeks work revolved around Foley so I explored advanced
          Foley editing and returned back to the article I read about before by
          Sonnenschein. He discusses how doley is not just about realism, but
          expression, using subtle editing, layering and processing to reflect
          the tone of the scene. He explains that advanced foley editing often
          involves maniplating frequency content, timing and reverb! Which are
          all things I have researched before! For example, high frequencies
          could be softed to simulate distance, or maybe attacks shortened to
          blend with surrounding sounds. This could be especially useful in the
          footsteps for The Disconnect, applying EQ automation, pitch shifting,
          etc.
          <br />
          <br />
          I also looked into foley techniques that are specifically adapted for
          interactive media and games. The article I read explains that Foley in
          games requires both variation and responsiveness because players can
          often trigger the same sound repeatedly. She describes how recording
          multiple versions of each effect can add randomization, preventing
          listener fatigue and enhances immersion. Also implementing
          randomization in the playback parameters is also another way to
          introduce this. I will record several takes for all the foley I
          design. The author also notes that foley for games often prioritize
          clarity over realism, making sure that the sound communicates what is
          needed, even if it is stylized.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For the research, I was trying to find some cool sound designers. I
          found Randy Thom, a film sound designer and he emphasizes that sound
          should participate in the storytelling, not just decorate it.
          Recording unconventional materials to acheive an emotional timbre,
          like using layers of wind, animal groans, etc to blur the boundary
          between just ambience and also emotion. What I found interesting was
          Thom's belieg that every sound should reflect the psychology of the
          scene. Reminds me that my work, especially in The Disconnect should
          reflect the protagonist's feat and isolation, maybe by adjusting
          reverb or spatial depth. I can create sound that feels like part of
          her perception, rather than just the physical space itself.
          <br />
          <br />
          Schappler, the lead sound designer that worked on The Last of Us Part
          2, explains how his team used foley to emphasize vulnerability and
          presence, capturing the texture of skin, fabric, etc, all in extreme
          detail. The footsteps and breaths and foley were recorded with
          multiple mic perspectives to allow for dynamic layering depending on
          how the player moves and the camera distance. This was pretty cool to
          hear, as he designed sounds based on contextual realism, recording
          sounds that could flexibly adapt to gameplay without losing his
          vision. This informs my decision of recording multiple takes of the
          same thing to make sure it isn't repetitive.
          <br />
          <br />
          Both Thom and Schappler show how foley can function as a bridge
          between the person and the world, and can reveal their emotional state
          and not just used as a grounding tool.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week, I focused on learning how to implement sound into Unity
          without relying on middle like FMOD or Wwise. Since this is a
          completely new skill it took be a bit to grasp, but I eventually do
          want to learn how to use middlware! Anyways I watched a tutorial which
          explained the workflow for importing, triggering, and controlling
          audio using Unity's built-in AudioSource and AudioClip components. I
          briefly used this last year for another class, but nothing this
          extensively. The vieo demonstrated how to assign individual sound
          effects to player actions, and how to script logic that ensures that
          sounds pnly play when certain conditions are met, for example,
          detecting when a player si grounded before triggering a step sound
          <br />
          <br />
          What I found most valuable was learning about randomisation and
          variation using code. It showed me how to create an array of multiple
          sound clips then use a simple script to random select one each time an
          action is triggered. He also explained adding the subtle pitch and
          volume variation in the script to make repeated sounds feel more
          natural, which is exactly what I wanted to do!
          <br />
          <br />
          Another important aspect that was taught was understanding Unity's
          spatial audio system, which allows sounds to feel closer or farther
          away depending on the player's positino. Adjusting the 3D sound
          settings, specifically rolloff curve and spatial blend, I will be able
          to create depth for foley sounds!
          <br />
          <br />
          Lastly it talks about the importance of normalizing audio elvels and
          compressing files BEFORE importing, to prevent inconsistencies between
          different recordings and that I should take more time than I think I
          should when importing and editing the files!
        </p>
        <h3>Progress Report</h3>
        <p>
          This week I worked on The Disconnect! I think most of this blog I was
          calling it The Reconnect, but I just revisited the brief, and saw that
          it was the Disconnect... Anyways, she finally sent through the final
          draft with all the timing and scenes, which I found out was actually
          the same thing as I had before, I just had no idea because I didn't
          have the narration.
          <br />
          <br />
          The animator also sent through the narration! Which is also what I was
          waiting for because the animation is very narration heavy. I solo'ed
          the track and used automation to make sure everything was around the
          same volume, as some parts were peaking, and others it was too quiet.
          Overall I think the volume in the entire narration is too low, and I
          mentioned this to the animator and she said she would send me one with
          higher volume because she was the one who put it lower, so I am just
          waiting on that!
          <br />
          <br />
          When putting all the scenes I had together and made the sound for, I
          wasn't really a fan of how everything flowed together, because again,
          I didn't know the sequence of the scenes. So I actually ended up
          starting from scratch.
          <br />
          <br />
          I began with the foley, and this time I spent more time making sure
          everything was lined up, because I felt that on the first draft, it
          was pretty out of time. I couldn't find any sound samples or sound in
          libraries that had exactly what I wanted, so I actually recorded some
          of the footsteps and foley myself! I didn't have access to the
          professional equipment when I decided this, so I actually decided to
          just use my phone which I think didn't work too bad! I edited the
          sound in audacity to cut out the white noise and other stuff I didn't
          want, then just used EQ inside Ableton to get the depth and pitch I
          wanted.
          <br />
          <br />
          I was doing the foley around the narration and made sure nothing too
          loud was happening during the narration, which I think the animator
          was also doing because a lot of the events happen during the silences,
          like the scene where the main girl gets hit, etc. Because of this, I
          was really able to emphasise the important parts which I thought was
          good!
          <br />
          <br />
          There wasn't much foley to do besides the footsteps to be honest. I
          made sure to use different kinds of shoes for each character, so they
          had a more distinct footstep, and used automation and pitch shifting
          to make each step sound lighter, heavier, etc. For the first scene, I
          also added a layered a bunch of creaking noises and used EQ to
          highlight different aspects, but also made it echo in a creepy way! I
          also kept the original shaky breaths of the girl at the end that I had
          in my first draft.
          <br />
          <br />
          <img src="photos/week9/9.1.png" alt="" />
          <br />
          <br />
          For the classroom scene I think I kept everything exactly the same as
          the original draft, but just changed the timing based on the changed
          animation. As you can see here:
          <br />
          <br />
          <img src="photos/week9/9.2.png" alt="" />
          <br />
          <br />
          The interrogation scene changed quite a bit, so the ambience that I
          will do next week will change quite a bit from the draft. For now, I
          just did the footsteps to match what was happening on screen, as well
          as clothes rustling when the character moves around. I also added a
          sound effect for the slap/hit that happens, which I exaggerated quite
          a bit.
          <br />
          <br />
          <img src="photos/week9/9.3.png" alt="" />
          <br />
          <br />
          Then for the last scene, I also again just did the footsteps as the
          character walks up. When watching this I had a lot of ideas for the
          ambience so I am excited to start that next week! The original draft
          had the girl touching a candle, but I think it changes to her being
          scared of her reflection in the mirror, so I also changed the timing
          for that. It's hard to see exactly whats happening since the entire
          thing hasn't been rendered, but I'm excited to see the final product,
          because I already get a glimpse of what it will look like because of
          the end scene. I also then added a heartbeat (which I wasn't sure if
          it is diegetic or non-diegetic because I think it could technically be
          both) at the end of the scene to build up to the end, and also removed
          it from the starting sequence so it didn't play twice in the
          animation.
          <br />
          <br />
          <img src="photos/week9/9.4.png" alt="" />
          <br />
          <br />
          Overall I was very happy with how much better the foley was this time
          around, it was in time and I thought already fit the environment. Next
          week I plan to add the non-diegetic sound and the ambiences!
          <br />
          <br />
          You can watch it here: INSERTLINK
          <br />
          <br />
          Other than working on The Disconnect, I also started to make the
          in-game sounds for Fired Before Hired! I wasn't sure how audio worked
          in Unity so I called the game developer up and he gave me a quick
          rundown of how I could do each thing I was planning on doing for
          sound! For example, how I could make footsteps, but make sure it
          doesn't play when the character has jumped, etc.
          <br />
          <br />
          I started by recording some sounds for the sword. To create variation
          I wanted to have 3 basic layers of the sword. The swoosh in the air,
          the metallic sound and then the impact of the sword. I thought it
          would be good to have 3-5 versions of this, which I can then randomize
          in the code and even randomize pitch and speed by a small amount. This
          would fix the problem of sounds being too repetitive! I will do the
          same general idea for all the sounds!
          <br />
          <br />
          I then also recorded fabric rustling for the character movement, I
          just made a super long sound and will mute and unmute as needed based
          on what keys are pressed and if the character is grounded or not. For
          other character movement, I also worked on the footsteps for both the
          main character and the enemies. I will use the same trick as the sword
          with randomizing the pitch and the speed to get that variation without
          having to record a bunch of different footstep sounds.
          <br />
          <br />
          That was all I recorded for this week, and next week I will finish
          recording the rest and also start editing them and potentially start
          implementing them in Unity!
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 10 ---------------------------------------------- -->

    <details class="week" id="week-10">
      <summary>Week 10</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          I reviewed the material discussed in class, specifically the Invisible
          Boys case study and the chapter from 'From Anthems to Abstractions:
          Creative Practice and Effect in Contemporary Screen Scoring'. Both
          look into how music and sound interact to express identity, emotion
          and place beyond traditional techniques.
          <br />
          <br />
          The chapter talks about that contemporary composers can often use
          textural and ambient approaches to avoke an effect, allowing sound to
          function as an emotional atmosphere rather than following
          melody-driven or thematic structures. This can be seen in Invisible
          Boys, where sound gives a voice to internal emotion and expresses
          identity, tension and vulnerability.
          <br />
          <br />
          Reading about this also allowed me to reflect on how I've been using
          sound in my projects, noticing that some of my ambient layers and
          drones already function in a similar way, as I tend to try and create
          atmosphere to get the emotion, rather than what you can actual hear in
          the environment! For example, the pitched down-choir I added in The
          Disconnect wasn't meant to sound melodic, it was more about creating
          an uncomfortable presence in the space.
          <br />
          <br />
          This weeks concepts reminds me that sound can express emotion through
          timer, rhythm, dynamic contrast and not just melody or harmony. It
          reinforced my belief that the boundary between music and sound design
          can often be blurred, especially when both are working toward the same
          emotional goal. Going forward I want to explore this boundary more
          intentionally!
        </p>
        <h3>Academic Research</h3>
        <p>
          This week I focused on two areas, the use of environmental sound in
          games and also I continued reading into the expressive potential of
          affective scoring in screen media!
          <br />
          <br />
          To begin with I read The Acoustic Ecology of the First Person Shooter,
          which analyses how environmental sound functions in game spaces. The
          author argues that ambience in games provides spatial orientation,
          emotional subtext and a sense of presence and how environmental
          soundscapes should feel like living systems that react to the players
          movements and virtual environment. The idea of an 'acoustic ecolofy'
          helped me reframe my approach to The Disconnect and Fired Before
          Hired. WHen I was layering winds, drones, etc. I realized that these
          sounds could also serve as emotional environments and not just
          background noise and can shape its mood and perception.
          <br />
          <br />
          I also continued reading the article that we read for class. Their
          discussion of affect, and how sound can create emotinoal resonance
          beyond narrative or melody was particularly relevent. Using a musical
          form that is more abstract can allow sound to communicate more fluidly
          with imagery and editingm evoking sensation rather than explicit
          meaning and I found this idea useful when reflecting on my own
          workflow on my projects.
          <br />
          <br />
          Both readings deepened my understanding of environmental sound and
          abstract scoring techniques to work together to define space and
          emotion. It reinforced that soundscapes should feel like its alive,
          breathe and shift, connecting the listerner to the space, but the tone
          of the scene
        </p>
        <h3>Artistic Research</h3>
        <p>
          To expand on this week's focus on environmental sound and affect, Ben
          Frost is a composer who blurs the line between music and sound design.
          His work in Fortitude and Dark uses dense layers of processed
          environmetal recordings liek wind, hums and feedback to evoke unease
          and psychological tension. In interviews he explains that he often
          records unstable sounds and then processes them until they feel alive,
          describing this process as building organisms out of noise. I thought
          this was such a creative way to make sounds and something that could
          definitely make a sound so unique, making things feel alive and using
          imperfections to make sounds feel human and emotional.
          <br />
          <br />
          I also looked into Brian Eno's ambient works, to demonstrate ho sound
          environments can create a sense is affective stillness. His philosophy
          of generative ambience means creating a soundscape that changes subtly
          without demanding attention. Eno descibes his ambient music as
          something that is ignorable as it is interesting, which was really
          interesting to think because I've never thought that I would want my
          music to be ignored. But his attention to space pacing and emotion and
          allows me to think about minimalism and atmosphere.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I focused on exporting and loudness management. I started to
          notice that my mixes sounded very different across devices and that my
          drafts sounded balanced and clear in headphones and speakers, but on
          my phone or laptop speakers, some frequencies were harsh and
          irritating. This inconsistency made me realize how important it is to
          monitor exports at a consistent loudness and also across multiple
          playback systems to make sure the sound translates properly.
          <br />
          <br />
          I watched a tutorial titled "Understanding LUFS and Headroom for Film
          and Game Audio", which broke down how loudness is measured using LUDS
          and why exports must follow certain standards. The video explained
          that broadcast and streaming content usually sits around -23 LUFS for
          dialogue based work and peaks no higher than -1 dBFS. Web based and
          game products can go slightly louder. These targets ensure that sound
          dpesn't clip or distort on common consumer devices and that the
          overall listening experience feels comfortable and consistent. I also
          learned about true peak limiting which prevents overstaturation that
          can occur in compressed formats like mp3 or mp4! After learning this,
          I found a LUFS meter plug in for Ableton that I will definitely start
          using.
          <br />
          <br />
          I also began to switch more between my studio headphones, laptop and
          phone speakers after each draft to see how the frequencies changes and
          will note the differences so I could apply EQ adjustments and focus on
          frequencies that are clear on all playback devices.
          <br />
          <br />
          This made it clear that exporting isn't just the final step, it is one
          of the most important because it actually determines how the audiences
          actually experiences the sound. If I do all this work for the export
          to suck, there is no point. Being aware of the loudness standards,
          headroom and playback can ensure that the detailed foley, ambience and
          music that I have been working on remains clear, dynbamic and balanced
          so that everyone can experience it.
        </p>
        <h3>Progress Report</h3>
        <p>
          This first thing I worked on this week was the ambience for The
          Disconnect. Even though I started from scratch, I still liked some of
          the ambiences I used in the first draft I made so I imported them all
          and was figuring out what to use for each scene! I also tried to
          highlight more of the foley throughout the entire animation, because
          one of the things I wasn't a fan of in the first animation was that
          the foley couldn't really be heard, and I think the footsteps and such
          could be a good tension builder.
          <br />
          <br />
          For the first scene, in the original draft, I felt like it was very
          dry and everything was in the mid range of frequency. In this new
          draft I tried to find and layer ones with different frequencies.
          Keeping this kind of low drones, but also higher pitched notes to
          build tension! I added a riser and a creepy sting when the main
          character is creeping around and then makes too big of a creaking
          sound. I decided to do this because when we watched A Bug's Life, I
          noticed there was musical stings alongside things that happened in the
          film, so I wanted to try it out. I think it worked really well! I also
          added this sample of a church choir singing, pitched it down a bunch
          and made it really low that you could just barely hear it to create a
          creepy atmosphere, without it being too overpowering. I wanted it to
          influence the atmosphere, but not sound like it was a diegetic sound
          that was coming from inside the church.
          <br />
          <br />
          <img src="photos/week10/10.1.png" alt="" />
          <br />
          <br />
          I then started to work on the classroom scene. But I had no idea what
          to put, any sound that I put felt wrong. I added this metallic
          scraping sound from the transition between the classroom scene to the
          interrogation scene and changed the pitch so it matched the main
          ambience of the next scene, which I'll talk about next.
          <br />
          <br />
          <img src="photos/week10/10.2.png" alt="" />
          <br />
          <br />
          I got this horror ambience and also reused one I made ages ago, which
          I had automated to make sure everything was around the same volume. I
          also found this pack of drones that had some cool sounds that I really
          liked, and used a low and high one to fill in the rest of the
          frequency spectrum. I kind of used the same ambiences for this scene
          and the next, but I think for the next draft I want it to slowly
          increase in intensity so theres a distinction from this scene and the
          next (sound wise).
          <br />
          <br />
          <img src="photos/week10/10.3.png" alt="" />
          <br />
          <br />
          Then for the final scene, I added a few more things. I added a bit
          more ambient noises, and especialy at the end I added a bunch of
          things like harsh wind and a drone. These were things that were
          present in the original draft, I just changed it up a tiny bit! I also
          added the stings nd suspense strings. I especially liked the tension
          built when she looks at herself in the mirror! I also kept the fire
          sound in the scene because I assume there will still be a candle in
          the scene even if she doesn't interact with it. It really depends on
          what is happening in the final animation!
          <br />
          <br />
          <img src="photos/week10/10.4.png" alt="" />
          <br />
          <br />
          You can watch the full (new) draft here:
          <a href="https://youtu.be/wsZlbXjXo5k"
            >https://youtu.be/wsZlbXjXo5k</a
          >
          <br />
          <br />
          I also recorded the rest of the things I needed for the Fired Before
          Hired game! I unfortunately did not do any editing this week and just
          recorded everything because something came up, but at least I have
          everything for next week.
          <br />
          <br />
          I recorded the sound that makes when an enemy is killed. I had
          previously recorded this audio of raw meat being squished around that
          I no longer have, but I thought had a good sound for a squelch of
          blood and flesh of someone dying. I did a few takes of that and also
          recorded other things! I recorded water splashing around and getting
          squeezed out of a bottle for maybe a blood spurt sound! Then also some
          random impacts into pillows with varying items like the cardboard
          container that badminton shuttlecocks come in, I tried punching the
          pillows, etc. Just things that make a thud sound basically. I think I
          will layer these alongside a sound I'll find online of a
          cartoon/animated death sound.
          <br />
          <br />
          The game developer also wanted some sounds for furniture destruction,
          but more simplified and less realistic. Just thuds when the walls fall
          over. I did this by letting thick textbooks fall on a variety of
          surfaces like wood, grass, etc. but my favorite and the one I will
          probably use is the one when it fell on carpet. I might use the others
          and layer them to create a more detailed sound. I might also get some
          samples of things breaking and put it very quiet just so the sound has
          some texture to it, and not just a dull thud. I'll be experimenting
          around with it when I start editing stuff!
          <br />
          <br />
          The last thing I recorded was for the sound when the job applications
          spawn. This makes it easier to figure out when things are coming at
          the player! What I did was recoord a jumprope whipping super fast to
          get that movement in the sound and I was thinking of slowing it down
          and pitching it down as well. I was thinking of also doing some higher
          sort of a synth sound as a motif when things get spawned so its
          recognizable.
          <br />
          <br />
          That is it for this week, and were getting so close to the end of the
          semester! The last stretch!
        </p>
      </div>
    </details>
  </body>
</html>

<!-- ------------------------------------ FUTURE WEEKS TEMPLATE ---------------------------------------------- -->

<!-- <details class="week" id="week-4">
    <summary>Week </summary>
    <div class="week-content">
      <h3>Reflection</h3>
      <p>...</p>
      <h3>Academic Research</h3>
      <p>...</p>
      <h3>Artistic Research</h3>
      <p>...</p>
      <h3>Technical Research</h3>
      <p>...</p>
      <h3>Progress Report</h3>
      <p>...</p>
    </div>
  </details> -->
<!-- </body>
</html> -->
