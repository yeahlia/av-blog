<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Audiovision Blog</title>

    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <h1>Audiovision Blog</h1>

    <!-- ------------------------------------ WEEK 1 ---------------------------------------------- -->
    <details class="week" id="week-1">
      <summary>Week 1</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week introduced sound as an emotional and perceptual experience
          that can guide interpretation and narrative. I was particularly
          interested in the exercise which played a score and we had to guess
          what the intended purpose was. I wrote it as a somber feeling mixed
          with hope, like an anticipation for something better when everything
          is wrong. It was nice to hear everyones perceptions of what they heard
          because they were geared towards the same sound, but different
          answers. It was very impressive when someone got the exact answer, I
          think that attests to how well the sound design was made for it.
        </p>
        <p>
          The Four Ways of Knowing stood out, especially participatory knowing
          (the knowing of feeling what it is like to be in an experience).
          Feeling what its like to be in a situation is very important with
          creating sound design (especially for films or games), even if youve
          never been in that situation yourself. Although it didn't just
          underline the importance to me, it also made me start thinking more
          about how the role of sound can be important for audiences to
          participate more emotionally to a piece of media. A lot of my personal
          projects with sound design have been very focused on realism, but I
          never really thought to consider exactly what I am aiming for with it.
          Am I trying to establish realism to make the visual space more
          believeable and draw attention to it? Am I trying to create certain
          emotions for things that haven't happened on screen? The video shown
          about how sound can change someones perception of visual media also
          led me to think more about it. A moment I found interesting was where
          it explained how sound could shift viewer attention, and how people
          change where they look on the screen based on what they can hear. I
          never really put 2 and 2 together that since I can hint towards
          something sonically, that would change how people perceive the visual
          media, thus changing where they look, maybe anticipating something. I
          just thought that was very interesting that sound can also be a tool
          to guide visual perception.
        </p>

        <h3>Academic Research</h3>
        <p>
          I was inspired by how sound could influence perception, and I ended up
          reading this article by Claudia Gorbman (Narrative Film Music), which
          just talks about how music can guide how people interpret narrative
          meaning. She talked specifically about 'unheard melodies' which is
          basically just how music can support narrative meaning in a subtle
          way, ways that the listener wont really realize unless thinking about
          it. It tied really nicely to what we learnt in class. I also went back
          to this previous paper that I had read previously by Chion. He
          discussed how audio could reflect on visual media in different ways,
          and how you use it is critical. For example, a disconnect in the music
          can create uneasy tension that adds complexity to the visuals, making
          something once seen as normal, into an underlaying feeling of tension.
        </p>

        <h3>Artistic Research</h3>
        <p>
          I started to look more deeply into music and emotions tying together,
          listening to my favorite songs and analysing what musically makes me
          love them so much. A specific example is Let Down by Radiohead. Or any
          radiohead song in general. The way that they build atmosphere, the
          experimental sound, the guiding structure, its moody, its spacey. Even
          i'm not 100% sure the technical reasons of why it makes people feel so
          much. There was a YouTube video I
          <a href="https://www.youtube.com/watch?v=PX2hQdcEvyA">watched</a>
          that I think explained it perfectly, and why it's so good alongside
          visual media. The use of harmony, texture, mood, rhythm. I think its
          refreshing considering a lot of songs nowadays follow certain
          formulas, but that's a completely different topic that I could go
          into.
        </p>

        <h3>Technical Research</h3>
        <p>
          Because of the Gina Moore assignment, i did a lot of research on how
          to use Ableton Live, which is my daw of choice. Experimenting with
          automation and how to make glitches were the highlights. Although I've
          done it automation before, I haven't done it extensively. The more I
          did it the more shocked I was of how it made a difference, especially
          because if I wanted a sound to go away, I just cut it out, but having
          it play soft in the background added that weight I always felt was
          missing from my sound projects. Learning different ways to make
          glitching sounds was really fun. I layered effects like pitch
          shifting, grain delay and reverb and then used automation to create
          sudden cuts, fade ins and randomized movement, which helped me achieve
          that glitch texture I was looking for.
        </p>
        <p>
          Other than Ableton Live, I actually learnt how to compose a simple
          orchestra piece for this assignment. I recognize that scores aren't my
          strong suit and I wanted to do something that challenged me,
          especially since the animation I chose was around 30 seconds, so I
          didn't have to make it too long. I watched
          <a href="https://www.youtube.com/watch?v=ZdhdC2wx2Ew">this video</a>
          which was actually extremely helpful and I learnt many new things. I
          thought the violin and viola were basically the same intstrument
          before this! It was interesting how different instruments in brass,
          woodwind and strings could play the same thing and it gave off
          different emotions and different vibes, even though they were in the
          same frequency spectrum. I particuarly liked the hack of making
          everything on a simple piano, then assigning it to different
          instruments, I definitely think it helped with my workflow as I
          started focusing more on chords and notes and why I wanted to press
          those, rther than being stuck figuring everything all out at once. I
          would highly reccommend that video to anyone looking at how to start
          with orchestral writing.
        </p>

        <h3>Progress Report</h3>
        <p>
          I began by choosing the animation I wanted, then thinking about what
          direction I wanted to go with. This will be written on my report but I
          originally started with a realistic approach but felt that the
          animation was too abstract, and wanted something to fit with that,
          which is why I chose to do a musical score. Of course as mentioned
          before, I chose to do an orchestral piece because of my nonexistent
          experience. Before watching the video I made the mistake of just
          jumping straigh into, which resulted in a terrible score. I just did
          chords and was playing random lead lines within the key, but it
          sounded terrible:
        </p>
        <img src="/photos/week1/original.png" alt="" />
        <p>
          My composition improved greatly after I watched the video. I started
          with a rough piano roll and kept refining it until I had a sound and
          structure I liked which consisted of a lead, a second lead, chords and
          a bass which was all I needed!
        </p>
        <img src="/photos/week1/pianoroll.png" alt="" />
        <p>
          From this I made different groups of the main instruments of strings,
          and assigned the different parts appropriately. The plugin I used was
          BBC's expressive strings, it was super great and useful and free!
        </p>
        <img src="photos/week1/strings.png" alt="" />
        <p>
          then i did the same for brass, using similar mapping techniques to
          fill out the orchestration
        </p>
        <img src="photos/week1/brass.png" alt="" />
        <p>and then woodwinds.</p>
        <img src="photos/week1/woodwind.png" alt="" />
        <p>
          after that it was just a matter of using automation to create
          structure! Then the sound design was finished! Originally it took up
          the entire time, but when I decided that the glitches would turn it
          into synths, I cut the midi short. I used a bunch of random effects
          tomake the glitch sounds as I mentioend earlier, and then to
          transition, I layered a abunch of the elad lines with different sounds
          and textures to lead into the glitchiness, but also give it some
          melodic strucutre so the change wasn't too jarring! This completed my
          first draft and I'm still refining, but I have a structure I like and
          an idea I like so I am happy.
        </p>
        <img src="photos/week1/transition.png" alt="" />
      </div>
    </details>

    <!-- ------------------------------------ WEEK 2 ---------------------------------------------- -->
    <details class="week" id="week-2">
      <summary>Week 2</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week in class we analysed the Turn Down for What music video with
          and without sound. I first watched it muted and made notes on what I
          thought the visuals were trying to communicate. Without the audio, it
          felt like a chaotic horror-comedy film where someone’s out of control
          and something is taking over them. The character’s movements were
          exaggerated and crazy, which made the visuals seem comedic, but also
          strange and hard to follow. Without sound, the pacing felt confusing,
          and the structure was harder to grasp.
        </p>
        <p>
          Once the audio was added back in, the whole thing made more sense. The
          music gave it rhythm and energy, making it feel more like a
          conventional music video, where weird visuals are usually expected.
          The sound helped me anticipate moments like when the chorus began, the
          movements got more repetitive and bizarre. it gave the piece form and
          made it easier to follow and highlighted how sound can structure
          visuals and change how we interpret movement.
        </p>
        <p>
          Since I’ve been away, I haven't able to attend the lectures this week,
          but I did get to watch some of my friend's work online. It was really
          inspiring to see how different people approached the same animations
          in their own way, especially because when I was choosing what
          animation to do last assignment, I had my own ideas for each one. It
          was interesting to listen to how they handled mood, timing, and
          transitions, and really hammered in how much creative interpretation
          in sound design is just as important as technical choices.
        </p>

        <h3>Academic Research</h3>
        <p>
          For this week, I mainly referenced two texts that helped shape how I
          approached the Resonance video. The first one was Karen Collins, who
          talked about how ambient textures can help set the emotional tone
          before anything even happens visually. That stuck with me, because I
          start my projecst by focusing on atmosphere rather than realism, and
          it gave me context on why I liked doing that. I wanted the sound to
          slowly evolve and feel like something unstable was underneath, and her
          ideas made me feel more confident in that choice as a lot of the
          people around me usually start with diegetic sound.
        </p>
        <p>
          The second article I read was by Sonnenschein, where he explains how
          different frequency layers being used can create space and shape how
          things feel. I used this information for the narrative arc, using the
          lower frequencies at the start, then evolving the atmosphere by
          introducing different sound frequencies. By balancing the low end
          rumbles with higher pitched textures, it gave the piece more depth,
          like something was exploding.
        </p>

        <h3>Artistic Research</h3>
        <p>
          For this week, I tried to find artists where the music builds mood and
          atmosphere that makes the audience feel emotional. One of them was
          Jeff Buckley, especially his Grace album. Music alone, his use of
          dynamics, reverb and vocal layering creates this haunting and
          immersive sound that really stuck with me. I was especially immersed
          with how his songs often start soft and open, then swells into
          intensity in a yearning way. The switch in dynamics is something that
          influenced how I looked at Resonance.
        </p>
        <p>
          I also listened to Johann Johannsson and his score for Arrival. The
          blending of orchestral instruments with electronic textures felt
          realistic, but otherwordly, which really suited the film. I liked the
          shifts in tone and textures to guide the audience through emotion
          without the need for strong, imposing melodies. It highlights the
          possibility that music can be felt rather than heard, and still have a
          strong impact.
        </p>

        <h3>Technical Research</h3>
        <p>
          I explored how to build space and emotion through effects this week.
          To make the sound design feel wide and immersive, I experimented with
          varying levels of reverb, delay, and layering and keeping the dry/wet
          balance lower at the start and opening it up as the visuals progress.
          I also experimented more with automation than I usually do, especially
          to make certain parts fade in or glitch out. Mastering has always been
          difficult for me, and I watched a few different videos to help me with
          it! One of them being
          <a href="https://www.youtube.com/watch?v=fZdlUiqiDoM">this video</a>
          which talks about different techniques to make audio spaces feel wider
        </p>

        <h3>Progress Report</h3>
        <p>
          This week I mainly focused on the completion of Assignment 2.1. I
          started by creating the atmosphere first by layering different
          horror-themed ambience to create the general structure
        </p>
        <img src="photos/week2/ambience.png" alt="" />
        <p>
          Then I added all the sound effects, including the wet sounds for the
          bursting tentacles (I used a variety of different slime sounds), the
          cinematic risers and faders, etc.
        </p>
        <img src="photos/week2/fx.png" alt="" />
        <p>
          After that I experimented with different odd and unique sounds to get
          a more interesting texture
        </p>
        <img src="photos/week2/unique.png" alt="" />
        <p>The finally just the static at the start.</p>
        <img src="photos/week2/static.png" alt="" />
        <p>
          After that I spent quite abit of time adding reverb and sending it
          through buses, and EQ shaping and effects to make the sound a bit
          wider a larger!
        </p>
        <p>
          Other than the Resonance project, I was also able to secure the spot
          to make sound design for one of my friends games! It is a pretty
          simple game, but it is a good introduction into make sound for games,
          as I have never tried that before. I will need to learn Unity and
          FMOD, and just interactive music in general.
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 3 ---------------------------------------------- -->
    <details class="week" id="week-3">
      <summary>Week 3</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          As I can't attend class, it's pretty difficult to write a reflection
          on ideas raised in class, especially when there isn’t any official
          lecture, but I will try my best!
          <br />
          <br />
          The first class focuses on presentations and feedback for Assignment
          2.1. I showed a few of my family members I am with my project and
          asked their thoughts on it. They aren't sound designers and don't
          really think like them, but this is exactly why I thought their
          feedback was so good. They were able to tell me feedback based on a
          normal consumer level. For this project I generally was told that it
          was very low and that I tend to focus on lower sounds and frequencies,
          leaving the higher ones lacking. I didn't make use of the entire
          frequency spectrum. This made the sound seem a bit muddy or even
          lacking, and hard to make out specific parts. I think especially
          listening to my own work on speakers made this very clear, as I didn’t
          really notice when I was mixing it with headphones in.
          <br />
          <br />
          Another piece of feedback I got was that my mixing could have been
          better. The atmospheric sounds were a bit low and hard to hear, while
          the squelching noise was loud and overbearing, it was basically all
          they could hear. I think feedback like this is especially important,
          and will keep in mind for the future! I think I really need to invest
          in some speakers... It also made me realise how important it is to
          check a mix on different playback systems, what sounds balanced on
          headphones can feel completely different on speakers. This is
          something I’ll try and consciously do for all my future projects.
          <br />
          <br />
          Although, they did like the amount of layers I put in, they felt like
          it had good amounts of sound, was realistic, and they could make the
          connection between the visuals and the audio I chose. They also liked
          the atmosphere I was able to create, and they were particularly
          impressed with the sound design when the tentacles started exploding
          out of the planet. Both the positive and negative feedback is very
          important to me, so I was happy to receive detailed feedback from a
          regular, consumer perspective. It reinforced the idea that while
          technical skill is important, the audience’s emotional and intuitive
          response is ultimately what matters most.
          <br />
          <br />
          The second class had a guest lecturer, and I’m honestly so annoyed I
          missed it because the topics covered sound like they would have been
          incredibly useful. Since I wasn't there I will just talk about my own
          thoughts...
          <br />
          <br />
          I’ve always been a bit confused about the difference between sound
          editing and sound design. A lot of my own process involves editing and
          adding effects to audio from sound libraries, so does that still count
          as editing, or is it considered sound design? Does sound design only
          mean foley or creating audio entirely from scratch? It makes me wonder
          what actually defines a sound designer, especially when I’ve seen
          purists online who refuse to use sound libraries at all, and shame
          anyone who does. Personally, I think there are certain sounds that are
          just impractical or nearly impossible to record yourself, so using a
          library can be the smarter option. I mean, if it already exists, why
          not use it? But I do think people should at least try to build up
          their own sound repertoire so they don't have to rely on sound
          libraries forever.
          <br />
          <br />
          I also think that knowing how to make exactly what you're thinking is
          incredibly hard, but super rewarding once you are able to. As with all
          creative practices, you get the hang of it once you start doing it
          more, and being in this studio and having to make audio for visuals
          every week so far has actually shown a significant improvement in my
          skills, which I am both surprised and thankful for. Missing the
          lecture makes me want to spend some time researching the roles and
          workflows the lecturer discussed, because I can already see that
          understanding the boundary between design and editing could help me
          plan my projects more efficiently.
          <br />
          <br />
          This also made me think about the phenomenon of the Wilhelm scream,
          which is that infamous stock sound effect everyone recognises. When
          it’s used, it can snap the audience out of immersion because they
          immediately associate it with other films. I think that raises an
          interesting question about when familiarity adds charm or nostalgia
          versus when it becomes distracting. In this case, I can see how using
          sound libraries can be not ideal, especially for big blockbuster
          films. Like surely they could have filmed a guy screaming instead of
          choosing to use a stock sound? I mean they have such big budgets
          right? This makes me reflect on my own sound choices, maybe I should
          be more conscious of using highly recognisable sounds unless I’m
          intentionally going for humour.
          <br />
          <br />
          Anyways, the rest of the lecture content, things like EQ, editing
          concepts, layering techniques, emotional architecture, and more, all
          sound super interesting. I’d love to dive deeper into those topics, so
          if the lecturer presents again or if there’s an online recording
          somewhere, I’d definitely want to check it out. In the meantime, I
          will probably explore some online tutorials or breakdowns from
          professional sound editors to get a better grasp of these concepts, so
          that next time I’m working on a project, I can apply more deliberate
          choices in rhythm, layering, and tonal shaping.
        </p>

        <h3>Academic Research</h3>
        <p>
          This week I read Zwicker and Fastl's Psychoacoustics: Facts and Models
          to understand more about how frequency content affects how we perceive
          time and motion in sound. They talk about how removing higher
          frequencies can make things feel slower and heavier, which is exactly
          what I needed for the slow-motion sections of my project. It gave me
          the idea to use frequency filtering instead of pitch-shifting or
          time-stretching. It was nice to have theory backed reasonings for my
          work, as I did notice a significant difference by using the pitch
          filtering because it was exactly what I was aiming for.
          <br /><br />
          I also read Altmans Sound Theory, Sound Practice which kinda goes into
          how sound and image work together to make meaning. Although this made
          sense to me already and in my head I thought this was obvious, I
          started to think more intentionally about my design choices, and why I
          chose a specific pad sound. I realized I chose it because
          subconsciously I wanted the pad to build up in the same way that the
          cube does visually. I connected this with the pitch frequency
          filtering, where the sound slowly regains its higher frequencies and
          the audience will get reinforcement that the video is progressing, and
          just gives it structure overall.
          <br /><br />
          Holman's Sound for Film and Television was also another useful text
          for this week. He talks about matching the texture of sound to the
          visuals, which made me change my EQ choices for the Lego building
          sound. I knew that the Lego sound felt out of place, but with EQ I was
          able to get it to feel shinier and cleaner which matches the visuals
          aesthetics way more. I did this by boosting the higher end, lowering
          the bottom and keeping the mids relatively the same to still maintain
          that tactile feeling of the original recording!
        </p>

        <h3>Artistic Research</h3>
        <p>
          I was rewatching a few old Pixar films recently and one of them
          particularly caught my attention sound wise. Wall-E's different sounds
          and expressions are made by using everyday objects to make sound, yet
          still felt so emotional and believable and had this weight to it. It
          felt authentic. I was inspired by this, as I thought it was so
          impressive how this sound was made and edited. It was what gave me the
          idea to find everyday objects for this weeks project, which ended up
          being Lego. It is a familiar and nostalgic sound and while its not a
          direct match for the sound I imaged for the cube, I could process it
          but still keep that tactile identity.
          <br /><br />
          I also listened to Royji Ikeda, who makes super minimalist and precise
          tones to make his sound. It's very clean and he uses a lot of control
          and timing to make his music (though I would just call it sounds
          rather than music). This is what made me want to try limiting the
          amount of sounds I can use, to try getting the full potential out of
          the sounds I already have in the project, by using control and timing.
          Ikeda showed me that simplicity can be just as powerful and impactful
          as complexity, and I really wanted to experiment with that in my
          project this week.
          <br /><br />
        </p>

        <h3>Technical Research</h3>
        <p>
          This week I mainly focused on how to make glassy and shimmery sounds
          from scratch, as I tend to rely on a lot of sound libraries for it.
          Although there is nothing wrong with using sound libraries and
          manipulating the sound, I thought it would be best to at least learn
          how to make some so I could build my own sound effect library. I
          followed this
          <a
            href="https://www.youtube.com/watch?v=CEZHfQOqQxQ&ab_channel=UnderdogElectronicMusicSchool"
            >YouTube video</a
          >
          which uses FM synthesis to make those glassy sounds. He is a very good
          teacher, calm and explains himself well, so I will be looking more
          into his other videos in the future. Although he used an Ableton
          feature that I don't have, I found the skills to be transferrable to
          other budget options and plugins, as he explains why he does each
          step, and even showing the difference between each parameter in the
          different effects he uses. It was a super informative video and I was
          able to make a few sounds I liked! <br /><br />
          I also watched
          <a href="https://www.youtube.com/watch?v=UQy7VHm-Ucg">this video</a>
          which is a tutorial on how the PaulXStretch plugin works. It goes
          super in depth and explained all the features and I can't believe the
          plug in is free. Although I ended up having some technical troubles
          and couldn't use it for this weeks projects, I am excited to fix it
          and use it in the future! Especially the ability to change different
          harmonics and use it in conjuction with other effects would make for
          really good experimenting.
        </p>

        <h3>Progress Report</h3>
        <p>
          I had a lot of projects to work on this week! I had a bunch of people
          reach out to me to do the sound design for their projects but
          unfortunately I wasn't able to do all of them, even though they were
          all such interesting concepts. One of the projects I chose to work on
          was "The Reconnect". Caroline, the animator, sent me her draft
          animatic, as well as a sound design brief and the project concept. I
          really appreciated how organized and professional the documents she
          gave me was, it was super helpful to decide what I wanted to do for
          her project, while bringing her vision to life. She gave me a small
          part of her animatic that was confirmed to stay the same, so I worked
          on that scene.
        </p>
        <p>
          I first started by making the ambience of the scene, like I usually
          do. I layered a few different textures to make the structure of the
          scene. Though it was pretty hard because it was a pretty small scene,
          but I'm sure when I work on the full thing, I'll be able to transition
          between sounds better, without sounding too abrupt.
        </p>
        <img src="photos/week3/reconnect2.png" alt="" />
        <p>Then I worked on the diegetic sound:</p>
        <img src="photos/week3/reconnect3.png" alt="" />
        <p>
          Then worked on the non-diegetic sound, and transitional sounds and
          wooshes:
        </p>
        <img src="photos/week3/reconnect1.png" alt="" />
        <p>
          And that was basically done for the scene! Again, it was pretty short
          and I kept the narration that was in the original animatic (which is
          why it might sound bad in terms of the mix, because the narration was
          also a draft), but I am happy with the general idea, and so was
          Caroline. She mentioned some changes when she finalizes the animation
          but it is a good start! You can watch the draft
          <a href="https://youtu.be/UgO7CwLyiRE">here</a>!
        </p>
        <p>
          Then I also worked (and finished) my Assignment 2.2 as I am busy next
          week and won't have time to do it! I first began by making my own pad
          sound using the tutorial I showed you earlier. I used his tip of
          turning the midi file into an audio file and used that to make all the
          changes, and added it into my personal sound library.
        </p>
        <img src="photos/week3/own.png" alt="" />
        <p>
          I then experimented with grain delay, using the pitch parameter to try
          and simulate the slow motion effect:
        </p>
        <img src="photos/week3/graindelay.png" alt="" />
        <p>
          I wasn't a fan of this as I wrote on my report, and decided to try
          using the PaulXStretch plugin:
        </p>
        <img src="photos/week3/paulstretch.png" alt="" />
        <p>
          But again, wasn't a fan and there was a steep learning curve and
          technical difficulties. I then decided to use frequencies filtering
          instead, which I was a fan of!:
        </p>
        <img src="photos/week3/padauto.png" alt="" />
        <p>
          It sounded exactly like I wanted. And then I did it for the lego
          building sound as well, with less of a sweep:
        </p>
        <img src="photos/week3/legoauto.png" alt="" />
        <p>
          Then it was just a matter of finishing touches! I added the sparkles
          that go throughout the entire video, but used volume automation so it
          was more prominent in the later half of the video:
        </p>
        <img src="photos/week3/sparkle.png" alt="" />
        <p>
          And that was done for AV2.2! I just had to write the report. But other
          than that, I also decided to use one of my old song projects for the
          music video I will do for this semester. Although I haven't 100%
          decided it will be a music video, I was also thinking of doing a light
          show for the Capitol Theatre! Which I thought would be cool. Anyways I
          went through a bunch of my old guitar files, and found this one I
          liked, where it was in the shoegaze genre, and I added some lead
          guitar over it and messed with the effects a bit, as well as adding
          the much needed drums. Although I am not a drummer and had to MIDI the
          entire thing, when I get back to Australia I'll probably use real
          drums and fix up the guitar parts, as well as add more instruments.
          You can watch the WIP of the song
          <a href="https://youtu.be/6MhdX7PVxWc">here</a>. I chose that song
          because it's very atmospheric, but still incorporates live instruments
          so it is the best of both worlds! (and I am a huge shoegaze fan)
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 4 ---------------------------------------------- -->

    <details class="week" id="week-4">
      <summary>Week 4</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This weeks lesson was about giving feedback to your peers about
          Assignment 2.2. Since once again I am away, I've decided to ask the
          people around me for feedback! Thankfully this is the last lesson I am
          away since next week there's no classes! The advice I got last week
          was actually extremely helpful in learning on what other people
          typically hear in a consumer standpoint, it helped me inform my
          decisions for this project.
          <br />
          <br />
          The main critique I had for my sound design was about the foley that I
          used for the cube-builing sequences and that it didn't feel as
          convincing as it could have been. They mentioned that it felt too
          clean and didn't match too much with the visuals. I realized that this
          probably happened because I relied too much on just the one sound
          layer, instead of using multiple diffrent textures. I usually do like
          to stack a bunch of effects and layers to achieve a sound, so I'm not
          sure why I decided not to do it here! Maybe because I was too focused
          in matching the clean visuals, and the fact I tried to impose a
          challenge on myself in using minimal layers. I think doing this lost a
          lot of weight into the sound that could have made the foley more
          believable. But now, I can actively use more layers on my next project
          (sourcing a variety of different sounds) and then layering them to
          create more depth and realism!
          <br />
          <br />
          The other criticism I received was that the atmosphere wasn't as
          immersive as my previous project. Even though there was that challenge
          of the minimal layers, I do think the atmospheric sounds and ambience
          was a bit low, and you could barely hear it once the layers started to
          build up. I definitely spent more time building layers and acheiving
          depth compared to this work, which is probably why they felt there was
          a lack of atmosphere in this project. Next time, I would like to be
          more intentional about the spatial audio, and making sure the
          environment feels alive before I start working on the diegetic sound!
          <br />
          <br />
          However, the consensus was that the slow motion sections felt great,
          they could feel that it was meant to be slow motion. It felt natural
          and well-timed, and they particularly liked how all the aspects felt
          like they were slowing dowm, and helped reinforce the visuals. I put a
          lot of focus into getting the feeling of slow motion down, and getting
          all the automation to match up to the visuals and all the sound, so I
          was very glad and happy to receive that feedback! Very encouraging.
          The changes between different motion speeds flowed in a way that kept
          the pacing smooth, which tells me that my use of automation has gotten
          much better! I'd like to experiment a bit more on the gradual blends
          and transitional elements to really nail it for my future projects!
          <br />
          <br />
          Overall this feedback made me realize I tend to prioritize certain
          aspects and because of that, potentially can overlook others. I'm
          happy with the parts I spent a lot of time on, but I know I really
          need to push my foley design and environmental layering if I want to
          improve my design further!
        </p>
        <h3>Academic Research</h3>
        <p>
          This week I researched a lot on how to create horror ambiences, and
          what can cause viewers to be scared just through audio alone, as well
          as how audio can enhance visuals and imply something is there to keep
          viewers on the edge of their seat
          <br />
          <br />
          Blumstein, Davitan and Kaye discuss how harsh, non-linear sounds can
          trigger instinctive threat responses in their article, 'Do film
          soundtracks contain nonlinear analogues to influence emotion?'. This
          can be sudden distortion, bursts of noise, unpredicatable modulation,
          etc. because they can resemble animal distress calls. I could use this
          idea by adding these short bursts of sound and effects at moments
          where I want to create sudden tension. Using this technique would give
          me a more natural way to make an audience feel unsettled without
          solely relying on visuals jumpscares.
          <br />
          <br />
          Another interesting article I read was by Menninghaus 'The
          Distancing-EMbracing model of the enjoyment of negative emotions in
          art reception'. It looks about how inharmonic or detuned sounds create
          a sense of unease because it disrputs the harmonic balance that we
          expect. This would be interesting to apply into my horror/triller
          ambiences that I am creating by layering slightly out of tuen drones
          or pads underneath other layers to make a sense of unease in my
          projects. It would let a scene feel tense even if nothing dramatic is
          happening visually, which is important for horror ambiences.
          <br />
          <br />
          A lot of the research I do on ambiences and music typically involve
          sound and making it. But I think this book by ZhouI read is
          particilarly interesting ebcause it talks about the absence of sound
          and its effects ('Effects of silence on emotion response to sound').
          He explains how silence and sudden changes in dynamic can actually
          heighten emotional impact and reactons to sound. I think I can use
          this in my own projects by adding short, deliberate pauses before a
          loud or important sound effect , which can bring attention to it and
          heighten its impact. It would make those moments stand out more,
          especially in suspenseful scenes.
        </p>
        <h3>Artistic Research</h3>
        <p>
          Because I've been so focused on horror this week, I went back to the
          Silent Hill series, a famous horror game known for its creepy
          atmosphere. Akira Yamaoka was the sound designer for the franchise,
          and he tends to blend industrial noise, detuned instruments and
          minimal melodic elements to create spaces that feel haunting. Since
          Silent Hill is a game, he uses looping textures that evolve slowly,
          making the player feel trapped in the emotional state he creates. I
          could use this technique in my future projects, and even the
          animations I am involved in this semester by creating subtle
          variations between each scene to sustain tension in scenes where not a
          lot of dramatic events happen. However this would be especially useful
          in long sequences where the user explores a space and they tend to get
          lost and immersed, that they forget about the sound.
          <br />
          <br />
          Another game that is pretty famous for its horror sound design is
          Resivent Evil. Shusaku Uchiyama relies heavily on lower drowns, tight
          reverb and metallic sounds to manipulate the players feeling of space,
          making them often feel ecnlosed and claustrophobic. I love this game
          series and I genuinely feel scared while playing the game, especially
          being hyper aware of my surroundings and getting scared over things
          that wouldn't scare me usually. I feel like I could adapt this into my
          own work by shaping the reverb and adding lower frequencies to make
          the space feel smaller and constricted, to try and make the viewers
          feel the same as I do when I play Resident Evil. I think using sharp
          high pitched stingers can also make the listener snap and pay
          attention to key moments if I use them alongside the low drones.
          <br />
          <br />
          Other than games, I also found this sound designer and composer called
          Lustmord, who specializes in making ambient music that feature low
          drones, reverb and little high frequency details. This skewed
          frequency spectrum can evoke a sense of oppressive vast emptiness. His
          music is kind of minimalist, which proves that horror sound design
          doesn't need to be super busy or full of layers to be effective, it
          comes from scale and patience and building (which I could probably use
          in my projects as I tend to just stack layers and hope it works). I
          could use his philopsophy to try to strip back my soundscapes and
          focus on depth and detail in a few elements to make the atmosphere
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I watched
          <a href="https://www.youtube.com/watch?v=U7wkq_s2t4M"
            >How to Design Dark and Scary Sounds</a
          >
          which went through a bunch of techniques on, well, how to design dark
          and scary sounds! It went through backmasking, backwards reverb,
          backwards talk, the importance of slowing things down and combining
          all of these. I liked this tutorial because it shows how different you
          can make sounds just by transforming existing recordings using effects
          and such, rather than trying to make a purely scary sound.
          Experimenting with my own sounds and seeing what I can create with
          them is an important part of sound design and I want to try
          experimenting with this in the projects this semester! I think it'd be
          cool to have backwards talk for an easter egg in one of the
          animations.
          <br />
          <br />
          I also watched
          <a href="https://www.youtube.com/watch?v=8QlWUGtB_9w"
            >How to Sound Design Horror Atmospheres using Vital</a
          >
          which was basically a deep dive into building horror ambiences from
          the very beginning. It also was kind of a tutorial on how to use Vital
          as I am still not 100% comfortable using it. Anyways, it went step by
          step on how to set up the oscillators and effects and adding pitch
          modulation and movement. It also went through a bit of basic music
          theory, specifically using minor seconds to create dissonance and also
          showed how to create intros, risers and just evolving atmospheres in
          general. This was super helpful because it explained what to do, but
          also why were doing each step so I could make my own atmospheres using
          the same basics. Even though it was specific to Vital, I can
          definitely use this information in other synths or plugins!
        </p>
        <h3>Progress Report</h3>
        <p>
          This week was when I was the most busy overseas, so I didn't get to
          work as much on my projects as other weeks(which is why a lot of my
          assignments were done super early if you noticed!), but I still tried
          to do a few.
          <br />
          <br />
          I mainly tried to work on things that I could do without external
          equipment (as I usually use a MIDI keyboard, and I don't have access
          to a professional recorder). I worked on the song I will use for the
          music video (although plans have changed and I'm not sure if my
          videographer has time, so I might do a lighting show alongside my
          song) and fixed up a bit of the drums. I used the free kits in the
          Steven Slate Drum's pack and it sounds way better than the default in
          Ableton. Though some of the snares were a bit off so I had to go in
          and manually change the different MIDI notes which was a bit tedious,
          but I am very happy with how it sounds so far. I also used Shreddage
          to add a bit of bass and other guitar parts that I would like to try
          once I'm back at home with my instruments. I also started to
          experiment with lyrics (this is what I was dreading) and melody, but
          haven't found anything that particularly stuck to me.
          <img src="photos/week4/1.png" alt="" />
          I did the same with some piano synths to create a kind of wall of
          sound. I used the video mentioned in the technical research to really
          help me with the order of effects and such. I'm very excited to work
          on this with real instruments (especially real drums!)
          <br />
          <br />
          This week had a lot of focusing on planning the semester, especially
          how I will organize my time once I am back.
          <br />
          <br />
          I had a call with Tara, the animator of The Hollow Child, as well as
          Maria (a sound designer from our class), as we are both working on
          this project together. We just talked about what her vision is for the
          sound design, and what we could achieve based on that. She was very
          understanding and very helpful, gave us a timeline of when things will
          be done, what she expected of each scene, but also trusts us and gave
          us a lot of room to be creative and do what we want to make the sound
          match the visuals. It's hard to work on specific sounds as the
          animatic isn't confirmed 100% yet and is still subject to change, but
          she said she will be done soon so we can start as soon as possible!
          She also put us in contact with the voice actors, so we are able to
          communicate directly in case certain sounds need to be rerecorded, I
          am happy to be working with someone who is on top of everything and is
          thinking ahead!
          <br />
          <br />
          The other animation, 'The Reconnect', that I am also working on, I
          also had a proper chat with animator on what she wanted the sound
          design to achieve. She was more loose with the descriptions, and
          instead wrote more about what she wants each scene to feel like. I
          think this will be good experience on creating my own visions based on
          client's requirements on the atmosphere and end product, while the
          other project will be good experience on achieving balance betweenthe
          specifics of what the client wants for each scene, versus what I
          believe as a sound designer. I am very grateful to both for being very
          flexible and understanding, while also giving guidance, I am very
          excited to be working with them! At the time of writing this, she
          actually reached out to me saying the first 10 seconds of the
          animation is fully complete so I will be working on that next week!
          <br />
          <br />
          Because both of these animations are very horror, thriller and tension
          based, I started to resarch a lot with creating ambiences, since I can
          use these as layering materials for any of these projects (and even
          future ones!). What I learnt can be seen in the technical research
          section! This was my favorite part as I was able to create a lot of
          audio clips that I can use in the projects, maybe with a bit of
          editing to fit the actual scene, but overall I am very happy with the
          progress. I have a lot to work on once I am back next week!
          <br />
          <br />
          I also had a call with the game developers of 'Fired Before Hired' and
          what sound design and foley they expected of me. This project is
          heavily reliant on realistic sounds, like whooshing of swords,
          furniture being broken and torn, etc. so I really get to work on my
          audio recording skills (which honestly need a lot of work...!). I am
          nervous to record, but also excited because I know its a skill that is
          necessary and would like to get better at. I was also asked to make
          the menu music, and kind of like office/elevator music. They said they
          weren't sure of what kind of vibe or genre they are going for, so they
          want me to make a small snippet of different ones, like 8bit,
          electronic, real instruments, etc. This makes it kind of difficult,
          but hopefully I can only make a 5-10 second clip so they know. Or I
          was thinking of using MIDI, so I am able to easily change the
          instruments so they can decide, and then I can further refine it.
          <br />
          <br />
          With this information I made a general timeline of when I want to have
          things done, but the general timeline is as follows:
          <br />
          21st-24th = Fired Before Hired menu music clips, figure out all parts
          of the song
          <br />
          25th-31st = Begin working on the animations (this is when both
          animators are expecting to finish the final draft so we know the
          timing of each scene), especially using the pod alongside Maria to get
          our ideas down, start officially producing song
          <br />
          1st-7th = Finishing producing song, as well as mixing and mastering,
          record and finish the sound bits for Fired Before Hired, continue to
          work on animations as scenes gets finished
          <br />
          8th-14th = Figure out how to work sound in Unity, start working on the
          level music for Fired Before Hired, continue to work on animations as
          scenes gets finished
          <br />
          <br />
          This is just the general plan for the next few weeks, I am very aware
          that it is a lot per week, and other classes may take priortity in
          certain weeks. This is just a general guideline of the best case
          scenario and finishing early, which I know won't 100% happen. But
          having a guide of what I want to do and pushing myself helps me to do
          more work! Anyways I am definitely savouring my last week of being
          away before all my assignments starts getting due soon....
          <br />
          <br />
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 5 ---------------------------------------------- -->

    <details class="week" id="week-5">
      <summary>Week 5</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          There was no class this week so nothing I can write for in class
          learning, but I am writing more reflective writing in the research
          section to make up for it!
        </p>
        <h3>Academic Research</h3>
        <p>
          I did some research in collaboration in creative audio communities,
          and found this article by Calefato, Lanubile, Novielli and Valetto,
          titled "Collaboration success factors in an online music community".
          It talks about successfull collaboration depends on momentum,
          recognition, and individual contributions for the project. This really
          helped me figure out a way to interact with the people I am
          collaborating with, changing my mindset into a more professional one
          and some tips on how to make the collaboration successful. The article
          made me consider how my sound design interacts with their creative
          vision, rather than just being a seperate entity. Since the
          collaborative works I am working on are majorly for someone elses
          creative visual work, it is important to keep a line of communication
          open, while also figuring out on creating a balance of what the
          collaborator wants, as well as putting my own creative vision into the
          work. I also found that receiving feedback and adjusting drafts was
          not only about fixing mistakes, but also creating cintinuity with the
          animators intention for the work. I also noted that some collaborative
          practices could be helpful in my own solo projects, for example,
          creating a variety of drafts to see which I like better, rather than
          continuously working on the same project to see if I'll end up liking
          it. The article really helped me develop this mindset of openness and
          responsiveness that could definitely imrpove both group and individual
          projects.
          <br />
          <br />
          I read another article titled "Footsteps with character: The art and
          craft of Foley" written by Wright. It describes Foley as not just a
          technical task to record sounds, but also a craft where sound
          designers have the chance to add character and intention into ordinary
          noises. This information will greatly help me in the future when I do
          more intensive foley tasks. I have taken foley as just a task that I
          needed to tick off before I start doing actual sound design work, but
          this shifted my mindset into adding more intention with my recording.
          I could experiment with textures and performance to transform regular
          sounds into something that would fit hte emotional tone of the
          project.
          <br />
          <br />
          The last article I read is by Sonnenschein, "Sound design: The
          expressive power of music, voice and sound effects in cinema". It
          talks more deeply into creating atmosphere, and especially reverb and
          other spacial effects can play a crucial role in establishing a
          storytelling place and guides audience's emotional and perceptual
          focus. This directly connects to by attempt to design the atmosphere
          of a church which is a setting in one of the animations I am working
          on. In my draft, I leaned heavily of reverb to create a sense of
          scale, but after reading this, I realize I might have prioritized size
          over clarify of my work. Sonnenschein'd discussion of how sound
          environments shoould evolve dynamically made me think of reverb as
          something I should modulate and change across scenes, which I will
          definitely keep in mind for the future scenes of the animation.
          <br />
          <br />
        </p>
        <h3>Artistic Research</h3>
        <p>
          Ben Burtt, who is best known for his work in Star Wars, is a foley
          artist that I looked into this week. He invented the sounds which have
          become icons in culture, and he approached it by using unexpected
          sources for sound. He shows that sound design is about creativity, not
          just accuract. I realized I could adopt a similar mindset for my own
          foley work. For example, instead of trying to find the most accurate
          footstep in the animation I'm working, I could isntead try to find a
          sound that could convey the weight and unease and tension in that
          moment. Burtt;s practive reminds me that emotional believability is
          also an important factor to consider, rather than just realism. I want
          to explore more freely with Foley in my upcoming projects so they can
          carry the sound I want it to express.
          <br />
          <br />
          Chris Watson is another foley artist that mainly captures amazing
          natural soundscapes with amazing detail and is often used in nature
          documentaries. His work shows how sound can transport a listener into
          the place and immerse them in its atmosphere, even without visuals.
          This pushes me to think of environmental sounds not just as background
          noise, but the backbone of the scene. I realize I should listen and
          put more intention in the ambience that I create, experimenting with
          layers of tone, resonance and silence. I want to to make a mix that
          tries to capture a living environment, and shape how the audience can
          feel inside that space.
          <br />
          <br />
          The last artist I looekd into was Suzanne Ciani, who makes electronic
          soundscapes using synthesizers to design immersive textures! She
          created electronic sounds for Atari and Coca Cola ads and was a
          pioneer with proving how synthetic audio could feel tactile and human.
          I really like her ability to make electronic tones sound like they
          have so much character and movement. I would like to see how I could
          experiment with this in the future, maybe in the game I am designing
          the sound for.
          <br />
          <br />
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I just researched how to record good foley, taking
          inspiration from professional foley artists.
          <a href="https://www.youtube.com/watch?v=amNxmSVYc34">This</a> video
          explores how professionals get ready and set up for recording, with
          most of the work before the actual recording date. Whenever I look at
          doing foley recordings I always just grab a microphone and look at the
          list of things I need, which gets the job done, but I noticed not as
          well as I hoped. From this video I saw that they have a specific
          recording schedule, packed with information like what props will be
          needed, the duration needed, where the recording will be held, etc. In
          the future I want to be more organized with my shooting schedule,
          similar to this so I know what exactly I need without just winging it.
          <br />
          <br />
          For the actual recording they had a few helpful tips that I will use
          in my future foley recording sessions. I tend to just make generic
          sounds and manipulate it to fit the scene later, especially if it's
          something generic like clothes rustling or such, but I see the benefit
          in actually recording while watching the visuals. It seems like such a
          simple thing, but because I don't have a studio and I tend to record
          everything myself, I find it difficult to put a screen down where I
          can see the visuals of what I'm recording, but still holding up the
          microphone (since I don't own a stand) and also making the sound
          needed. In the future I will probably ask for some help, or at least
          borrow a mic stand to make it easier on myself.
          <br />
          <br />
          The video also talks a lot about how certain sounds can be scarily
          close to other sounds. The famous example of pan frying bacon sounding
          like rain is a prime example of this. I want to be more creative with
          my ideas, and not be such a purist when it comes to sounds, as I tend
          to like recording the exact same sound that is in the film, even if
          there could be an easier version.
        </p>
        <h3>Progress Report</h3>
        <p>
          I was very busy this week again with my flight and then had a trip to
          the emergency room (I am fine!) but I was able to get a draft down for
          the animation 'The Reconnect'. Caroline the animator sent me the first
          scene of her animation and sent me a little brief as well as what
          sounds she imagined for that scene.
          <br />
          <br />
          She mentioned footsteps playing a main role in the animation, so I
          started with that. It was a bit tedious to get the timing right but I
          got it in the end. I layered a clip of wood creaking, as well as
          isolated footsteps so the individual footsteps would be a bit clearer
          and wouldn't get drowned in the creaking noise. Since the setting of
          this scene was in a church, I did some pretty spacious reverb to get
          that wide sound, and put the wetness lower for the isolated footsteps,
          again to be a bit more clear. Since there were 2 pairs of footsteps, I
          made the closer ones with a bit less space and delay, so the listeners
          can differentiate them, as well as they are in a more walled off area
          than the other character.
          <br />
          <br />
          <img src="photos/week5/1.png" alt="" />
          <br />
          <br />
          I then went and layered a few horror ambiences, trying to focus on
          having the full frequency range in the scene. I did some low humming
          drones, a creepy airy atmosphere sound, as well as a high pitched
          chimes to add to that creepy atmosphere. I also tried to experiment
          with a sample of a choir singing, and pitching that around, but I'm
          not sure if that vision is what the animator wanted, so I've sent her
          varying sound design works to see which one she likes the best.
          <br />
          <br />
          <img src="photos/week5/2.png" alt="" />
          <br />
          <br />
          Finally I added some finishing touches! I added a heartbeat which I
          automated to get increasing louder and faster to help build tension. I
          also added a riser near the end to accompany the buildup of the rising
          heartbeat. Other than that, I added a few shaky breaths of both the
          person dying at the start, as well as another shaky breath when the
          girl sees the scene and gets scared.
          <br />
          <br />
          You can watch the first draft
          <a href="https://youtu.be/bs8I764QiyE">here</a>! I think a bit of the
          ambience is lacking, I think I could have made it a bit more
          expansive. Caroline mentioned she wanted it to be "witchy" but I am
          not sure how to achieve that so I will be sure to ask her then add it
          to the second draft! I'm excited to hear the feedback for this draft.
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 6 ---------------------------------------------- -->

    <details class="week" id="week-6">
      <summary>Week 6</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This week in class we analysed the opening sequence of A Bug’s Life in
          three different times, one with effects only, music only, and then the
          combined track. Watching the film with only effects highlighted just
          how minimal yet purposeful the sound design is. The crickets, wind,
          and water effects built a naturalistic atmosphere, while footsteps
          were much heavier than I initially expected, maybe I expected it to be
          lighter because they were ants! That weight gave the characters a
          tangible presence and energy within the scene. I actually was shocked
          with how little effects were used. I alsways assumed there was more,
          but maybe because it is a children's animated film, it would have less
          than something recorded with real actors and such. I noticed how many
          environmental sounds faded away whenever the characters returned to
          focus, suggesting that hierarchy in the mix is used deliberately to
          guide attention.
          <br />
          <br />
          When listening to the music track separately, it revealed how much it
          shapes the emotional soundscape. There were stings placed on small
          visual actions, like the bass note hitting when the ant is struck by a
          falling fruit. At other times the score helped the flow of the
          narrative, for example when everything is calm, the melody was light
          and playful, but when problems emerged the bass slowed and thickened,
          and the music grew louder to match the threatening buzz of a chainsaw.
          <br />
          <br />
          Hearing both together showed how layers of sound work across the
          frequency spectrum. Music filled the higher registers while the foley
          sat lower, producing a more complete sense of space. What stood out
          most to me was how much character movement can be conveyed through
          sound alone, like their clumsiness, confidence, and awkwardness were
          all suggested simply by timing and emphasis. The exercise reminded me
          that sound is not just accompaniment to visuals, but a central force
          that defines mood, guides attention, and can even reshape how we
          interpret characters and their behaviors.
          <br />
          <br />
        </p>
        <h3>Academic Research</h3>
        <p>
          I read an article by Philipp Schmerheim and Tobias Kurwinkel titled
          "Sound Design in Children’s Film". It explains how sound in children’s
          media can play a crucial role in guiding perception, since younger
          audiences often rely on audio cues more than visual details to
          understand what is happening in scenes. The article pointed out how
          important it is to create a hierarchy of sounds so that key actions
          are easy to follow without overwhelming the listener. This perspective
          changed how I think about my own work, because I usually focus on
          adding detail and texture, but this showed me that sometimes clarity
          and restraint are more powerful, especially when designing for
          children. I can apply this to my own work even if the content I'm
          producing isn't really meant for children. I can do this by being more
          deliberate about which sounds need to be in the foreground, and which
          can fade back, ensuring that the main story always comes through
          clearly.
          <br />
          <br />
          I also read a scholarly reflection on sound design in contemporary
          animated films such as Coraline, The Incredibles and Bolt. The article
          compared how each film tailored its approach to sound design to suit
          its genre and mood. Coraline used minimal soundscapes to create a
          strange and unsettling atmosphere, while the other two relied on rich,
          layered mixes to heighten the action and energy in scenes. What stood
          out to me was the emphasis on clarity across all styles. This reminded
          me that sound design should never be about filling space, but about
          supporting the narrative with intention. I found this inspiring
          because it encouraged me to think more flexibly, and allowing a
          minimal approach to make a scene more effective, while at other times
          layering multiple sounds can build intensity. In both cases, clarity
          should remain the goal, and I want to carry that mindset into my
          future projects, especially with my tendency to add too many sounds.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For my artistic research, I decided to look into kids animated films
          again, and I looked into the work of Randy Thom, who has done
          extensive sound design for animated films including The Incredibles
          and How to Train Your Dragon (which is one of my favorite animated
          films of all time). Thom talks about how children’s films often
          require a very careful balance between realism and exaggeration
          assounds need to feel believable enough to anchor the world, but they
          also have to be expressive and playful. For example, he often layers
          real recordings with stylized or exaggerated effects to give everyday
          actions more personality I can use this for my own work by designing
          sound (especially if I ever do sound design for children's media) by
          finding the right degree of clarity and character, rather than aiming
          for complete realism. It made me realise that when I work on projects
          with younger audiences in mind, and even older audiences, I should
          think about sound as a way of storytelling on its own, where even
          small effects can reveal emotion or intention.
          <br />
          <br />
          I also decided to look into horror sound designers, and I found Gary
          Rydstrom, who has worked on films like Jurassic Park and A Quiet
          Place. Rydstrom describes horror sound design as being less about what
          is heard and more about what is implied through silences, distant
          rumbles, or subtle textures that can create more fear than overtly
          loud sounds. He also notes how horror often uses low frequencies and
          dynamic shifts in volume to keep audiences on edge, exploiting the
          body’s physical reactions to sound. This made me reflect on how
          different the goals of sound design can be across genres like where
          children’s films need clarity and guidance, horror thrives on
          ambiguity and tension. For my own projects, it pushes me to think
          about silence and restraint as creative tools, and not just the sounds
          themselves. By considering how audiences are meant to feel, I can
          adapt my design strategies to match the emotional core of the genre.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I watched a tutorial on the fundamentals of EQ and how it
          can be applied in sound design. The video explained how EQ isn't
          really just about boosting or cutting frequencies to make something
          sound better but about carving out space so each element in a mix can
          be heard clearly. It broke down the frequency spectrum into ranges,
          like sub-bass, low mids, high mids, and highs, and showed what
          typically lives in each area. It kind of reminded me when I created
          the sound design for Shelves by Gina Moore, when I was working with
          orchestra. All the different instruments focus on different frequency
          ranges to create this full sound! I just thought that was pretty cool
          and linked together something I've done before to something I focused
          learning this week.
          <br />
          <br />
          I found it especially useful when he demonstrated how too much low-mid
          energy can make a sound muddy, while carefully reducing those
          frequencies can suddenly make a mix feel clearer. Reflecting on this,
          I realised how often I tend to layer sounds without thinking about how
          they overlap sonically, which can cause clutter. EQ gives me the
          technical ability to separate those layers, making each one
          purposeful. In my own projects, I definitely can see myself using EQ
          to sculpt sounds to sit in their own space and highlighting the
          qualities that make them expressive. I will definitely try this next
          week for The Hollow Child draft.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week I mainly did work on The Hollow Child animation! I had a
          chat with the animator Tara in person, and we went scene by scene of
          what her plans were, what she expected and gave us a timeline to have
          the draft done next week! Which was kind of really quick for me which
          is why this week I mainly only did this project.
          <br />
          <br />
          Tara had hired voice actors for the different characters and sent us
          the voice recordings for it, so the first thing I did was do the
          timing for all the laughs and sounds for the ghost girl:
          <br />
          <br />
          <img src="photos/week6/girl.png" alt="" />
          <br />
          <br />
          There wasn't any actual dialogue or speaking, but there was a lot of
          laughing and random sounds, so I really focused on making sure I
          didn't miss anything. I then did the same for the werewolf:
          <br />
          <br />
          <img src="photos/week6/werewolf2.png" alt="" />
          <br />
          <br />
          It was pretty difficult in general because obviously the animation
          isn't finished, so I don't think the timing will be correct, but it is
          an easy fix and just a matter of moving the timings around once the
          animation is finished! These steps took me a surprisingly long time,
          especially listening to the MINUTES of each characters' different
          sounds and organizing them based on what it sounded like to me/what I
          will use it for, and then trying to find ones that fit the scene. I
          would have liked for more sounds from the werewolf voice actor, so I
          might ask if that is possible for the final sound design.
          <br />
          <br />
          I then worked on the intro, trying my best to create a lonely
          atmosphere, and focusing on the breaths of the werewolf, as well as
          the wind. I used reverb to achieve this:
          <br />
          <br />
          <img src="photos/week6/reverb.png" alt="" />
          <br />
          <br />
          And I automated this reverb so when the camera zoomed out on the
          werewolf laying down, the size and dry/wet ratio would go higher to
          get that sense of vastness. I also made it so at the start, the breath
          and sounds were mono, and as the reverb automates to get larger, the
          stereo width gets wider!
          <br />
          <br />
          <img src="photos/week6/automation.png" alt="" />
          <br />
          <br />
          I then made a reverb bus, and put all my tracks to have varying levels
          of reverb, including the girl's and werewolf's sounds for later in the
          animation. After this I also added some diegetic sound like crows, and
          birds chirping, as well as wind and grass/leaves rustling. This set me
          up to create the ambience and non-diegetic sounds next week!
          <br />
          <br />
          Other than working on The Hollow Child, I also followed up with my
          other projects, and am waiting to get more scenes to do sound design
          for The Reconnect! The animator said she would get them to me by the
          end of the week! I also started to master and mix some of my song that
          I made, as well as finalized the structure since I wasn't a fan of it
          before! I want to add a guitar solo to it!
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 7 ---------------------------------------------- -->

    <details class="week" id="week-7">
      <summary>Week 7</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This weeks class mainly centred around our project presentations,
          which provided a valuable checkpoint to evaluate our progress so far
          and make sure everything was on track. I also enjoyed looking at
          everyone else's projects and the sounds they made! Preparing the
          slides helped me organize my ideas clearly, as well as remind me and
          allowed me to reflect how each project connects to the goals of each
          of my collaborators. I aimed to make the presentation informative but
          concise to show both creative and technical development, but keep the
          attention of everyone watching. I also wanted to leave space for
          feedback from everyone in the class, since even if something is my
          intention, doesn't mean that it comes out that way.
          <br />
          <br />
          The feedback that I received was extremely helpful! For The Hollow
          CHild, the comments I got highlighted how important the final sequence
          will be to the emotional impact of the animation. Even though the
          current ending music was only a placeholder, it just reminded me that
          the final moments need to feel cohesive and powerful, and Maria was
          present at the feedback session so she is aware of it as well. I was
          also advised that the wind ambience throughout the scene felt too
          static, which made is sound dull and unalive. This was useful to hear
          because it encourages me to revisit the spacilization and dynamics of
          the wind, perhaps by layering subtle movements, low frequency swells
          or maybe modulation effects to create more presence and variation.
          Someone in the class suggested using EQ to do this!
          <br />
          <br />
          For Fired Before Hired, I received feedback on the menu music, mainly
          suggesting to add a small amount of reverb, especially to the horns to
          make the mix sound less dry and out of place. This was an easy but
          meaningful adjustment and it made me realize how much space and
          ambience can affect the tone, even if I just changed one tihng.
          Overall, the presentation week was an important oportunity to pause
          and assess my direction and reminded me the value of feedback and
          could help me stage the next phase of refinement!
        </p>
        <h3>Academic Research</h3>
        <p>
          While developing the Fired Before Hired menu music, I wanted to
          capture the relaxing qualities of bossa nova that is often seen in
          office music/elevator music! I read an article by Perrone 'Masters of
          Contemporary Brazilian Song' that talks about bossa nova being defined
          by its rhythms, jazz harmonies and instrumental delivery to create a
          sense of calm intimacy that contrarsts the energy of samba. The genre
          often relies on a classical nylon string guitar with more subtle
          percussions and chord extensions that add harmonics like 7ths or 9ths.
          This understanding helped me understand what I wanted in my own sound,
          using a guitar loop as a base and a vibraphone to create an airy
          texture. Bossa nova highlighting balance and restraint made me think
          carefully on how to make things expressive quietly, which was the key
          to making the loop feel natural and calming for players!
          <br />
          <br />
          I also researched into menu music, specifically in game design and how
          it sets the tone and guides the players emotion going into the game.
          Playing with Sound: A Theory of Interacting with Sound and Music in
          Video Games by Collins explains that menu music functions as a form of
          emotinoal priming, establishing the atmosphere and expectation before
          the gameplay actually begins. The most effective themes use loopable
          strucutres and a moderate tempo. This framework is something I wanted
          to explore with Fired Before Hired, as it appears before the intense
          gameplay, so I wanted something more calm to contrast it. I want to
          apply a looping piece that ensures endless playback because we won't
          know how long someone would stay in the menu screen. This helped me
          refine how I wanted to arrange the menu music, but also gave me an
          insight of how the sonic design can contribute to user experience!
        </p>
        <h3>Artistic Research</h3>
        <p>
          While researching examples of menu music, I went back to one of my
          favorite games growing up Life is Strange! Every time I hear that menu
          music I get hit with a wave of nostalgia, and its one of the only
          games where I sit and listen to the menu music for a along time. I
          wanted to see how and why it was so effective. Life is Strange uses
          sound to reflect its emotional tone, the main menu theme composed by
          the band Syd Matters, establishes a reflective and nostalgic
          atmosphere through its use of acoustic guitar, ambient pads and soft
          reverb. The track feels so intimate and melancholic, which mirrors the
          game's focus on memory, choice and adolescnece, which is why I think
          that this menu music is so compelling. It takes users into this calm
          space that aligns with the narratives emotional tone and also loops
          infinitely, which maintains immersion! I love this theme music so much
          (and side note I am very excited for the show they are making) and it
          helped me realize that I don't need complex melodies and instruments,
          but just subtle mood could be just as powerful.
          <br />
          <br />
          Another game I particularly enjoyed the main menu sound design was
          Detroit: Become Human, and achieves a very different effect. The story
          focuses around robots and androids, and the main menu features this
          girl that talks to you alongside the story, and changes what she says
          based on your choices. I thought this was extremely cool and also
          creepy and perfectly fit what the story was about! It was a creative
          way that a main menu can directly manipulate the player's emotions
          going into the actual game. In terms of the actual music, it uses
          orchestral strings and also very airy, ethereal vocals to convey a
          sense of future and moral tension. The piece slowly builds and also
          changes depending on the player's progress, for example when the story
          reaches darker points, the music shifts in tonality and
          instrumentation. This adaptiveness adds depth to the player's
          emotinoal engagement, especially when paired alongside the main menu
          android!
          <br />
          <br />
          Another strong example is Firewatch, where its menu music combines
          warm acoustic guitar and ambient drones to convey this sense of
          solitute and quiet, especially since that the game is set in a quiet
          forest and is about human connection and isolation. The track fades
          into the background but creates emotion through its harmonic structure
          and it demonstrates how environmental and emotional themes can be
          translated into sound through its texture and pacing, and not just the
          melody. Good exampels of main menu music helps build the world through
          sound and I want Fire Before Hired's menu music to feel appropriate
          for its statirical and comedic nature, but also using bossa nova to
          mirror the game's light heartedness.
        </p>
        <h3>Technical Research</h3>
        <p>
          To create the menu music, I watched this YouTube tutorial,
          <a href="https://www.youtube.com/watch?v=MDEzMTQXAs8"
            >How To Make Elevator Music</a
          >. Despite its 2 minute runtime it provided such a clear and practical
          overview on the stylistic and technical choices that define this
          genre. Although it only really goes into bossa nova as elevator music,
          now elevator music as whole. Anyways, this video breaks down how
          elevator and lounge musicc can typically blend elements of bossa nova,
          easy listening and jazz harmony and using a relaxed swing for a light
          instrumentation. Bossa nova works so effectively in this context
          because of its subtle sound, smooth chord progression and a
          non-intrusive melodic phrasing. It talks about layering a range of
          insturments like soft percussion, electric piano, brushed drums and a
          vibraphone to create a rich textural environment without overwhellming
          the listener.
          <br />
          <br />
          Watching this helped me think more critically about how I would make
          my own arrangement for Fired Before Hired. I realized that the menu
          music I wanted to create was a catchy, melodic tune but more
          importantly, a comfortable sound that can repeat endlessly without
          feeling annoying.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week included a lot because I'm also counting the break week! I
          was also thinking of dropping the music video project because things
          have come up unexpectedly and I still believe that I can hit the 100
          hour mark even without it, considering how much work I've done so far.
          This was a mix between the videographer pulling out, and me being
          unable to do a Pharos project because of how late I was given notice.
          If I have time, I might do the music video myself since I already the
          song worked on, but it really depends on time in regards to making the
          music video myself.
          <br />
          <br />
          For the projects I am continuing to pursue, I mainly did a lot of work
          for The Hollow Child since the deadline was closer compared to the
          other projects.
          <br />
          <br />
          I then started to make the ambience for The Hollow Child. I was having
          a bit of trouble because I was meant to create a forest soundscape,
          but empty, so no animal sounds, no rain, etc. I decided to just add
          wind, and also wind chimes to create a musical and creepy sound
          effect, which the animator liked! I used automation and EQ and pitch
          shifting to make the chimes less predictable and different each time
          it plays.
          <br />
          <br />
          <img src="photos/week7/7.2.png" alt="" />
          <br />
          <br />
          I added a lot of drones to add this feeling of dread when the werewolf
          looks around, especially when he is hearing the laughs in the distance
          , but hasn't seen the girl yet. I tend to go towards the deeper and
          lower sounds because I know that Maria is going higher for the music
          effects, and I want don't want to muddle her sounds. I did the same
          droning and impact when the werewolf sees the dead animals, and he
          starts to realize who and what the ghost is.
          <br />
          <br />
          <img src="photos/week7/7.1.png" alt="" />
          <br />
          <br />
          When the girl gives the werewolf a gift of the dead rabbit, I thought
          it would be cool to echo the laughs to create a creepy atmosphere. I
          was also thinking of doing a high pitched sound, like the effect of
          when you hit your head, but I decided to see what Maria does for the
          music, and if it will be too much if I add it in. Instead, I did a
          heartbeat, but I also wasn't sure if the werewolf guy would even have
          a heartbeat, so I need to double check with the animator.
          <br />
          <br />
          For the end sequence, it is very music heavy so I just added a
          placeholder song for now while I wait for Maria's track. I tried to
          make it very sparkly and big, but I do think it needs more, but I
          think I will experiment around with it once I have the song.
          <br />
          <br />
          <img src="photos/week7/7.3.png" alt="" />
          <br />
          <br />
          So overall, I just basically added all of the diegetic and
          non-diegetic sound, not including the music. In the coming weeks I aim
          to do the automation and refinements as the animator sends in more of
          the finished parts, so I can properly line up the sounds.
          <br />
          <br />
          You can watch it here:
          <a
            href="https://www.youtube.com/watch?v=JH6n9ncHDc8&list=PLvt9uDYuOQg7i28uXG7PKJC8Hk7bAglt2&index=4"
            >https://www.youtube.com/watch?v=JH6n9ncHDc8&list=PLvt9uDYuOQg7i28uXG7PKJC8Hk7bAglt2&index=4</a
          >
          <br />
          <br />
          I also started to do work on Fired Before Hired, the game I am doing
          the sound design for. I started with doing the menu music! I gave the
          main game designer a few different clips of different videos to see
          what his vision of what "Office Music" sounded like. This ranged from
          jazz, elevator music, waiting room music, etc. What resonated with him
          the most actually ended up being this waiting room music that had a
          bossa nova vibe to it, so that was the direction I wanted to head
          towards.
          <br />
          <br />
          I usually think of either a soft electric piano or a classical guitar
          for the main chords of a bossa nova song, so I started with looking
          for classical guitar loops on Splice. I would have done it myself but
          I actually don't have a very good mic to capture the sound of a guitar
          and not a lot of quiet areas in my house, and most of the recordings
          of guitar I do are through an audio interface with an electric guitar.
          <br />
          <br />
          Anyways, I found this classical guitar loop playing different chords
          in D minor and I loved it immediately! The artist actually played a
          piano version but I liked the guitar so much more. I imported that
          into Ableton and got my MIDI keyboard and used a soft electric piano
          sound and played around with what chords I wanted to play on top of
          it. I put a lot of focus into making it very soft, so it supports the
          guitar in the background, rather than trying to overpower it because I
          wanted the guitar to be the main focus. I just did a basic 4 chord
          structure and looped it.
          <br />
          <br />
          Then after that I noodled around with a vibraphone and came up with a
          simple melody. Since I was aiming for kind of elevator music, I
          thought it'd be fitting to go up and down the notes in the key and the
          simplicity helps it pass as like waiting room/office music.
          <br />
          <br />
          I thought it needed some percussion so I spent quite a while trying to
          search for one that fit the vibe, and I eventually found one! I added
          some maraccas and a tambourine as well to make it fit better with the
          waiting room theme, the percussion was very light and friendly and
          playful, which was exactly the vibe I was going for! Everything I just
          talked about can be seen here
          <br />
          <br />
          <img src="photos/week7/1.png" alt="" />
          <br />
          <br />
          While I was seacrhing for the percussion, I also found this horn
          sample that I really liked. I was drawn to it for some reason. I added
          it in and asked the game developer if he liked it and he actually
          really liked it and said it was "fire" (his words)! Even though its
          not typical for waiting room music or anything, I think it added to
          the jazzy bossa nova vibe and gave the sound a bit of texture.
          <br />
          <br />
          After this I then started cutting up everything into 3 sections; the
          intro, main loop, and end. Since it is a song for a game, I need to be
          able to have 3 clips of it so I could code that the intro plays, then
          straight after the main loop plays for however long, and then it ends.
          For the intro, I took an open high hat sound and reversed it to work
          as a riser for the start of the percussion part. You can see me do
          this here:
          <br />
          <br />
          <img src="photos/week7/2.png" alt="" />
          <br />
          <br />
          Now that I had all of the parts done, it was just a matter of actually
          assembling the song. I didn't want all of them to play at the same
          time since it makes for a very flat song, theres no variation and
          won't be very interesting. I decided that the main rhythm, which was
          the classical guitar actually doing the bossa nova, would play
          throughout, and the rest would change/get cut off, etc.
          <br />
          <br />
          When the horns were palying, I felt like there was a lot going on, so
          I decided for a part of it, the percussion would stop, highlighting
          the horns and the rhythm, then come back. This part turned out to be
          my favorite part of the entire menu music song! After the horns, I
          felt like the main melody was just being repeated, so I kind of wanted
          to add like a 'bridge' or a B section, but wanted to keep it simple as
          it was just a short loopable song. I just decided to do a little play
          around with the melody, so at least there was some melodic difference
          between that section, and the rest of the song!
          <br />
          <br />
          Then that was done for the menu music! I exported a version where it
          was the intro, main loop twice, then outro so I could gather feedback
          with the song as a whole. The developer absolutely loved it so I'm
          very happy. You can see all the final parts here (I removed the horns
          for some of the parts, but the screenshot is outdated, but everything
          else is right):
          <br />
          <br />
          <img src="photos/week7/3.png" alt="" />
          <br />
          <br />
          You can listen to it here:
          <a href="https://youtu.be/SjpkX_O2ez0"
            >https://youtu.be/SjpkX_O2ez0</a
          >
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 8 ---------------------------------------------- -->

    <details class="week" id="week-8">
      <summary>Week 8</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          Although I wasn't able to attend class this week, but I still explored
          the weekly theme of jingles through my own independent research! I was
          curious about what made such short pieces of music so memorable, and
          what makes them effective as a form of communication. Jingles are
          interesting because it is not only just sound/music but also branding.
          It needs to delivery identity , tone and emotion with just a few
          seconds. Every note and sound choice has to carry meaning.
          <br />
          <br />
          While researching I realized that succesful jingles share three main
          traits: clarity, repoition and revognizability. Jingles usually rely
          on simple melodic intervals and hooks to create a logo with audio.
          Even though I am not creating any jingles for any prohects, something
          linked in my head about how sound can establish identity in other
          contexts such as games, animations, etc. It can embody a character or
          world, which is something that I was talking to Maria about for The
          Hollow Child, we wanted to create a melodic motif that plays when the
          ghost appears! I realize it is a similar thing!
          <br />
          <br />
          I also started thinking about how certain melodic shapes or timbres
          can become associated with emotion or nostalgia! It reminded me that
          people can respond to sound subconsciously and that motifs can carry
          emotional weight when used purposefully. I think it made me solidify
          my decision to ask for motifs for the ghost! Overall, exploring
          jingles gave me a renewed appreciation for sound design!
        </p>
        <h3>Academic Research</h3>
        <p>
          For some academic research, as I mentioned in my reflection, i
          explored jingles and how they are effective and memorable. 'Effects of
          popular music in advertizing on attention and memory' by Allan talks
          about the interaction between melody, rhythm and verbal phrasing to
          form an identity, as I mentioned earlier. It also highlights
          repitition and simplicity to allow the jingle to stick in the
          listeners mind even after just one single listen. It is also important
          to make sure it matches the brand's mood and creating a clear tone
          within seconds!
          <br />
          <br />
          Building on that, I also researched how to create music that loops
          seamlessly, especially for the Fired Before Hired wave system that I
          am working on! It was talked about one of the previous articles I read
          in one of the other weeks, Collins explains that loopable game music
          relies on modular composition, and that musical phrases are written to
          resolve back into themselves so the listenrs don't perceive an
          endpoin. She also describes how transitions can be managed through
          layer and dynamic mixing rather than hard cuts. This informed my
          decision and also lined with what Santino said I should do for the
          wave music! A good loop design depends on the phrasing and rhythm just
          as much as the intensity stages!
          <br />
          <br />
          I also looked into h ow to build musical intensity and tension, since
          the wave music needed to escalate without overwhelming the listener.
          'Writing Interactive Music for Video Games: A Composer's Guide'
          discusses how intensity is often achieved through one or more of these
          points; density, contrast, acceleration. Rather than just increasing
          volume or adding more instruments, the variation of articulation like
          staccato vs legato and layering is more important for listner
          engagement! This helped me refine how I approached my wave
          progression. Instead of making each section louder, I should focus on
          introducing subtle rhythmic and instrumental changes to create motion
          and movement I want to make the viewers feel the escalation rather
          than hear it mechanically.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For artistic research, I looekd into Gordon's work on DOOM, which is
          one of the most striking examples of sound design and structure
          creating physical and emotional intensity. His process is what he
          calls 'dynamic layering' where he records multiple different stems of
          instruments and sounds that can be mixed interactively based on player
          behavior! Which I thought was pretty cool. In interviews he mentioned
          using aggressive compression and distortion to create a wall of sound,
          but still highlight harmonic movement underneath that wall of sound. I
          want to emulate that with my wave music for Fired Before Hired, a big
          wall of sound with a layer of melodic elements so its still tightly
          knit together. It encourages me to make and think about intensity as
          storytelling rather than just volume.
          <br />
          <br />
          I also wanted to explore a simpler example of loopable music, and
          looked into Super Mario Bros themes to show how simplicity and
          repitition can create a different kind of intensity. Kondo, the
          composer, has spoken about designing his music to feel part of the
          gameplay rhythm, matching the players jumps and movement and timing.
          Relying on tight phrasing and small melodic loops that feels
          satisfying to hear so the user wouldn't get annoyed if it played
          endlessly. He also added subte variations to keep it fresh and less
          repetitive! Which I think would be cool if I were making a longer
          game, and is important to keep in mind.
        </p>
        <h3>Technical Research</h3>
        <p>
          For this weeks technical research, I watched this short tutorial that
          explained how to consutrct cinematic loops that build tension without
          obvious start or end points. The key point was just creating a perfect
          point and compositional balance and ensuring that every bar resolves
          naturally into the next. Some points that were brought up were ending
          phrases on suspended or unresolved chords so that people naturally
          want to hear the next chord, which is the start of the bar. Using
          motifs could also maintain that forward momentum, This helped me
          rethink how to structure each of my waves so that the transitions
          between intensity levels would feel organic when stacked.
          <br />
          <br />
          Another technical aspect I was focused on was autmoation and dynamic
          mixing. I watched another YouTube video about advanced automation in
          Ableton! This could help me create the illusion of energy building
          even when the core tempo or melody remains constant. Things such as
          reverb, delay feedback and volume! It makes me think of using low
          frequency EQ automation and slowly opening filters over time to reveal
          more of the low end instruments to make this tonal widening to make
          the feel like it was expanding outward.
        </p>
        <h3>Progress Report</h3>
        <p>
          This week, I started creating the wave music for Fired Before Hired.
          This was difficult at first because it was meant to go up in intensity
          as the players finished each wave, but obviously everyone is not going
          to finish the same wave at the exact same time, so I had to figure out
          a way to make sure the music doesn't suddenly stop and start in a
          different place, and to keep the immersion of the player. I was
          talking to Santino and he gave me the idea to make sure everything was
          a loop. After that I could export each one as a wav file, and then
          mute and unmute the tracks as needed with coding in Unity, which I
          thought was a great idea!
          <br />
          <br />
          I began searched for different loops and samples on Splice to see what
          I could use as my base, and build it up from there. The game designer
          mentioned that it would be funny if the wave music was super serious
          and orchestral and big, because it severely contrasts the visuals.
          Since the game was satirical and comedic, I thought it fit the vibe
          and decided to go towards that direction, and found this super cool
          strings to use as a starting point. I duplicated that 10 times so I
          could have a visual layout of what happens at each stage, the final
          product will just all be one loop stacked on top of each other
          <br />
          <br />
          <img src="photos/week8/8.1.png" alt="" />
          <br />
          <br />
          I then went to find some shakers for a lighter percussion that I
          thought could be used at the start, which will change to a bigger one
          when the player gets into the more intense levels.
          <br />
          <br />
          I also recorded a cello playing the bass note and letting it ring out
          basically the entire song. I wanted to have each part pretty simple
          because I knew that by the end, if I made each part too complex it
          would definitely be too busy, and take away from what I wanted. I then
          also added a melody and counter melody (stuff that I learned when
          doing Gina Moore's Shelves!) with a violin, which I changed to piccolo
          later on!
          <br />
          <br />
          <img src="photos/week8/8.3.png" alt="" />
          <br />
          <br />
          I thought it would be cool to add like a DUN DUN (I don't know how
          else to describe it) to help build up anticipation and build tension.
          ALongside that I added another cello to add a lower bass that wasn't
          just playing the root note the entire time.
          <br />
          <br />
          <img src="photos/week8/8.2.png" alt="" />
          <br />
          <br />
          At this time I also decided to add the more intense drums, I found
          different percussion samples that I thought fit the vibe I was going
          for, and also had varying levels of intensity, so I used that to help
          build up the intensity for each wave. This worked exptremely well and
          I don't think that the final product would have been as intense if I
          didn't decide to do this.
          <br />
          <br />
          <img src="photos/week8/8.4.png" alt="" />
          <br />
          <br />
          From this it was just a matter of finding smaller things to add for
          each section. This ranged from finding different horns with varying
          intensities, and I even found another violin sample with a cool
          melody, and layered that on top of the exisiting one. I even added
          some piano in there that added more rhythm to the entire sound.
          <br />
          <br />
          <img src="photos/week8/8.5.png" alt="" />
          <br />
          <br />
          And that was basically done! I sent it off to the game designer for
          some feedback and am waiting to hear from him regarding that. Each
          section didn't have to be very long since when I was playing the game,
          I pretty much only spent around 10 seconds in each wave, but I also
          have a lot of experience playing FPS games and such, so I had to keep
          in mind that other people may spend longer.
          <br />
          <br />
          You can watch the full thing here:
          <a href="https://youtu.be/zWbvsTAuDr0"
            >https://youtu.be/zWbvsTAuDr0</a
          >
          <br />
          <br />
          Other than working on this wave music, I also started to refine The
          Hollow Child, because the animator reached out with more finished
          parts of the animation. All the work I did last week was all on top of
          the draft, and I let the finished animations build up a bit more
          before I started to refine, so I had a lot to catch up on! ALthough
          this basically just consisted of making sure all the timing matched up
          to the actual animation instead of the draft. I didn't add any new
          sound, just moved things around in the timeline and did some
          automation and a tiny bit of FX editing, but not that much.
          <br />
          <br />
          I don't want to do much more refining work on The Hollow Child until
          Maria sends through the music part, so I know how much I should do for
          foley, as I think there are a few parts I know I'll have to cut out
          because Maria is making the music kind of like ambience for some
          parts.
          <br />
          <br />
          This week I also did the basic ambiences for the scenes I received for
          The Reconnect! I still didn't have the final narration or the final
          sequence, so I just added sound to create the atmosphere that I was
          going for.
          <br />
          <br />
          For the classroom scene, I was having trouble figuring out what I
          wanted to do for atmosphere. I decided to just focus on the ticking
          clock, which I automated to get lower in volume because I wanted to
          highlight the rest of the sound layer on too. I did some foley for
          when the chair/desk moves and the clothes rustling for realism. For
          the atmosphere I found this sound clip of people from a school talking
          and I fiddled around with the EQ (taking away the high end) to make it
          sound like it was muddled and coming from outside. I also added a
          basic room tone to fill in the empty space and to give it this kind of
          boring atmosphere to contrast the next scene.
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144221.png" alt="" />
          <br />
          <br />
          Then it goes into an interrogation scene, switching between the
          protagonist and antagonist, and in the police station. The animator
          told me to keep the same creepy ambience throughout both, and there is
          a narration throughout the entire thing. I started off with this
          creepy cave ambience I found, and mixed that with this sample of a
          violin bow making creepy high pitched sound. I then added another
          sample of deep footsteps to give the antagonist this powerful
          prescence. Mixed that with risers, booms and impact for the scene that
          reveals who the antagonist is to build up the tension and a big
          release for one of the bigger parts of the animation.
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144235.png" alt="" />
          <br />
          <br />
          From then it was the last scene until the final sequence. As a
          reminder the animator friend is doing the music at the end! So this is
          the buildup before then. I used a mix of recorded ambiences and from
          sample and sound libraries. I added a lot near the end, with a low
          drone and a high wind to build and overwhelm the viewer before the
          final riser and release. I also added a fire sound because the girl
          touches a candle that turned out to be hot, which I also added sound
          of the girl wincing to!
          <br />
          <br />
          <img src="photos/week8/Screenshot 2025-10-06 144302.png" alt="" />
          <br />
          <br />
          That is just the general ambience of the vibe I wanted to go for, I
          really am just waiting for the narration, as I mentioned in the
          progress presentation! You can watch it here:
          <a href="https://youtu.be/IUlOBy_d0Tg"
            >https://youtu.be/IUlOBy_d0Tg</a
          >
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 9 ---------------------------------------------- -->

    <details class="week" id="week-9">
      <summary>Week 9</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          This class was about the relationship between sound design and music
          in action film sequences and decided to do a bit of my own research
          myself and how these two layers interact to shape tension and pacing.
          Action sequences often rely of precise syncing between the diegetic
          and non-diegetic sound, things like footsteps, impacts, gunfire, etc
          typically merge with percussive elements in the score to create a
          sense of momentum. What stoof out tome was how the most effective
          sequences don't actually seperate sound and music, but it blurs
          together into one cohesive texutre and soundscape. This connect also
          made me think about how I could apply strategies to my own work,
          especially the animations I am working onm particularly The Hollow
          Child. I am definitely going to bring it up to Maria to see how we
          could try and implement it!
          <br />
          <br />
          This week I also received some detailed feedback on The Hollow Child
          from the animator, which helped me see new possibilities for
          integrating sound and music more effectively. I was encouraged to
          expierment with panning when the ghost moves across or off-screen to
          give a even stronger sense of movement and spatial depth! This was
          something that I was planning on doing, but haven't yet while I wait
          for the music to be sent to me to make sure my refinements are
          cohesive with the music. There was also a suggestion to include
          quieter, more minimal moments to heighten tension, basically using
          silence as a narrative device to put the audience on edge. I found
          this advice valuable as I realize I tend to fill space with sound, but
          I realize that restraint can make the next sound hit even harder. I
          was also advised to refine the ambience, maybe experimening with
          adding textures like winter winds, creaking trees, rustling, etc. to
          evoke the setting more vividly. Finally, I got feedback to vary the
          ghost's laughter, beginning subtly and letting it built to a peak
          before dying back down again, creating a clearer emotinoal arc and
          avoids fatigue from the repititon.
          <br />
          <br />
          Overall this week just helped me think about rhythm and restraint in
          how all audio elements flow together, making sure each sound has
          intention and contributes to the large scale of things rather than
          just filling space.
        </p>
        <h3>Academic Research</h3>
        <p>
          Most of this weeks work revolved around Foley so I explored advanced
          Foley editing and returned back to the article I read about before by
          Sonnenschein. He discusses how doley is not just about realism, but
          expression, using subtle editing, layering and processing to reflect
          the tone of the scene. He explains that advanced foley editing often
          involves maniplating frequency content, timing and reverb! Which are
          all things I have researched before! For example, high frequencies
          could be softed to simulate distance, or maybe attacks shortened to
          blend with surrounding sounds. This could be especially useful in the
          footsteps for The Disconnect, applying EQ automation, pitch shifting,
          etc.
          <br />
          <br />
          I also looked into foley techniques that are specifically adapted for
          interactive media and games. The article I read explains that Foley in
          games requires both variation and responsiveness because players can
          often trigger the same sound repeatedly. She describes how recording
          multiple versions of each effect can add randomization, preventing
          listener fatigue and enhances immersion. Also implementing
          randomization in the playback parameters is also another way to
          introduce this. I will record several takes for all the foley I
          design. The author also notes that foley for games often prioritize
          clarity over realism, making sure that the sound communicates what is
          needed, even if it is stylized.
        </p>
        <h3>Artistic Research</h3>
        <p>
          For the research, I was trying to find some cool sound designers. I
          found Randy Thom, a film sound designer and he emphasizes that sound
          should participate in the storytelling, not just decorate it.
          Recording unconventional materials to acheive an emotional timbre,
          like using layers of wind, animal groans, etc to blur the boundary
          between just ambience and also emotion. What I found interesting was
          Thom's belieg that every sound should reflect the psychology of the
          scene. Reminds me that my work, especially in The Disconnect should
          reflect the protagonist's feat and isolation, maybe by adjusting
          reverb or spatial depth. I can create sound that feels like part of
          her perception, rather than just the physical space itself.
          <br />
          <br />
          Schappler, the lead sound designer that worked on The Last of Us Part
          2, explains how his team used foley to emphasize vulnerability and
          presence, capturing the texture of skin, fabric, etc, all in extreme
          detail. The footsteps and breaths and foley were recorded with
          multiple mic perspectives to allow for dynamic layering depending on
          how the player moves and the camera distance. This was pretty cool to
          hear, as he designed sounds based on contextual realism, recording
          sounds that could flexibly adapt to gameplay without losing his
          vision. This informs my decision of recording multiple takes of the
          same thing to make sure it isn't repetitive.
          <br />
          <br />
          Both Thom and Schappler show how foley can function as a bridge
          between the person and the world, and can reveal their emotional state
          and not just used as a grounding tool.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week, I focused on learning how to implement sound into Unity
          without relying on middle like FMOD or Wwise. Since this is a
          completely new skill it took be a bit to grasp, but I eventually do
          want to learn how to use middlware! Anyways I watched a tutorial which
          explained the workflow for importing, triggering, and controlling
          audio using Unity's built-in AudioSource and AudioClip components. I
          briefly used this last year for another class, but nothing this
          extensively. The vieo demonstrated how to assign individual sound
          effects to player actions, and how to script logic that ensures that
          sounds pnly play when certain conditions are met, for example,
          detecting when a player si grounded before triggering a step sound
          <br />
          <br />
          What I found most valuable was learning about randomisation and
          variation using code. It showed me how to create an array of multiple
          sound clips then use a simple script to random select one each time an
          action is triggered. He also explained adding the subtle pitch and
          volume variation in the script to make repeated sounds feel more
          natural, which is exactly what I wanted to do!
          <br />
          <br />
          Another important aspect that was taught was understanding Unity's
          spatial audio system, which allows sounds to feel closer or farther
          away depending on the player's positino. Adjusting the 3D sound
          settings, specifically rolloff curve and spatial blend, I will be able
          to create depth for foley sounds!
          <br />
          <br />
          Lastly it talks about the importance of normalizing audio elvels and
          compressing files BEFORE importing, to prevent inconsistencies between
          different recordings and that I should take more time than I think I
          should when importing and editing the files!
        </p>
        <h3>Progress Report</h3>
        <p>
          This week I worked on The Disconnect! I think most of this blog I was
          calling it The Reconnect, but I just revisited the brief, and saw that
          it was the Disconnect... Anyways, she finally sent through the final
          draft with all the timing and scenes, which I found out was actually
          the same thing as I had before, I just had no idea because I didn't
          have the narration.
          <br />
          <br />
          The animator also sent through the narration! Which is also what I was
          waiting for because the animation is very narration heavy. I solo'ed
          the track and used automation to make sure everything was around the
          same volume, as some parts were peaking, and others it was too quiet.
          Overall I think the volume in the entire narration is too low, and I
          mentioned this to the animator and she said she would send me one with
          higher volume because she was the one who put it lower, so I am just
          waiting on that!
          <br />
          <br />
          When putting all the scenes I had together and made the sound for, I
          wasn't really a fan of how everything flowed together, because again,
          I didn't know the sequence of the scenes. So I actually ended up
          starting from scratch.
          <br />
          <br />
          I began with the foley, and this time I spent more time making sure
          everything was lined up, because I felt that on the first draft, it
          was pretty out of time. I couldn't find any sound samples or sound in
          libraries that had exactly what I wanted, so I actually recorded some
          of the footsteps and foley myself! I didn't have access to the
          professional equipment when I decided this, so I actually decided to
          just use my phone which I think didn't work too bad! I edited the
          sound in audacity to cut out the white noise and other stuff I didn't
          want, then just used EQ inside Ableton to get the depth and pitch I
          wanted.
          <br />
          <br />
          I was doing the foley around the narration and made sure nothing too
          loud was happening during the narration, which I think the animator
          was also doing because a lot of the events happen during the silences,
          like the scene where the main girl gets hit, etc. Because of this, I
          was really able to emphasise the important parts which I thought was
          good!
          <br />
          <br />
          There wasn't much foley to do besides the footsteps to be honest. I
          made sure to use different kinds of shoes for each character, so they
          had a more distinct footstep, and used automation and pitch shifting
          to make each step sound lighter, heavier, etc. For the first scene, I
          also added a layered a bunch of creaking noises and used EQ to
          highlight different aspects, but also made it echo in a creepy way! I
          also kept the original shaky breaths of the girl at the end that I had
          in my first draft.
          <br />
          <br />
          <img src="photos/week9/9.1.png" alt="" />
          <br />
          <br />
          For the classroom scene I think I kept everything exactly the same as
          the original draft, but just changed the timing based on the changed
          animation. As you can see here:
          <br />
          <br />
          <img src="photos/week9/9.2.png" alt="" />
          <br />
          <br />
          The interrogation scene changed quite a bit, so the ambience that I
          will do next week will change quite a bit from the draft. For now, I
          just did the footsteps to match what was happening on screen, as well
          as clothes rustling when the character moves around. I also added a
          sound effect for the slap/hit that happens, which I exaggerated quite
          a bit.
          <br />
          <br />
          <img src="photos/week9/9.3.png" alt="" />
          <br />
          <br />
          Then for the last scene, I also again just did the footsteps as the
          character walks up. When watching this I had a lot of ideas for the
          ambience so I am excited to start that next week! The original draft
          had the girl touching a candle, but I think it changes to her being
          scared of her reflection in the mirror, so I also changed the timing
          for that. It's hard to see exactly whats happening since the entire
          thing hasn't been rendered, but I'm excited to see the final product,
          because I already get a glimpse of what it will look like because of
          the end scene. I also then added a heartbeat (which I wasn't sure if
          it is diegetic or non-diegetic because I think it could technically be
          both) at the end of the scene to build up to the end, and also removed
          it from the starting sequence so it didn't play twice in the
          animation.
          <br />
          <br />
          <img src="photos/week9/9.4.png" alt="" />
          <br />
          <br />
          Overall I was very happy with how much better the foley was this time
          around, it was in time and I thought already fit the environment. Next
          week I plan to add the non-diegetic sound and the ambiences!
          <br />
          <br />
          You can watch it here:
          <a href="https://youtu.be/5wcFfg43Ghk"
            >https://youtu.be/5wcFfg43Ghk</a
          >
          <br />
          <br />
          Other than working on The Disconnect, I also started to make the
          in-game sounds for Fired Before Hired! I wasn't sure how audio worked
          in Unity so I called the game developer up and he gave me a quick
          rundown of how I could do each thing I was planning on doing for
          sound! For example, how I could make footsteps, but make sure it
          doesn't play when the character has jumped, etc.
          <br />
          <br />
          I started by recording some sounds for the sword. To create variation
          I wanted to have 3 basic layers of the sword. The swoosh in the air,
          the metallic sound and then the impact of the sword. I thought it
          would be good to have 3-5 versions of this, which I can then randomize
          in the code and even randomize pitch and speed by a small amount. This
          would fix the problem of sounds being too repetitive! I will do the
          same general idea for all the sounds!
          <br />
          <br />
          I then also recorded fabric rustling for the character movement, I
          just made a super long sound and will mute and unmute as needed based
          on what keys are pressed and if the character is grounded or not. For
          other character movement, I also worked on the footsteps for both the
          main character and the enemies. I will use the same trick as the sword
          with randomizing the pitch and the speed to get that variation without
          having to record a bunch of different footstep sounds.
          <br />
          <br />
          That was all I recorded for this week, and next week I will finish
          recording the rest and also start editing them and potentially start
          implementing them in Unity!
        </p>
      </div>
    </details>

    <!-- ------------------------------------ WEEK 10 ---------------------------------------------- -->

    <details class="week" id="week-10">
      <summary>Week 10</summary>
      <div class="week-content">
        <h3>Reflection</h3>
        <p>
          I reviewed the material discussed in class, specifically the Invisible
          Boys case study and the chapter from 'From Anthems to Abstractions:
          Creative Practice and Effect in Contemporary Screen Scoring'. Both
          look into how music and sound interact to express identity, emotion
          and place beyond traditional techniques.
          <br />
          <br />
          The chapter talks about that contemporary composers can often use
          textural and ambient approaches to avoke an effect, allowing sound to
          function as an emotional atmosphere rather than following
          melody-driven or thematic structures. This can be seen in Invisible
          Boys, where sound gives a voice to internal emotion and expresses
          identity, tension and vulnerability.
          <br />
          <br />
          Reading about this also allowed me to reflect on how I've been using
          sound in my projects, noticing that some of my ambient layers and
          drones already function in a similar way, as I tend to try and create
          atmosphere to get the emotion, rather than what you can actual hear in
          the environment! For example, the pitched down-choir I added in The
          Disconnect wasn't meant to sound melodic, it was more about creating
          an uncomfortable presence in the space.
          <br />
          <br />
          This weeks concepts reminds me that sound can express emotion through
          timer, rhythm, dynamic contrast and not just melody or harmony. It
          reinforced my belief that the boundary between music and sound design
          can often be blurred, especially when both are working toward the same
          emotional goal. Going forward I want to explore this boundary more
          intentionally!
        </p>
        <h3>Academic Research</h3>
        <p>
          This week I focused on two areas, the use of environmental sound in
          games and also I continued reading into the expressive potential of
          affective scoring in screen media!
          <br />
          <br />
          To begin with I read The Acoustic Ecology of the First Person Shooter,
          which analyses how environmental sound functions in game spaces. The
          author argues that ambience in games provides spatial orientation,
          emotional subtext and a sense of presence and how environmental
          soundscapes should feel like living systems that react to the players
          movements and virtual environment. The idea of an 'acoustic ecolofy'
          helped me reframe my approach to The Disconnect and Fired Before
          Hired. WHen I was layering winds, drones, etc. I realized that these
          sounds could also serve as emotional environments and not just
          background noise and can shape its mood and perception.
          <br />
          <br />
          I also continued reading the article that we read for class. Their
          discussion of affect, and how sound can create emotinoal resonance
          beyond narrative or melody was particularly relevent. Using a musical
          form that is more abstract can allow sound to communicate more fluidly
          with imagery and editingm evoking sensation rather than explicit
          meaning and I found this idea useful when reflecting on my own
          workflow on my projects.
          <br />
          <br />
          Both readings deepened my understanding of environmental sound and
          abstract scoring techniques to work together to define space and
          emotion. It reinforced that soundscapes should feel like its alive,
          breathe and shift, connecting the listerner to the space, but the tone
          of the scene
        </p>
        <h3>Artistic Research</h3>
        <p>
          To expand on this week's focus on environmental sound and affect, Ben
          Frost is a composer who blurs the line between music and sound design.
          His work in Fortitude and Dark uses dense layers of processed
          environmetal recordings liek wind, hums and feedback to evoke unease
          and psychological tension. In interviews he explains that he often
          records unstable sounds and then processes them until they feel alive,
          describing this process as building organisms out of noise. I thought
          this was such a creative way to make sounds and something that could
          definitely make a sound so unique, making things feel alive and using
          imperfections to make sounds feel human and emotional.
          <br />
          <br />
          I also looked into Brian Eno's ambient works, to demonstrate ho sound
          environments can create a sense is affective stillness. His philosophy
          of generative ambience means creating a soundscape that changes subtly
          without demanding attention. Eno descibes his ambient music as
          something that is ignorable as it is interesting, which was really
          interesting to think because I've never thought that I would want my
          music to be ignored. But his attention to space pacing and emotion and
          allows me to think about minimalism and atmosphere.
        </p>
        <h3>Technical Research</h3>
        <p>
          This week I focused on exporting and loudness management. I started to
          notice that my mixes sounded very different across devices and that my
          drafts sounded balanced and clear in headphones and speakers, but on
          my phone or laptop speakers, some frequencies were harsh and
          irritating. This inconsistency made me realize how important it is to
          monitor exports at a consistent loudness and also across multiple
          playback systems to make sure the sound translates properly.
          <br />
          <br />
          I watched a tutorial titled "Understanding LUFS and Headroom for Film
          and Game Audio", which broke down how loudness is measured using LUDS
          and why exports must follow certain standards. The video explained
          that broadcast and streaming content usually sits around -23 LUFS for
          dialogue based work and peaks no higher than -1 dBFS. Web based and
          game products can go slightly louder. These targets ensure that sound
          dpesn't clip or distort on common consumer devices and that the
          overall listening experience feels comfortable and consistent. I also
          learned about true peak limiting which prevents overstaturation that
          can occur in compressed formats like mp3 or mp4! After learning this,
          I found a LUFS meter plug in for Ableton that I will definitely start
          using.
          <br />
          <br />
          I also began to switch more between my studio headphones, laptop and
          phone speakers after each draft to see how the frequencies changes and
          will note the differences so I could apply EQ adjustments and focus on
          frequencies that are clear on all playback devices.
          <br />
          <br />
          This made it clear that exporting isn't just the final step, it is one
          of the most important because it actually determines how the audiences
          actually experiences the sound. If I do all this work for the export
          to suck, there is no point. Being aware of the loudness standards,
          headroom and playback can ensure that the detailed foley, ambience and
          music that I have been working on remains clear, dynbamic and balanced
          so that everyone can experience it.
        </p>
        <h3>Progress Report</h3>
        <p>
          This first thing I worked on this week was the ambience for The
          Disconnect. Even though I started from scratch, I still liked some of
          the ambiences I used in the first draft I made so I imported them all
          and was figuring out what to use for each scene! I also tried to
          highlight more of the foley throughout the entire animation, because
          one of the things I wasn't a fan of in the first animation was that
          the foley couldn't really be heard, and I think the footsteps and such
          could be a good tension builder.
          <br />
          <br />
          For the first scene, in the original draft, I felt like it was very
          dry and everything was in the mid range of frequency. In this new
          draft I tried to find and layer ones with different frequencies.
          Keeping this kind of low drones, but also higher pitched notes to
          build tension! I added a riser and a creepy sting when the main
          character is creeping around and then makes too big of a creaking
          sound. I decided to do this because when we watched A Bug's Life, I
          noticed there was musical stings alongside things that happened in the
          film, so I wanted to try it out. I think it worked really well! I also
          added this sample of a church choir singing, pitched it down a bunch
          and made it really low that you could just barely hear it to create a
          creepy atmosphere, without it being too overpowering. I wanted it to
          influence the atmosphere, but not sound like it was a diegetic sound
          that was coming from inside the church.
          <br />
          <br />
          <img src="photos/week10/10.1.png" alt="" />
          <br />
          <br />
          I then started to work on the classroom scene. But I had no idea what
          to put, any sound that I put felt wrong. I added this metallic
          scraping sound from the transition between the classroom scene to the
          interrogation scene and changed the pitch so it matched the main
          ambience of the next scene, which I'll talk about next.
          <br />
          <br />
          <img src="photos/week10/10.2.png" alt="" />
          <br />
          <br />
          I got this horror ambience and also reused one I made ages ago, which
          I had automated to make sure everything was around the same volume. I
          also found this pack of drones that had some cool sounds that I really
          liked, and used a low and high one to fill in the rest of the
          frequency spectrum. I kind of used the same ambiences for this scene
          and the next, but I think for the next draft I want it to slowly
          increase in intensity so theres a distinction from this scene and the
          next (sound wise).
          <br />
          <br />
          <img src="photos/week10/10.3.png" alt="" />
          <br />
          <br />
          Then for the final scene, I added a few more things. I added a bit
          more ambient noises, and especialy at the end I added a bunch of
          things like harsh wind and a drone. These were things that were
          present in the original draft, I just changed it up a tiny bit! I also
          added the stings nd suspense strings. I especially liked the tension
          built when she looks at herself in the mirror! I also kept the fire
          sound in the scene because I assume there will still be a candle in
          the scene even if she doesn't interact with it. It really depends on
          what is happening in the final animation!
          <br />
          <br />
          <img src="photos/week10/10.4.png" alt="" />
          <br />
          <br />
          You can watch the full (new) draft here:
          <a href="https://youtu.be/wsZlbXjXo5k"
            >https://youtu.be/wsZlbXjXo5k</a
          >
          <br />
          <br />
          I also recorded the rest of the things I needed for the Fired Before
          Hired game! I unfortunately did not do any editing this week and just
          recorded everything because something came up, but at least I have
          everything for next week.
          <br />
          <br />
          I recorded the sound that makes when an enemy is killed. I had
          previously recorded this audio of raw meat being squished around that
          I no longer have, but I thought had a good sound for a squelch of
          blood and flesh of someone dying. I did a few takes of that and also
          recorded other things! I recorded water splashing around and getting
          squeezed out of a bottle for maybe a blood spurt sound! Then also some
          random impacts into pillows with varying items like the cardboard
          container that badminton shuttlecocks come in, I tried punching the
          pillows, etc. Just things that make a thud sound basically. I think I
          will layer these alongside a sound I'll find online of a
          cartoon/animated death sound.
          <br />
          <br />
          The game developer also wanted some sounds for furniture destruction,
          but more simplified and less realistic. Just thuds when the walls fall
          over. I did this by letting thick textbooks fall on a variety of
          surfaces like wood, grass, etc. but my favorite and the one I will
          probably use is the one when it fell on carpet. I might use the others
          and layer them to create a more detailed sound. I might also get some
          samples of things breaking and put it very quiet just so the sound has
          some texture to it, and not just a dull thud. I'll be experimenting
          around with it when I start editing stuff!
          <br />
          <br />
          The last thing I recorded was for the sound when the job applications
          spawn. This makes it easier to figure out when things are coming at
          the player! What I did was recoord a jumprope whipping super fast to
          get that movement in the sound and I was thinking of slowing it down
          and pitching it down as well. I was thinking of also doing some higher
          sort of a synth sound as a motif when things get spawned so its
          recognizable.
          <br />
          <br />
          That is it for this week, and were getting so close to the end of the
          semester! The last stretch!
        </p>
      </div>
    </details>
  </body>
</html>


<!-- ------------------------------------ WEEK 11 ---------------------------------------------- -->

<details class="week" id="week-11">
    <summary>Week 11</summary>
    <div class="week-content">
      <h3>Academic Research</h3>
      <p>
        This week I revisted John-Steiner's ideas on Collaboration from his article 
        about Creative Collaborations. He describes creativity as something that grows 
        through dialogue and also shared experiences. I think this specifically was 
        something that I always kept in mind when working with the animators and the game 
        designers I was working with. Keeping the open line of communication and sending 
        them the updated versions of my work helped me keep their vision alive, and also 
        put less stress for doing last minute refinements at the end. I also applied this 
        while working with Maria, as we often went back and forth refining the ideas until 
        the music, foley and ambience felt like a cohesive whole. The article just solidifies 
        and adds to what I already believed about collaborative projects. It's not just about 
        agreeing to whatever hte other person says, but also finding the right balance 
        between staying open to compromise, and getting your own ideas across.
        <br>
        <br>
        I also looked into Juslins research on emotional responses to sound and music, 
        which basically breaks down how certain frequencies, tempos and timres affect's 
        mood. This was something I already could tell just from critical thinking alone, 
        but having that academic reinforcement helped. I think alongside my other research 
        especially with EQ carving, I was able to carve out certain frequencies to get that 
        mood that Juslin is talking about. I was also more critical about my layers of 
        ambience, taking particular care into seeing which EQ's were muddy, and whether 
        the layer was adding to the piece or taking away. Suprisingly, a lot of layers I 
        decided were actually adding not particular benefit. This helped me refine my 
        mixes more intentionally, to try to catch that timbre and frequencies that are 
        associated with the mood I want.
        <br>
        <br>
        I also read through Donald Schon's reflective practionier concept, which is 
        basically about learning through doing and making. I thought this perfectly 
        described this stage of these projects to me, since I really felt like I 
        was learning as I go. No matter how much academic research or artistic 
        research I undertake, I always learn better while doing. Since a lot of my 
        refinements weren't really planned, it came from using my own ears, adjusting 
        and reflecting. Every small change that I made, I tried to ask myself, why am I 
        doing this? What does it achieve? I think Schon's article really helped solidify 
        that mindset in me and that refining isn't just polishing the project, but also 
        polishing my creative thinking process skills.
      </p>
      <h3>Artistic Research</h3>
      <p>
        This week I looked into some composers that make some cool japanese films, and 
        saw Ryuichi Sakamoto, because of his work in Monster. His film scores feel so 
        delicate but also full of texture and can really create the environments well. He 
        often mixes environmental sounds and instruments (like a lot of the composers that 
        I am a fan of, I think I have a type!) and that he perfectly captures big emotions 
        without being obvious, which is important in genres like thrillers. When I was 
        refining all my projects, I tried to think of that balance, specifically in The 
        Hollow Child, when mixing the music with the ambience I created. Maybe in the future 
        as a project, I would like to record my own ambient layers, and try to use them 
        like instruments instead of effects, and see how well that works!
      </p>
      <h3>Technical Research</h3>
      <p>
        I also watched Joker, and I noticed that the sound designer uses a lot of minimal 
        drones that slowly get larger and evolve as the scene progresses. The sounds felt 
        like they were coming from the environment themselves, rather than just something 
        added in post production if that makes sense. Subtle movement is something I want 
        to explore more in future projects, I only really did a bit of it for these projects 
        but things like doing EQ animation and reverb to make soundscapes breathe and move 
        is something I want to practice on, since I'm not fully confident in my skills in 
        that area. Watching Joker basically taught me that istead of adding more layers to create 
        more tension, I could make my existing ones evolve over time.
      </p>
      <h3>Progress Report</h3>
      <p>
        This week I did a lot of work on The Hollow Child! Maria sent through her music 
        so I was finally able to start some mixing and refinements based on her music. 
        The first thing that I worked on was getting the timing right. There were a few changes 
        from the animation, but the animator promised that she wouldn't change the 
        timing anymore, she just needs to finish the final drawings and will put it on 
        top. Since the timing changed, I spent quite a lot of time getting all the existing 
        sounds correct, It was quite annoying to be honest because a lot of it was like just 
        a few seconds off, and I had a lot of layers. Anyways you can see it here:

        <img src="photos/week11/11.1.png" alt="">

        After that, I worked on getting Maria's music to not peak, and be more dynamic. I did 
        this by adding a compressor to make the louder sounds a bit quieter, and the quieter 
        sounds a bit louder. I also used some automation to get exactly what I wanted in terms 
        of sound dynamics and such, to try and keep the essence of what Maria wanted to be quiet, 
        and what she wanted to be loud. I only really did this because the music was way too low 
        compared to the ambience, and was getting pretty drowned out and I wanted to make sure 
        that Maria's hard work could be heard. You can see the automation here:

        <img src="photos/week11/11.3.png" alt="">

        I then started to work on some cool panning effects! I thought it'd be cool to have the ghost 
        pan around the headphones, as if the listner was the werewolf, and was hearing these creepy 
        laughs all around him, not knowing what it is! I wanted to try and capture that eerie creepy 
        feeling of someone being close to your ear, and moving around. I did this for all the ghost 
        laughs, the wooshes and the humming!  

        <img src="photos/week11/11.2.png" alt="">
        <img src="photos/week11/11.4.png" alt="">

        I then added this long sample of wind that I found online, as one of the feedback was that 
        the wind felt placed on, rather than in the environment. I also got another sample of a creepier 
        forest, which basically had more reverb and empty space, but also some creepy sounds like creaks. 
        So when I wanted the forest to feel a bit emptier and creepy, I'd slowly automate it and fade the 
        sound to change, and change it back when I wanted it back to 'normal'. The normal sounds basically 
        just consisted of more defined wind, and some rustling. I purposefuly avoided those with animals 
        like bird's chirping etc, to make sure that the forest didn't feel too alive.

        <img src="photos/week11/11.6.png" alt="">

        From there it was just about tweaking some effects to make sure everything was a bit smoother!
         I layered a bit more wind sounds when I wanted the atmosphere to be mroe intense, as well as 
         changed up the jumpscare of the rabbit! I did this because Maria highlighted the rabbit with a 
         impact on strings, so I added more low drones, and also did my own tension rising and impact to 
         play alongside her music, so it sounds more like combined as one. I also looked through a lot of 
         the ambiences I had and was taking note of the different frequencies that they laid at. I removed 
         a lot of the ones that interfered with them music, so I kept a lot of low end, and a few high end 
         sounds, with only a very little audio in a mid frequency range, mainly kept to accompany the 
         music so it didn't feel so alone, if that makes sense!

         <br>
         <br>
         That was all I did for The Hollow Child! We actually got the animator to listen to the sounds with 
         a professional speaker set up, and there we were able to hear a few problem areas. Maria, the animator 
         and I were all discussing in person what we should do, how we should change it, and just in general how 
         to make the entire thing align with the animators vision, as well as how we could imrpove on what she 
         wanted! I will work on the things I got feedback on next week, as the animator also said she has 
         a few more scenes she is nearly done with, and I can do more of the sound design on top of that! You 
         can watch the new draft with the music <a href="https://www.youtube.com/watch?v=2jiT6h20BZc">here</a>!

         <br>
         <br>
         This week I also worked on The Disconnect! From the draft from last week (which I was pretty happy 
         about!), Caroline the animator gave me a bit of feedback but was overall really happy about it and 
         was really just nitpicking (her words) and some parts she thinks could be slightly better!
         <br>
         <br>
        The first thing she mentioned was that the narration felt a bit quiet compared to the sound effects. 
        I actually completely agreed with this, but the issue was that I already had maxxed out the volume on 
        the narration she gave me. I mentioned this and she realized that she had made the narration way quieter 
        in the file that she gave me, as she was editing it in Premiere Pro. She gave me the raw wav file which 
        I then automated again, to make sure it didn't peak too high. This made a significant difference, I could 
        really hear the dialogue clearer, and didn't have to make my ambience less bold to make room for the dialogue!

         <img src="photos/week11/11.7.png" alt="">

         The next thing she mentioned was that she wanted the ending to be a bit more powerful. She wants the sound 
         to build up as she gets closer to the end, as if she was walking closer to danger. I moved around the 
         different ambiences I had, and played around with EQ and automation to make it get progressively louder 
         and more chaotic near the end. I also changed up the riser and impact to make it less of a cinematic 
         sound, and more of a creepy, scary sound! I don't know if I went too ham on the build up, as it was so close 
         to hitting red on the frequency marker, but Caroline mentioned that she liked what I did so I am happy!

         <img src="photos/week11/11.8.png" alt="">

         The last piece of feedback that she said was that she wanted the audio from the classroom scene to cut off 
         completely before the next scene. That was an easy fix! But she also wanted the sound of people talking 
         in the background to be changed to just girls speaking! I found a random sample and just adjusted the EQ 
         to muffle it up a bit to make it sound like it was coming from the hallway. I also did a bit of automation 
         to get the levels a bit more together, as there were a lot of peaks in the original recording.

         <img src="photos/week11/11.9.png" alt="">

         Next was just a matter of adding effects to foley! I basically added an EQ, a compressor, then a reverb 
         to most of the foley, just changed the parameters based on the character and the environment. I tried 
         to make the antogaonist have lower footsteps, to make them sound more menacing and scary, while the 
         protagonist would be on the higher side. I adjusted the reverb based on the environment they are in, like 
         in the cathedral in the first scene the size was pretty big, and the classroom was pretty small in comparison, 
         with basically no reverb, I also used the compressor to just make sure that everything was around the 
         same volume, so you could clearly hear it above everything else!

         <img src="photos/week11/11.10.png" alt="">

         After this, I did a LOT of EQ carving. I basically added an EQ Eight on all tracks and checked which frequencies 
         it was focused on. I added a little boost, and carved out that frequency in the other tracks, if that makes 
         sense. I wanted to make sure each highlighted aspect of each sound was at the front, and it prevented 
         a lot of muddiness in other frequency ranges! This was particularly useful when trying to make sure 
         that the dialogue could be heard, I made sure that the dialogue frequency range was boosted, and was 
         a bit lower for all the other sound! I think I could have done this a bit better with this, especially 
         at the end, but thats something for next week I think!
         <img src="photos/week11/11.11.png" alt="">

         You can watch the updated version <a href="https://youtu.be/KsAbEDjkiRM">here</a>! 
         <br>
         <br>
         And that's all for this week! One more week to go.
      </p>
    </div>
  </details> 

<!-- ------------------------------------ WEEK 12 ---------------------------------------------- -->

<details class="week" id="week-4">
    <summary>Week </summary>
    <div class="week-content">
      <h3>Academic Research</h3>
      <p>
        Since this is the last week of class, I have basically moved into the refinment phase in all my projects! I researched how play testing and iteration 
        can guide improvements in both design and suond. Fullerton has a article that discusses playtesting specifically, and how it is not supposed to be just 
        a polishing step, but it should be designed as a feedback driven process that can reveal a lot about how palyers actually perceieve certain mechanics, 
        pacing and atmosphere. She explaisn that repeated testing throughout the project lets the developers discover player responses that they don't expect, 
        as well as see how they react to certain things or do things a certain way and adjust accordingly. I really kept this in mind throughout making the Fired 
        Before Hired game, because there were multiple things I would have missed if I didn't ask someone else to try something. This is why I had multiple testing 
        phases for the game, and will have another one this weekend. 
        <br><br>
        Even though this specifically talks about game design, I realize how important it also is for things that aren't interactive. Seeing what people focus on 
        in video, their reactions, what they focus on hearing, they can show how I can improve my sound design for my animations as well. Things like foley layers 
        being too soft are things I would have never known simply because I knew it was there, so I could hear it. But it could be differnt for someone who is listening 
        to it for the first time. Iterative refinement is super important and will be something I consistently implement in my future projects, and wish that I had 
        done more of that for these projects. I did do the drafts but I didn't always have a feedback-driven session for each one of them, and if I did, maybe didn't 
        ask the right questions for it.
        <br>
        <br>
        I also looked into Collins' work again to learn more about what to consider when implementing audio systems in interactive media. It highlights the importance 
        of reacting to player input in real time while remaining coherent with gameplay goals. Good game audio must respond dynamically to context rather than 
        following a linear timeline, making it more interesting, but also more difficult to execute. This was directly relevant to the audio systems I was coding 
        in Unity and I needed to find a way to make sure that sounds could overlap, fade and randomize smoothlly without the players noticing. Perceptual believability 
        stood out to me, which is basically just how player interpret sound rather than how real it is. It reinforced my decisions around timing, amplitude and even 
        which audio clips played for each sound.
      </p>
      <h3>Artistic Research</h3>
      <p>
        For this weeks artistic research/inspiration, I looked at Silent Hill F, because I am currently playing through it right now! One of the first things I immediately 
        noticed when playing that game was how good the sound was. I am not sure if it' because I am more aware about sound and their choices and effects because of this 
        course, but it was something that stood out to me immediately. The way that the sound was panned in my headphones felt so immersive, and I noticed myself wanting to 
        pause or take my headphones off because I was getting way too immersed and wanted to break it because I kept screaming and waking my family up (I was playing at the 
        middle of the night). Anyways that's pretty irrelevent, I just wanted to say that it was super good. 
        <br>
        <br>
        Thinking about it, it uses spatial audio, texture and also restraint to constantly shift the focus from extreme closeness to distant echoes. Doing this made those 
        jumps feel so big, I felt the breathing behind my neck when there was someone behind me, as well as the empty atmosphere of the abandoned town. The subtle animation 
        and EQ made the ambience feel so alive and the reverb tail constantly evolves, so even in the quiet moments, there was never any total silence, but it definitely felt 
        like it. One of the things that I noticed was during the cutscenes, there would be wind playing, and then suddenly the wind transforms into like a low drone that is used 
        for jumpscares and it was super immersive because I didn't notice the change, but I felt that there was one happening, like I could predict the jumpscare happening. Basically 
        everything about it just reenforced things that I already knew, but to a much bigger scale. It showed me that even though I know what they are doing and the effects 
        of it, I still have so much left to improve on with it to reach that kind of level.
        <br>
        <br>
        Because of that, I also decided to look into the indie game 'Celeste'. I wanted to compare how the sound from a super popular A level game, differs from an indie game 
        that had a much lower budget, and how they are both at the same level of popularness among gamers. Since it's a different genre of game it's also a pretty hard comparison 
        but in general, Celeste is very music heavy, without piano textures and pads shifting dynamically as the player progresses. What struck me the most interesting was 
        how each area's theme evolved. Similar to other level based games, each area's melodies subtly changed as the player climbs/progresses, showing the progress and also struggle 
        as it gets more intense.In interviews, the developer talked about composing with emotional momentum and ensuring taht every piece felt personal rather than procedural. 
        This approach was able to capture the player's innter experience and matches the heart and tone of the work, rather than just the technical aspect. It was nice to see 
        how something other than horror/thriller can still have the same base goal of achieving the emotion of the world that is being created. 
      </p>
      <h3>Technical Research</h3>
      <p>
        This weeks technical reserach was mainly about Unity. Everything about it. From beginner tutorials to advanced tutorials about sound. I started off with just skimming 
        through the beginner tutorials so I could get around and play scenes and make changes and things like that. That was pretty easy to get the hold of. Then 
        I started too watch the beginner Unity audio tutorials. It was super easy to follow actually and just talked about how audio is played in Unity and the different types. 
        This was particularly useful as there were a few different types of audio that I wanted to have (background sound, sound based on states, player sounds, etc.) It briefly talked 
        about how that worked and I was able to get a grasp on it pretty quickly!
        <br>
        <br>
        The next and main thing was creating audio managers in Unity. It was basically just code that you write that effects how sound is played through, the parameters, when it plays 
        and stuff like that. But the main difference is that you can basically do anything in it as long as you know how to code it. I followed this tutorial that talks about how 
        to create a reusable audio manager system. A system where you write the code and what it will do, but basically leave the audio actually blank, and you can drag and drop what 
        audio you want to play with that. So I could basically copy and paste that script and use it for another project as it doesn't have any audio files attached to it! This tutorial 
        was super useful and I coded everything with that base. It also went through how to call the functions and methods to make things actually play, as well as how to make sliders that 
        effect parameters like pitch and stuff, so I wouldn't have to go inside the script everytime I wanted to change how loud an audio plays, etc.!
        <br>
        <br>
        There was another specific one about UI and background music that I followed! It basically just watched hover states and was something along the lines of "If button has hoverstate:true, then play 
        this audioclip"! Then it was the same for the click state and things like that. I think that since I have a lot of background with coding it was super easy to pick up, but just 
        learning the new language was a bit tough. I think in th future I want to learn how to use middleware like Wwise or FMOD to really focus on the audio aspect of it! 

        <br>
        <br>
        There were way more things that I learned but I don't want to bore you with explaining all the code! Just know I found it super interesting and fun to learn about!
      </p>
      <h3>Progress Report</h3>
      <p>
        Now we are finlly at this last week! I wasn't able to do much of the animations in this last week because the animators are still working hard
        on their project. But I did finish Fired Before Hired! Keep in mind that everything that had to do with coding and Unity took me HOURS and I have 
        greatly summarized what happened but I was no where near this calm and it did not go this smoothly.
        <br>
        <br>
        From last time I recorded everything but I didn't put much of them on my DAW. I did that this week as well as editing to give
        it a bit of EQ and reverb to make it come to life. I will probably add a bit of reverb in Unity code itself as well. So firstly,
        I actually planned out what I wanted for each sound. I'm going to try my best to explain what I wanted with an example. For example,
        for the sword sound, it consists of 2 main sounds, the woosh/swing and the metallic sound. I wanted to create an array for both sounds 
        that hold multiple variations that it could choose from. Then with Unity, I could also randomize say the pitch by 1-2 cents up, as 
        well as the volume, etc. So I would write with the code, sword-swing has 2 elements, choose form the array and randomize these parameters,
        then play. I wanted this basic concept for everything which is what I wrote here:
        <img src="photos/week12/list.png" alt="">

        After that I basically lined everything up and make sure it cut properly, as well as can be used with all the other sounds. 
        I used a mix of whatever I recorded, as well as some from online libraries, especially for the hurt sound and the game over sound. 
        In terms of effects, I basically just put EQ, compression and a bit of reverb in nearly every single sound. I'll need to add reverb in 
        the actual game though so it has a tail. 

        <img src="photos/week12/12.1.png" alt="">

        I don't want to put all the photos of everything I did but this is just an overview of the groups:

        <img src="photos/week12/12.5.png" alt="">

        As well as the amount in each of those groups:

        <img src="photos/week12/12.3.png" alt="">

        It has multiple verions of a single sound, as well as other sounds that can come from that player. Exporting that and renaming 
        everything was such a huge pain but I got there in the end! I just made a big folder that I imported in Unity that you can see here:

        <img src="photos/week12/1.png" alt="">

        I was basically using Unity from the bare minimum that I learnt last year but it was kind of intuitive that I knew what to do. The game 
        developers had everything named super well so it was easy to find exactly what I needed. Since they also already had existing sound as a 
        placeholder, it was easy to figure out how to link my own, which is what I did at first, but the code that they used made it super difficult 
        for me to do my idea of the arrays, so I started from scratch.

        <br>
        <br>
        Firstly, I did the menu music. I originally tried to do this thing where I upload 2 different audio clips, it plays the first one (which would be the 
        intro) and then as soon as that ended, would loop the next one until they get into the atual game. But for some reason there was a pause inbetween 
        each one and I couldn't for the life of me get rid of it so I created a script alongside this video that I place my full audio in there and I 
        assign a time where it starts to loop. So basically it will play the intro, then when the entire audio ends (I removed the ending) it starts back again 
        from where the loop is. You can see part of the code here:

        <img src="photos/week12/2.png" alt="">

        The tutorial I followed showed me how to make my own Audio Manager where I can make the basic script (which I can then also use for future 
        projects), and in the Unity engine, you can specify what audio clip and other parameters to put in. I used this logic for all the code I did 
        and basically now I have a bunch of code I can use for other scenarios which is super nice. Anyways this is what that code looked like in Unity, 
        you can see at the bottom left where I dragged in my clip, and when to start the loop.
        
        <img src="photos/week12/3.png" alt="">

        I then made another audio manager but this time it handled the UI. It was basically code that checked the hover/click state of buttons 
        and assigned an audio clip to them based on it. I had 3 different sounds, one for hover, one for click, and a seperate one for start game.

        <br><br>

        I then started on the audio manager for the sounds in game. I basically did what I told you I wanted earlier, but the script I made sure that 
        I could re use it. This is what part of the script looked like:

        <img src="photos/week12/4.png" alt="">

        You can see where all the variations are held. I did a few changes and this is what it looked like in the Unity engine:

        <img src="photos/week12/5.png" alt="">

        Basically it has the title of what the type of sound is, you can drag the audio clips inside and add as amny variations as you want, 
        then you can use the slider to get the minimum and maximum pitch and volume that the audio clip can have! Then inside the script I made multiple 
        methods that I can call whenever I want, that chooses which sounds pairs with what sound. Super confusing. I then went into the game manager 
        and put the methods where they need to be put which was pretty easy because again there was the placeholder audio, but there were a lot missing 
        that I had, like there was no footsteps, furniture sounds, etc. There was basically only audio for the sword and killing, and I kept the original 
        sound they had for the collection and the dash. 

        <br>
        <br>
        I realized that the sounds were all coming from the player with that original code so I altered a bit so that the methods for the enemy sounds 
        are called per enemy spawned and set max distance so that if you were too far you wouldnt hear them! This solved a lot of the issues where I felt 
        that there was an overwhelming amount of sound!

        <br>
        <br>

        Then the last thing I needed to do was the wave sound. This one was actually surprisingly easy to implement, but the actual planning was 
        so time consuming. I went back into my DAW and renamed the linear progression that I had made, as well as the sections for each wave 
        which looked like this:

        <img src="photos/week12/6.png" alt="">

       I exported the individual sounds for one bar since everything is a loop anyways and then I wrote a guide of what I was so it was easier for me to code what plays for each wave:

        <img src="photos/week12/7.png" alt="">

        I then did the code and made the functions that defines each wave. Basically everything is playing, but the code decides what layers are 
        muted and unmuted. I had a few issues especially with the mute and unmute setting button, but surprisingly this was the easiest thing to code!

        <br>
        <br>
        So everything I added was, furniture breaks (different for when you walk into it vs when you slash it), sword swing, parry (both when you 
        hit or miss), enemy death, enemy movement, enemy projectile spawn, player movement, player jump, player hurt,  game end sound, UI sound, main menu music and 
        wave music. All that was left to do was make sure everything was working together! I did a few play testing with the developers and fixed 
        the issues that arose (mainly some of the sound would carryt hrough different scenes, there was one where if you mute the audio then leave and 
        jhoin back it would be playing but still says its muted, stuff like that). Were doing more intense play testing this weekend so we will have 
        more people trying to look for any bugs in terms of audio!

        <br>
        <br>
        After that I did some audio mixing because I just let the audio play at full volume! For the game sound there was already the minimum and maximum 
        sound slider so that was pretty easy to fix. But in terms of the wave music that actually was pretty difficult. I began going through every single 
        layer and I had like 20, and was manually adjusting it before I got the idea to just make a volume curve. So I made a method that got lower as the 
        waves increased! I can set it to different values but I left it at 55%, as I felt like it was a good blend to still have the intensity of the music 
        but also can hear all the other sound! This is something I might change with later playtesting! Lastly, I added a bit of audio effects manually, like 
        reverb and delay, to make sure that the sound doesn't just randomly cut out, and it sounds like it is in the space.

        <br>
        <br>
        This one was definitely the hardest because I had to learn completely new things but I actually had the most fun with this! I actually went 
        into sound design so I could potentially be a sound designer for games so knowing that I had fun was actually super relieving for me! And that's done!
        You can play the game here (but at the time of writing it, the game developer has fully published all the changes) so it may or may not be updated 
        depending on when it will be published. <a href="https://sekrets.itch.io/fired-before-hired">https://sekrets.itch.io/fired-before-hired</a>

        <br>
        <br>
        I also did a few small tweaks for both The Hollow Child! It was mainly just adding sounds to the extra animated scenes if needed, but there 
        were also a few points of feedback from the last draft! This included lining up the singing to the music, boosting the jumpscare and lowering 
        the foley volume at the end! Another piece of feedback from that draft was to add a bit of silent parts in the animation. I did this by 
        removing the singing for the crow and let the music take over the scene. I also automated the wind around it to really get more quiet, then 
        put it back up for the next scene! The best change I did though was fixing the ghost floating sound. For the previous draft I wasn't a fan of 
        how it was flowing and was mainly there as a placeholder but it was very obviously out of place, so I made a a way better sound for it! I 
        reautomated those as well. And that was basically all I did for The Hollow Child as it's basically nearly done, just waiting for the animator to 
        finish all the scenes for final refinement! 

        <br>
        <br>
        You can watch it here: <a href="https://youtu.be/PRlyLOzdAEE">https://youtu.be/PRlyLOzdAEE</a>
      </p>
    </div>
  </details>



<!-- ------------------------------------ PRESENTATION---------------------------------------------- -->

<details class="week" id="week-0">
    <summary> Final Presentation </summary>
    <div class="week-content">
      <h1>The Hollow Child - Animation</h1>
      <h3>Summary</h3>
      <p>
        A gothic, dark fantasy short animation about a werewolf finding himself in a forest and encounters a myserious child who appears innocent, but carries 
        an unsettling presence. Overtime, he realizes that she is the source of death surrounding him. The film explores themes of death, empathy, lonelinness, 
        innocence and dread.
      </p>
      <h3>Goals</h3>
      <p>
        I was responsible for the foley, ambience and non-diegetic sound design, with Maria doing the musical score. My goal was to use sound to convey both 
        the emotional and supernatural layers of the story as well as the making the soundscape feel alive and an extension of the forest itself. I want it to feel 
        atmpshere heavy and less focused on realism.
      </p>
      <h3>Approach</h3>
      <p>
        Used Ableton as my DAW and used a combination of self-recorded foley and samples from sound libaries. I applied automation alongside EQ, reverb and other 
        forms of modulation to make the sounds feel alive and flow through the scenes smoothly. I also experimented with the stereo field to give the illusion of movement, 
        mainly through the use of panning to create movement, and added subtle motion and quieter moments to heighten suspense at certain parts.
      </p>
      <p>
        Link to most recent draft: <a href="https://rmiteduau-my.sharepoint.com/:v:/g/personal/s4009260_student_rmit_edu_au/EccehwMWsSdEiMR3MDZsgV0BH5-sN-MWsLezEGhGyb5aVw?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=zsOLao">The Hollow Child</a>
      </p>
      <h1>The Disconnect</h1>
      <h3>Summary</h3>
      <p>
        A horror-thriller short animation that follows a schoolgirl frieving the death of her friend, who becomes convinced that supernatural forces are responsible. 
        When she visits the crime scene in search of answers, she uncovers evidence, but soon realizes she may have walked into a trap herself. The film explores themes 
        of identity, loneliness and fear through the lens of the supernatural.
      </p>
      <h3>Goals</h3>
      <p>
        I was responsible for the entire sound design, besides the music at the end. My goal was to use sound to convey the psychological unease and gradually build tension, 
        allowing the audience to experience the protagonist's paranoia. Rather than focusing on realism, I focused on atmosphere. Although, it is a very narrative heavy 
        film, so my goal was to find the perfect blend.
      </p>
      <h3>Approach</h3>
      <p>
        I combined self recorded Foley with layered drones and environmental textures to create the sound. I wanted to take risks in terms of sound design, layering a lot 
        of uneasy sounds and slowly building up towards the end as the protagonist gets closer to danger. Feedback helped me refine the pacing and balance of intensity, but I still 
        do think that I could blend it better before the final version.
      </p>
      <p>
        Link to the most recent draft: <a href="https://rmiteduau-my.sharepoint.com/:v:/g/personal/s4009260_student_rmit_edu_au/EUwCYDnQyxZHqlWWbiuhJagB_nQL19F-wiqt62u35PHNYQ?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=9hb7mB">The Disconnect</a>
      </p>
      <h1>Fired Before Hired</h1>
      <h3>Summary</h3>
      <p>
        A satirical, office-themed action game where players defend themselves against an endless wave of job recruiters. Uses hack and slash mechanics and its tone balances 
        action and parody exploring themes of absurdism through exaggerated sound and design.
      </p>
      <h3>Goals</h3>
      <p>
        I was responsible for all espects of sound design, including menu music, wave-based combat music, UI sounds and in-game foley. My main goal was to make the sound 
        both humorous and satisfying to support the fast paced combat and game's comedic tone.I wanted the combat music to intentionally feel to serious to contrast the 
        less-serious visuals and sound effects, while still keeping the sound effects feel mostly realistic. Enhancing the satire through contrast.
      </p>
      <h3>Approach</h3>
      <p>
        Created a lot of scripts for each aspect, but I'll talk about them briefly. The menu music plays the clip of the music, and with a user-inserted time stamp, will loop starting there once the clip 
        finishes, creating an endless looping menu music. For the wave-based music, I made loopable sections, and used to code to mute/unmute certain layers when each wave starts. 
        For the UI, I just wrote a script listening for a hover/click state and played the sounds accordingly. 
        For the sound effects, there are 2 kinds. The first one comes from the player, this includes sword slashes, footsteps, etc. The other type is enemy sounds which are scripted so that the sounds play 
        from the spawned enemies, giving the sense of distance and direction. To avoid repitition, I created a script that holds different sound clips in an array (for example, footsteps). It chooses one at 
        random, and then chooses a random pitch and volume to play it at within a range. Some sounds are made up of two arrays. An example is for the sword slash, it uses the sounds from the array for the air wooshes/swings 
        as well as the metallic shing sound, making sure each one still randomizes as well.
      </p>
      <p>
        Link to the game: <a href="https://sekrets.itch.io/fired-before-hired">Fired Before Hired</a>
      </p>
      <h1>Self Assessment</h1>
      <p>
        This semester I developed a much deeper understanding that for sound, it is both equally important to have the technical skill, but also to use it an expressive and meaningful way. 
        each project challenged a different part of my skill set. The Hollow Child strengthened my ability to create and mix foley and music together to create a cohesive whole, as well as 
        building emotion through atmosphere. The Disconnect helped me gain new skills in narrative dialogue, and mixing with that focus in mind, as well as improving on my psychological sound design. 
        Fired Before Hired helped me implement audio systems in Unity, as well as thinking about sound inside interactive media. Through these works, I have becme more confident in my recording, editing, 
        layering and atmospheric sound, but also thinking conceptually on why I like certain sounds, and the effects different sounds can have. I also have gained valuable experience with iterative processes, 
        and ensuring that I get feedback as much as I can to create a design that fits the client's vision. 
        <br>
        <br>
        I think to improve I would like to refine my mixing and mastering workflow, I feel like that is what is most lacking in all my projects, which gets highlighted when the sound 
        gets played from other forms of playback devices that aren't my headphones. I also wish that I took on some roles for musical composition and scoring, as I want to become more confident 
        in that aspect, so that in the future I can handle both design and musical elements seamlessly. I want to further work on my time management and communication during collaboration, as even though 
        I kept an open line of communication, I think I could have asked better questions in order to fully see what my client wanted from the sound.
        <br>
        <br>
        Overall, I am proud of my work this semester and have learnt a lot in this class and helped me realize a lot of places where I want to improve on!
      </p>
    </div>
  </details>
</body>
</html>
